## [200] Uncertainty-Guided Pixel Contrastive Learning for Semi-Supervised Medical Image Segmentation

**Authors**: *Tao Wang, Jianglin Lu, Zhihui Lai, Jiajun Wen, Heng Kong*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/201](https://doi.org/10.24963/ijcai.2022/201)

**Abstract**:

Recently, contrastive learning has shown great potential in medical image segmentation. Due to the lack of expert annotations, however, it is challenging to apply contrastive learning in semi-supervised scenes. To solve this problem, we propose a novel uncertainty-guided pixel contrastive learning method for semi-supervised medical image segmentation. Specifically, we construct an uncertainty map for each unlabeled image and then remove the uncertainty region in the uncertainty map to reduce the possibility of noise sampling. The uncertainty map is determined by a well-designed consistency learning mechanism, which generates comprehensive predictions for unlabeled data by encouraging consistent network outputs from two different decoders. In addition, we suggest that the effective global representations learned by an image encoder should be equivariant to different geometric transformations. To this end, we construct an equivariant contrastive loss to strengthen global representation learning ability of the encoder. Extensive experiments conducted on popular medical image benchmarks demonstrate that the proposed method achieves better segmentation performance than the state-of-the-art methods.

----

## [201] CARD: Semi-supervised Semantic Segmentation via Class-agnostic Relation based Denoising

**Authors**: *Xiaoyang Wang, Jimin Xiao, Bingfeng Zhang, Limin Yu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/202](https://doi.org/10.24963/ijcai.2022/202)

**Abstract**:

Recent semi-supervised semantic segmentation methods focus on mining extra supervision from unlabeled data by generating pseudo labels. However, noisy labels are inevitable in this process which prevent effective self-supervision. This paper proposes that noisy labels can be corrected based on semantic connections among features. Since a segmentation classifier produces both high and low-quality predictions, we can trace back to feature encoder to investigate how a feature in a noisy group is related to those in the confident groups. Discarding the weak predictions from the classifier, rectified predictions are assigned to the wrongly predicted features through the feature relations. The key to such an idea lies in mining reliable feature connections. With this goal, we propose a class-agnostic relation network to precisely capture semantic connections among features while ignoring their semantic categories. The feature relations enable us to perform effective noisy label corrections to boost self-training performance. Extensive experiments on PASCAL VOC and Cityscapes demonstrate the state-of-the-art performances of the proposed methods under various semi-supervised settings.

----

## [202] Corner Affinity: A Robust Grouping Algorithm to Make Corner-guided Detector Great Again

**Authors**: *Haoran Wei, Chenglong Liu, Ping Guo, Yangguang Zhu, Jiamei Fu, Bing Wang, Peng Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/203](https://doi.org/10.24963/ijcai.2022/203)

**Abstract**:

Corner-guided detector enjoys potential ability to yield precise  bounding boxes. However, unreliable corner pairs, generated by heuristic grouping guidance, hinder the development of this detector. In this paper, we propose a novel corner grouping algorithm, termed as Corner Affinity, to significantly boost the reliability and robustness of corner grouping. The proposed Corner Affinity is a couple of two interactional factors, namely, 1) the structure affinity (SA), applying to generate preliminary corner pairs through the corresponding object's shallow construction information. 2) the contexts affinity (CA), running as optimizing corner pairs via embedding deeper semantic features of affiliated instances.  Equipped with the Corner Affinity, a detector can produce high-quality bounding boxes upon preferable paired corner keypoints. Experimental results show the superiority of our design on multiple benchmark datasets. Specifically, for CornerNet baseline, the proposed Corner Affinity brings AP boostings of 5.8% on COCO, 35.8% on Citypersons, and 17.2% on UCAS-AOD without bells and whistles.

----

## [203] Multi-scale Spatial Representation Learning via Recursive Hermite Polynomial Networks

**Authors**: *Yuanbo Lin Wu, Deyin Liu, Xiaojie Guo, Richang Hong, Liangchen Liu, Rui Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/204](https://doi.org/10.24963/ijcai.2022/204)

**Abstract**:

Multi-scale representation learning aims to leverage diverse features from different layers of Convolutional Neural Networks (CNNs) for boosting the feature robustness to scale variance. For dense prediction tasks, two key properties should be satisfied: the high spatial variance across convolutional layers, and the sub-scale granularity inside a convolutional layer for fine-grained features. To pursue the two properties, this paper proposes Recursive Hermite Polynomial Networks (RHP-Nets for short). The proposed RHP-Nets consist of two major components: 1) a dilated convolution to maintain the spatial resolution across layers, and 2) a family of Hermite polynomials over a subset of dilated grids, which recursively constructs sub-scale representations to avoid the artifacts caused by naively applying the dilation convolution. The resultant sub-scale granular features are fused via trainable Hermite coefficients to form the multi-resolution representations that can be fed into the next deeper layer, and thus allowing feature interchanging at all levels. Extensive experiments are conducted to demonstrate the efficacy of our design, and reveal its superiority over state-of-the-art alternatives on a variety of image recognition tasks. Besides, introspective studies are provided to further understand the properties of our method.

----

## [204] A Decoder-free Transformer-like Architecture for High-efficiency Single Image Deraining

**Authors**: *Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Tian-Jing Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/205](https://doi.org/10.24963/ijcai.2022/205)

**Abstract**:

Despite the success of vision Transformers for the image deraining task, they are limited by computation-heavy and slow runtime. In this work, we investigate Transformer decoder is not necessary and has huge computational costs. Therefore, we revisit the standard vision Transformer as well as its successful variants and propose a novel Decoder-Free Transformer-Like (DFTL) architecture for fast and accurate single image deraining. Specifically, we adopt a cheap linear projection to represent visual information with lower
computational costs than previous linear projections. Then we replace standard Transformer decoder block with designed Progressive Patch Merging (PPM), which attains comparable performance and efficiency. DFTL could significantly alleviate the computation and GPU memory requirements through proposed modules. Extensive experiments
demonstrate the superiority of DFTL compared with competitive Transformer architectures, e.g., ViT, DETR, IPT, Uformer, and Restormer. The code is available at https://github.com/XiaoXiao-Woo/derain.

----

## [205] Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation

**Authors**: *Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/206](https://doi.org/10.24963/ijcai.2022/206)

**Abstract**:

Due to the prosperity of Artificial Intelligence (AI) techniques, more and more backdoors are designed by adversaries to attack Deep Neural Networks (DNNs). Although the state-of-the-art method Neural Attention Distillation (NAD) can effectively erase backdoor triggers from DNNs, it still suffers from non-negligible Attack Success Rate (ASR) together with lowered classification ACCuracy (ACC), since NAD focuses on backdoor defense using attention features (i.e., attention maps) of the same order. In this paper, we introduce a novel backdoor defense framework named Attention Relation Graph Distillation (ARGD), which fully explores the correlation among attention features with different orders using our proposed Attention Relation Graphs (ARGs). Based on the alignment of ARGs between teacher and student models during knowledge distillation, ARGD can more effectively eradicate backdoors than NAD. Comprehensive experimental results show that, against six latest backdoor attacks, ARGD outperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by up to 3.23%.

----

## [206] SCMT: Self-Correction Mean Teacher for Semi-supervised Object Detection

**Authors**: *Feng Xiong, Jiayi Tian, Zhihui Hao, Yulin He, Xiaofeng Ren*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/207](https://doi.org/10.24963/ijcai.2022/207)

**Abstract**:

Semi-Supervised Object Detection (SSOD) aims to improve performance by leveraging a large amount of unlabeled data. Existing works usually adopt the teacher-student framework to enforce student to learn consistent predictions over the pseudo-labels generated by teacher. However, the performance of the student model is limited since the noise inherently exists in pseudo-labels. In this paper, we investigate the causes and effects of noisy pseudo-labels and propose a simple yet effective approach denoted as Self-Correction Mean Teacher(SCMT) to reduce the adverse effects. Specifically, we propose to dynamically re-weight the unsupervised loss of each student's proposal with additional supervision information from the teacher model, and assign smaller loss weights to possible noisy proposals. Extensive experiments on MS-COCO benchmark have shown the superiority of our proposed SCMT, which can significantly improve the supervised baseline by more than 11% mAP under all 1%, 5% and 10% COCO-standard settings, and surpasses state-of-the-art methods by about 1.5% mAP. Even under the challenging COCO-additional setting, SCMT still improves the supervised baseline by 4.9% mAP, and significantly outperforms previous methods by 1.2% mAP, achieving a new state-of-the-art performance.

----

## [207] Boosting Multi-Label Image Classification with Complementary Parallel Self-Distillation

**Authors**: *Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, Bo Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/208](https://doi.org/10.24963/ijcai.2022/208)

**Abstract**:

Multi-Label Image Classification (MLIC) appro-aches usually exploit label correlations to achieve good performance. However, emphasizing correlation like co-occurrence may overlook discriminative features and lead to model overfitting. In this study, we propose a generic framework named Parallel Self-Distillation (PSD) for boosting MLIC models. PSD decomposes the original MLIC task into several simpler MLIC sub-tasks via two elaborated complementary task decomposition strategies named Co-occurrence Graph Partition (CGP) and Dis-occurrence Graph Partition (DGP). Then, the MLIC models of fewer categories are trained with these sub-tasks in parallel for respectively learning the joint patterns and the category-specific patterns of labels. Finally, knowledge distillation is leveraged to learn a compact global ensemble of full categories with these learned patterns for reconciling the label correlation exploitation and model overfitting. Extensive results on MS-COCO and NUS-WIDE datasets demonstrate that our framework can be easily plugged into many MLIC approaches and improve performances of recent state-of-the-art approaches. The source code is released at https://github.com/Robbie-Xu/CPSD.

----

## [208] Webly-Supervised Fine-Grained Recognition with Partial Label Learning

**Authors**: *Yu-Yan Xu, Yang Shen, Xiu-Shen Wei, Jian Yang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/209](https://doi.org/10.24963/ijcai.2022/209)

**Abstract**:

The task of webly-supervised fine-grained recognition is to boost recognition accuracy of classifying subordinate categories (e.g., different bird species) by utilizing freely available but noisy web data. As the label noises significantly hurt the network training, it is desirable to distinguish and eliminate noisy images. In this paper, we propose two strategies, i.e., open-set noise removal and closed-set noise correction, to both remove such two kinds of web noises w.r.t. fine-grained recognition. Specifically, for open-set noise removal, we utilize a pre-trained deep model to perform deep descriptor transformation to estimate the positive correlation between these web images, and detect the open-set noises based on the correlation values. Regarding closed-set noise correction, we develop a top-k recall optimization loss for firstly assigning a label set towards each web image to reduce the impact of hard label assignment for closed-set noises. Then, we further propose to correct the sample with its label set as the true single label from a partial label learning perspective. Experiments on several webly-supervised fine-grained benchmark datasets show that our method obviously outperforms other existing state-of-the-art methods.

----

## [209] BiCo-Net: Regress Globally, Match Locally for Robust 6D Pose Estimation

**Authors**: *Zelin Xu, Yichen Zhang, Ke Chen, Kui Jia*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/210](https://doi.org/10.24963/ijcai.2022/210)

**Abstract**:

The challenges of learning a robust 6D pose function lie in 1) severe occlusion and 2) systematic noises in depth images. Inspired by the success of point-pair features, the goal of this paper is to recover the 6D pose of an object instance segmented from RGB-D images by locally matching pairs of oriented points between the model and camera space. To this end, we propose a novel Bi-directional Correspondence Mapping Network (BiCo-Net) to first generate point clouds guided by a typical pose regression, which can thus incorporate pose-sensitive information to optimize generation of local coordinates and their normal vectors. As pose predictions via geometric computation only rely on one single pair of local oriented points, our BiCo-Net can achieve robustness against sparse and occluded point clouds. An ensemble of redundant pose predictions from locally matching and direct pose regression further refines final pose output against noisy observations. Experimental results on three popularly benchmarking datasets can verify that our method can achieve state-of-the-art performance, especially for the more challenging severe occluded scenes. Source codes are available at https://github.com/Gorilla-Lab-SCUT/BiCo-Net.

----

## [210] Towards Adversarially Robust Deep Image Denoising

**Authors**: *Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/211](https://doi.org/10.24963/ijcai.2022/211)

**Abstract**:

This work systematically investigates the adversarial robustness of deep image denoisers (DIDs), i.e, how well DIDs can recover the ground truth from noisy observations degraded by adversarial perturbations. Firstly, to evaluate DIDsâ€™ robustness, we propose a novel adversarial attack, namely Observation-based Zero-mean Attack (OBSATK), to craft adversarial zero-mean perturbations on given noisy images. We find that existing DIDs are vulnerable to the adversarial noise generated by OBSATK. Secondly, to robustify DIDs, we pro- pose an adversarial training strategy, hybrid adversarial training (HAT), that jointly trains DIDs with adversarial and non-adversarial noisy data to ensure that the reconstruction quality is high and the denoisers around non-adversarial data are locally smooth. The resultant DIDs can effectively remove various types of synthetic and adversarial noise. We also uncover that the robustness of DIDs benefits their generalization capability on unseen real-world noise. Indeed, HAT-trained DIDs can recover high-quality clean images from real-world noise even without training on real noisy data. Extensive experiments on benchmark datasets, including Set68, PolyU, and SIDD, corroborate the effectiveness of OBSATK and HAT.

----

## [211] Weakening the Influence of Clothing: Universal Clothing Attribute Disentanglement for Person Re-Identification

**Authors**: *Yuming Yan, Huimin Yu, Shuzhao Li, Zhaohui Lu, Jianfeng He, Haozhuo Zhang, Runfa Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/212](https://doi.org/10.24963/ijcai.2022/212)

**Abstract**:

Most existing Re-ID studies focus on the short-term cloth-consistent setting and thus dominate by the visual appearance of clothing. However, the same person would wear different clothes and different people would wear the same clothes in reality, which invalidates these methods. To tackle the challenge of clothes change, we propose a Universal Clothing Attribute Disentanglement network (UCAD) which can effectively weaken the influence of clothing (identity-unrelated) and force the model to learn identity-related features that are unrelated to the worn clothing. For further study of Re-ID in cloth-changing scenarios, we construct a large-scale dataset called CSCC with the following unique features: (1) Severe: A large number of people have cloth-changing over four seasons. (2) High definition: The resolution of the cameras ranges from 1920×1080 to 3840×2160, which ensures that the recorded people are clear. Furthermore, we provide two variants of CSCC considering different degrees of cloth-changing, namely moderate and severe, so that researchers can effectively evaluate their models from various aspects. Experiments on several cloth-changing datasets including our CSCC and short-term dataset Market-1501 prove the superiority of UCAD. The dataset is available at https://github.com/yomin-y/UCAD.

----

## [212] Multi-level Consistency Learning for Semi-supervised Domain Adaptation

**Authors**: *Zizheng Yan, Yushuang Wu, Guanbin Li, Yipeng Qin, Xiaoguang Han, Shuguang Cui*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/213](https://doi.org/10.24963/ijcai.2022/213)

**Abstract**:

Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from a fully labeled source domain to a scarcely labeled target domain. In this paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA. Specifically, our MCL regularizes the consistency of different views of target domain samples at three levels: (i) at inter-domain level, we robustly and accurately align the source and target domains using a prototype-based optimal transport method that utilizes the pros and cons of different views of target samples; (ii) at intra-domain level, we facilitate the learning of both discriminative and compact target feature representations by proposing a novel class-wise contrastive clustering loss; (iii) at sample level, we follow standard practice and improve the prediction accuracy by conducting a consistency-based self-training. Empirically, we verified the effectiveness of our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet, and Office-Home datasets, and the experimental results demonstrate that our MCL framework achieves the state-of-the-art performance.

----

## [213] Perceptual Learned Video Compression with Recurrent Conditional GAN

**Authors**: *Ren Yang, Radu Timofte, Luc Van Gool*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/214](https://doi.org/10.24963/ijcai.2022/214)

**Abstract**:

This paper proposes a Perceptual Learned Video Compression (PLVC) approach with recurrent conditional GAN. We employ the recurrent auto-encoder-based compression network as the generator, and most importantly, we propose a recurrent conditional discriminator, which judges raw vs. compressed video conditioned on both spatial and temporal features, including the latent representation, temporal motion and hidden states in recurrent cells. This way, the adversarial training pushes the generated video to be not only spatially photo-realistic but also temporally consistent with the groundtruth and coherent among video frames. The experimental results show that the learned PLVC model compresses video with good perceptual quality at low bit-rate, and that it outperforms the official HEVC test model (HM 16.20) and the existing learned video compression approaches for several perceptual quality metrics and user studies. The project page is available at https://github.com/RenYang-home/PLVC.

----

## [214] CrowdFormer: An Overlap Patching Vision Transformer for Top-Down Crowd Counting

**Authors**: *Shaopeng Yang, Weiyu Guo, Yuheng Ren*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/215](https://doi.org/10.24963/ijcai.2022/215)

**Abstract**:

Crowd counting methods typically predict a density map as an intermediate representation of counting, and achieve good performance. However, due to the perspective phenomenon, there is a scale variation in real scenes, which causes the density map-based methods suffer from a severe scene generalization problem because only a limited number of scales are fitted in density map prediction and generation. To address this issue, we propose a novel vision transformer network, i.e., CrowdFormer, and a density kernels fusion framework for more accurate density map estimation and generation, respectively. Thereafter, we incorporate these two innovations into an adaptive learning system, which can take both the annotation dot map and original image as input, and jointly learns the density map estimator and generator within an end-to-end framework. The experimental results demonstrate that the proposed model achieves the state-of-the-art in the terms of MAE and MSE (e.g., it achieved a MAE of 67.1 and MSE of 301.6 on NWPU-Crowd dataset.), and confirm the effectiveness of the proposed two designs. The code is  https://github.com/special-yang/Top_Down-CrowdCounting.

----

## [215] Entity-aware and Motion-aware Transformers for Language-driven Action Localization

**Authors**: *Shuo Yang, Xinxiao Wu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/216](https://doi.org/10.24963/ijcai.2022/216)

**Abstract**:

Language-driven action localization in videos is a challenging task that involves not only visual-linguistic matching but also action boundary prediction. Recent progress has been achieved through aligning language queries to video segments, but estimating precise boundaries is still under-explored. In this paper, we propose entity-aware and motion-aware Transformers that progressively localize actions in videos by first coarsely locating clips with entity queries and then finely predicting exact boundaries in a shrunken temporal region with motion queries. The entity-aware Transformer incorporates the textual entities into visual representation learning via cross-modal and cross-frame attentions to facilitate attending action-related video clips. The motion-aware Transformer captures fine-grained motion changes at multiple temporal scales via integrating long short-term memory into the self-attention module to further improve the precision of action boundary prediction. Extensive experiments on the Charades-STA and TACoS datasets demonstrate that our method achieves better performance than existing methods.

----

## [216] Learning Prototype via Placeholder for Zero-shot Recognition

**Authors**: *Zaiquan Yang, Yang Liu, Wenjia Xu, Chong Huang, Lei Zhou, Chao Tong*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/217](https://doi.org/10.24963/ijcai.2022/217)

**Abstract**:

Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting semantic descriptions shared between seen classes and unseen classes. 
Current methods show that it is effective to learn visual-semantic alignment by projecting semantic embeddings into the visual space as class prototypes. However, such a projection function is only concerned with seen classes. When applied to unseen classes, the prototypes often perform suboptimally due to domain shift. 
In this paper, we propose to learn prototypes via placeholders, termed LPL, to eliminate the domain shift between seen and unseen classes. Specifically, we combine seen classes to hallucinate new classes which play as placeholders of the unseen classes in the visual and semantic space. Placed between seen classes, the placeholders encourage prototypes of seen classes to be highly dispersed. And more space is spared for the insertion of well-separated unseen ones. Empirically, well-separated prototypes help counteract visual-semantic misalignment caused by domain shift.
Furthermore, we exploit a novel semantic-oriented fine-tuning method to guarantee the semantic reliability of placeholders. 
Extensive experiments on five benchmark datasets demonstrate the significant performance gain of LPL over the state-of-the-art methods.

----

## [217] Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields

**Authors**: *Guangming Yao, Hongzhi Wu, Yi Yuan, Lincheng Li, Kun Zhou, Xin Yu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/218](https://doi.org/10.24963/ijcai.2022/218)

**Abstract**:

In this paper, we present a novel double diffusion based neural radiance field, dubbed DD-NeRF, to reconstruct human body geometry and render the human body appearance in novel views from a sparse set of images. We first propose a double diffusion mechanism to achieve expressive representations of input images by fully exploiting human body priors and image appearance details at two levels. At the coarse level, we first model the coarse human body poses and shapes via an unclothed 3D deformable vertex model as guidance. At the fine level, we present a multi-view sampling network to capture subtle geometric deformations and image detailed appearances, such as clothing and hair, from multiple input views. Considering the sparsity of the two level features, we diffuse them into feature volumes in the canonical space to construct neural radiance fields. Then, we present a signed distance function (SDF) regression network to construct body surfaces from the diffused features. Thanks to our double diffused representations, our method can even synthesize novel views of unseen subjects. Experiments on various datasets demonstrate that our approach outperforms the state-of-the-art in both geometric reconstruction and novel view synthesis.

----

## [218] RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization

**Authors**: *Hongyi Yao, Pu Li, Jian Cao, Xiangcheng Liu, Chenying Xie, Bingzhang Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/219](https://doi.org/10.24963/ijcai.2022/219)

**Abstract**:

We introduce a Power-of-Two post-training quantization( PTQ) method for deep neural network that meets hardware requirements and does not call for long-time retraining. PTQ requires a small set of calibration data and is easier for deployment, but results in lower accuracy than Quantization-Aware Training( QAT). Power-of-Two quantization can convert the multiplication introduced by quantization and dequantization to bit-shift that is adopted by many efficient accelerators. However, the Power-of-Two scale has fewer candidate values, which leads to more rounding or clipping errors. We propose a novel Power-of-Two PTQ framework, dubbed RAPQ, which dynamically adjusts the Power-of-Two scales of the whole network instead of statically determining them layer by layer. It can theoretically trade off the rounding error and clipping error of the whole network. Meanwhile, the reconstruction method in RAPQ is based on the BN information of every unit. Extensive experiments on ImageNet prove the excellent performance of our proposed method. Without bells and whistles, RAPQ can reach accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively with  weight INT2 activation INT4. We are the first to propose PTQ for the more constrained but hardware-friendly Power-of-Two quantization and prove that it can achieve nearly the same accuracy as SOTA PTQ method. The code will be released.

----

## [219] Learning Sparse Interpretable Features For NAS Scoring From Liver Biopsy Images

**Authors**: *Chong Yin, Siqi Liu, Vincent Wai-Sun Wong, Pong C. Yuen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/220](https://doi.org/10.24963/ijcai.2022/220)

**Abstract**:

Liver biopsy images play a key role in the diagnosis of global non-alcoholic fatty liver disease (NAFLD). The NAFLD activity score (NAS) on liver biopsy images grades the amount of histological findings that reflect the progression of NAFLD. However, liver biopsy image analysis remains a challenging task due to its complex tissue structures and sparse distribution of histological findings. In this paper, we propose a sparse interpretable feature learning method (SparseX) to efficiently estimate NAS scores. First, we introduce an interpretable spatial sampling strategy based on histological features to effectively select informative tissue 
 regions containing tissue alterations. Then, SparseX formulates the feature learning as a low-rank decomposition problem. Non-negative matrix factorization (NMF)-based attributes learning is embedded into a deep network to compress and select sparse features for a small portion of tissue alterations contributing to diagnosis. Experiments conducted on the internal Liver-NAS and public SteatosisRaw datasets show the effectiveness of the proposed method in terms of classification performance and interpretability.  regions containing tissue alterations. Then, SparseX formulates the feature learning as a low-rank decomposition problem. Non-negative matrix factorization (NMF)-based attributes learning is embedded into a deep network to compress and select sparse features for a small portion of tissue alterations contributing to diagnosis. Experiments conducted on the internal Liver-NAS and public SteatosisRaw datasets show the effectiveness of the proposed method in terms of classification performance and interpretability.

----

## [220] Learning to Hash Naturally Sorts

**Authors**: *Jiaguo Yu, Yuming Shen, Menghan Wang, Haofeng Zhang, Philip H. S. Torr*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/221](https://doi.org/10.24963/ijcai.2022/221)

**Abstract**:

Learning to hash pictures a list-wise sorting problem. Its testing metrics, e.g., mean-average precision, count on a sorted candidate list ordered by pair-wise code similarity. However, scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper, we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples' hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations, the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally, we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning, which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets.

----

## [221] Multi-Proxy Learning from an Entropy Optimization Perspective

**Authors**: *Yunlong Yu, Dingyi Zhang, Yingming Li, Zhongfei Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/222](https://doi.org/10.24963/ijcai.2022/222)

**Abstract**:

Deep Metric Learning, a task that learns a feature embedding space where semantically similar samples are located closer than dissimilar samples, is a cornerstone of many computer vision applications. Most of the existing proxy-based approaches usually exploit the global context via learning a single proxy for each training class, which struggles in capturing the complex non-uniform data distribution with different patterns. In this work, we present an easy-to-implement framework to effectively capture the local neighbor relationships via learning multiple proxies for each class that collectively approximate the intra-class distribution. In the context of large intra-class visual diversity, we revisit the entropy learning under the multi-proxy learning framework and provide a training routine that both minimizes the entropy of intra-class probability distribution and maximizes the entropy of inter-class probability distribution. In this way, our model is able to better capture the intra-class variations and smooth the inter-class differences and thus facilitates to extract more semantic feature representations for the downstream tasks. Extensive experimental results demonstrate that the proposed approach achieves competitive performances. Codes and an appendix are provided.

----

## [222] To Fold or Not to Fold: a Necessary and Sufficient Condition on Batch-Normalization Layers Folding

**Authors**: *Edouard Yvinec, Arnaud Dapogny, Kevin Bailly*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/223](https://doi.org/10.24963/ijcai.2022/223)

**Abstract**:

Batch-Normalization (BN) layers have become fundamental components in the evermore complex deep neural network architectures. Such models require acceleration processes for deployment on edge devices. However, BN layers add computation bottlenecks due to the sequential operation processing: thus, a key, yet often overlooked component of the acceleration process is BN layers folding. In this paper, we demonstrate that the current BN folding approaches are suboptimal in terms of how many layers can be removed. We therefore provide a necessary and sufficient condition for BN folding and a corresponding optimal algorithm. The proposed approach systematically outperforms existing baselines and allows to dramatically reduce the inference time of deep neural networks.

----

## [223] S2 Transformer for Image Captioning

**Authors**: *Pengpeng Zeng, Haonan Zhang, Jingkuan Song, Lianli Gao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/224](https://doi.org/10.24963/ijcai.2022/224)

**Abstract**:

Transformer-based architectures with grid features represent the state-of-the-art in visual and language reasoning tasks, such as visual question answering and image-text matching. However, directly applying them to image captioning may result in spatial and fine-grained semantic information loss. Their applicability to image captioning is still largely under-explored. Towards this goal, we propose a simple yet effective method, Spatial- and Scale-aware Transformer (S2 Transformer) for image captioning. Specifically, we firstly propose a Spatial-aware Pseudo-supervised (SP) module, which resorts to feature clustering to help preserve spatial information for grid features. Next, to maintain the model size and produce superior results, we build a simple weighted residual connection, named Scale-wise Reinforcement (SR) module, to simultaneously explore both low- and high-level encoded features with rich semantics. Extensive experiments on the MSCOCO benchmark demonstrate that our method achieves new state-of-art performance without bringing excessive parameters compared with the vanilla transformer. The source code is available at https://github.com/zchoi/S2-Transformer

----

## [224] Towards Universal Backward-Compatible Representation Learning

**Authors**: *Binjie Zhang, Yixiao Ge, Yantao Shen, Shupeng Su, Fanzi Wu, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/225](https://doi.org/10.24963/ijcai.2022/225)

**Abstract**:

Conventional model upgrades for visual search systems require offline refresh of gallery features by feeding gallery images into new models (dubbed as “backfill”), which is time-consuming and expensive, especially in large-scale applications. The task of backward-compatible representation learning is therefore introduced to support backfill-free model upgrades, where the new query features are interoperable with the old gallery features. Despite the success, previous works only investigated a close-set training scenario (i.e., the new training set shares the same classes as the old one), and are limited by more realistic and challenging open-set scenarios. To this end, we first introduce a new problem of universal backward-compatible representation learning, covering all possible data split in model upgrades. We further propose a simple yet effective method, dubbed as Universal Backward-Compatible Training (UniBCT) with a novel structural prototype refinement algorithm, to learn compatible representations in all kinds of model upgrading benchmarks in a unified manner. Comprehensive experiments on the large-scale face recognition datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method. Source code is available at https://github.com/TencentARC/OpenCompatible.

----

## [225] Region-level Contrastive and Consistency Learning for Semi-Supervised Semantic Segmentation

**Authors**: *Jianrong Zhang, Tianyi Wu, Chuanghao Ding, Hongwei Zhao, Guodong Guo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/226](https://doi.org/10.24963/ijcai.2022/226)

**Abstract**:

Current semi-supervised semantic segmentation methods mainly focus on designing pixel-level consistency and contrastive regularization. However, pixel-level regularization is sensitive to noise from pixels with incorrect predictions, and pixel-level contrastive regularization has a large memory and computational cost. To address the issues, we propose a novel region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation. Specifically, we first propose a Region Mask Contrastive (RMC) loss and a Region Feature Contrastive (RFC) loss to accomplish region-level contrastive property. Furthermore, Region Class Consistency (RCC) loss and Semantic Mask Consistency (SMC) loss are proposed for achieving region-level consistency. Based on the proposed region-level contrastive and consistency regularization, we develop a region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation, and evaluate our RC^2L on two challenging benchmarks (PASCAL VOC 2012 and Cityscapes), outperforming the state-of-the-art.

----

## [226] Improving Transferability of Adversarial Examples with Virtual Step and Auxiliary Gradients

**Authors**: *Ming Zhang, Xiaohui Kuang, Hu Li, Zhendong Wu, Yuanping Nie, Gang Zhao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/227](https://doi.org/10.24963/ijcai.2022/227)

**Abstract**:

Deep neural networks have been demonstrated to be vulnerable to adversarial examples, which fool networks by adding human-imperceptible perturbations to benign examples. At present, the practical transfer-based black-box attacks are attracting significant attention. However, most existing transfer-based attacks achieve only relatively limited success rates. We propose to improve the transferability of adversarial examples through the use of a virtual step and auxiliary gradients. Here, the “virtual step” refers to using an unusual step size and clipping adversarial perturbations only in the last iteration, while the “auxiliary gradients” refer to using not only gradients corresponding to the ground-truth label (for untargeted attacks), but also gradients corresponding to some other labels to generate adversarial perturbations. Our proposed virtual step and auxiliary gradients can be easily integrated into existing gradient-based attacks. Extensive experiments on ImageNet show that the adversarial examples crafted by our method can effectively transfer to different networks. For single-model attacks, our method outperforms the state-of-the-art baselines, improving the success rates by a large margin of 12%~28%. Our code is publicly available at https://github.com/mingcheung/Virtual-Step-and-Auxiliary-Gradients.

----

## [227] Plane Geometry Diagram Parsing

**Authors**: *Mingliang Zhang, Fei Yin, Yi-Han Hao, Cheng-Lin Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/228](https://doi.org/10.24963/ijcai.2022/228)

**Abstract**:

Geometry diagram parsing plays a key role in geometry problem solving, wherein the primitive extraction and relation parsing remain challenging due to the complex layout and between-primitive relationship. In this paper, we propose a powerful diagram parser based on deep learning and graph reasoning. Specifically, a modified instance segmentation method is proposed to extract geometric primitives, and the graph neural network (GNN) is leveraged to realize relation parsing and primitive classification incorporating geometric features and prior knowledge. All the modules are integrated into an end-to-end model called PGDPNet to perform all the sub-tasks simultaneously. In addition, we build a new large-scale geometry diagram dataset named PGDP5K with primitive level annotations. Experiments on PGDP5K and an existing dataset IMP-Geometry3K show that our model outperforms state-of-the-art methods in four sub-tasks remarkably. Our code, dataset and appendix material are available at https://github.com/mingliangzhang2018/PGDP.

----

## [228] SAR-to-Optical Image Translation via Neural Partial Differential Equations

**Authors**: *Mingjin Zhang, Chengyu He, Jing Zhang, Yuxiang Yang, Xiaoqi Peng, Jie Guo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/229](https://doi.org/10.24963/ijcai.2022/229)

**Abstract**:

Synthetic Aperture Radar (SAR) becomes prevailing in remote sensing while SAR images are challenging to interpret by human visual perception due to the active imaging mechanism and speckle noise. Recent researches on SAR-to-optical image translation provide a promising solution and have attracted increasing attentions, though still suffering from low optical image quality with geometric distortion due to the large domain gap. In this paper, we mitigate this issue from a novel perspective, i.e., neural partial differential equations (PDE). First, based on the efficient numerical scheme for solving PDE, i.e., Taylor Central Difference (TCD), we devise a basic TCD residual block to build the backbone network, which promotes the extraction of useful information in SAR images by aggregating and enhancing features from different levels. Furthermore, inspired by the Perona-Malik Diffusion (PMD), we devise a PMD neural module to implement feature diffusion through layers, aiming at removing the noises in smooth regions while preserving the geometric structures. Assembling them together, we propose a novel SAR-to-Optical image translation network named S2O-NPDE, which delivers optical images with finer structures and less noise while enjoying an explainability advantage from explicit mathematical derivation. Experiments on the popular SEN1-2 dataset show that our model outperforms state-of-the-art methods in terms of both objective metrics and visual quality.

----

## [229] A Probabilistic Code Balance Constraint with Compactness and Informativeness Enhancement for Deep Supervised Hashing

**Authors**: *Qi Zhang, Liang Hu, Longbing Cao, Chongyang Shi, Shoujin Wang, Dora D. Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/230](https://doi.org/10.24963/ijcai.2022/230)

**Abstract**:

Building on deep representation learning, deep supervised hashing has achieved promising performance in tasks like similarity retrieval. However, conventional code balance constraints (i.e., bit balance and bit uncorrelation) imposed on avoiding overfitting and improving hash code quality are unsuitable for deep supervised hashing owing to their inefficiency and impracticality of simultaneously learning deep data representations and hash functions. To address this issue, we propose probabilistic code balance constraints on deep supervised hashing to force each hash code to conform to a discrete uniform distribution. Accordingly, a Wasserstein regularizer aligns the distribution of generated hash codes to a uniform distribution. Theoretical analyses reveal that the proposed constraints form a general deep hashing framework for both bit balance and bit uncorrelation and maximizing the mutual information between data input and their corresponding hash codes. Extensive empirical analyses on two benchmark datasets further demonstrate the enhancement of compactness and informativeness of hash codes for deep supervised hash to improve retrieval performance (code available at: https://github.com/mumuxi/dshwr).

----

## [230] CATrans: Context and Affinity Transformer for Few-Shot Segmentation

**Authors**: *Shan Zhang, Tianyi Wu, Sitong Wu, Guodong Guo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/231](https://doi.org/10.24963/ijcai.2022/231)

**Abstract**:

Few-shot segmentation (FSS) aims to segment novel categories given scarce annotated support images. The crux of FSS is how to aggregate dense correlations between support and query images for query segmentation while being robust to the large variations in appearance and context. To this end, previous Transformer-based methods explore global consensus either on context similarity or affinity map between support-query pairs. In this work, we effectively integrate the context and affinity information via the proposed novel Context and Affinity Transformer (CATrans) in a hierarchical architecture. Specifically, the Relation-guided Context Transformer (RCT) propagates context information from support to query images conditioned on more informative support features. Based on the observation that a huge feature distinction between support and query pairs brings barriers for context knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures attention-aware affinity as auxiliary information for FSS, in which the self-affinity is responsible for more reliable cross-affinity. We conduct experiments to demonstrate the effectiveness of the proposed model, outperforming the state-of-the-art methods.

----

## [231] Few-Shot Adaptation of Pre-Trained Networks for Domain Shift

**Authors**: *Wenyu Zhang, Li Shen, Wanyue Zhang, Chuan-Sheng Foo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/232](https://doi.org/10.24963/ijcai.2022/232)

**Abstract**:

Deep networks are prone to performance degradation when there is a domain shift between the source (training) data and target (test) data. Recent test-time adaptation methods update batch normalization layers of pre-trained source models deployed in new target environments with streaming data. Although these methods can adapt on-the-fly without first collecting a large target domain dataset, their performance is dependent on streaming conditions such as mini-batch size and class-distribution which can be unpredictable in practice. In this work, we propose a framework for few-shot domain adaptation to address the practical challenges of data-efficient adaptation. Specifically, we propose a constrained optimization of feature normalization statistics in pre-trained source models supervised by a small target domain support set. Our method is easy to implement and improves source model performance with as little as one sample per class for classification tasks. Extensive experiments on 5 cross-domain classification and 4 semantic segmentation datasets show that our proposed method achieves more accurate and reliable performance than test-time adaptation, while not being constrained by streaming conditions.

----

## [232] Enhancing the Transferability of Adversarial Examples with Random Patch

**Authors**: *Yaoyuan Zhang, Yu-an Tan, Tian Chen, Xinrui Liu, Quanxin Zhang, Yuanzhang Li*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/233](https://doi.org/10.24963/ijcai.2022/233)

**Abstract**:

Adversarial examples can fool deep learning models, and their transferability is critical for attacking black-box models in real-world scenarios. Existing state-of-the-art transferable adversarial attacks tend to exploit intrinsic features of objects to generate adversarial examples. This paper proposes the Random Patch Attack (RPA) to significantly improve the transferability of adversarial examples by the patch-wise random transformation that effectively highlights important intrinsic features of objects. Specifically, we introduce random patch transformations to original images to variate model-specific features. Important object-related features are preserved after aggregating the transformed images since they stay consistent in multiple transformations while model-specific elements are neutralized. The obtained essential features steer noises to perturb the object-related regions, generating the adversarial examples of superior transferability across different models. Extensive experimental results demonstrate the effectiveness of the proposed RPA. Compared to the state-of-the-art transferable attacks, our attacks improve the black-box attack success rate by 2.9\% against normally trained models, 4.7\% against defense models, and 4.6\% against vision transformers on average, reaching a maximum of 99.1\%, 93.2\%, and 87.8\%, respectively.

----

## [233] Visual Emotion Representation Learning via Emotion-Aware Pre-training

**Authors**: *Yue Zhang, Wanying Ding, Ran Xu, Xiaohua Hu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/234](https://doi.org/10.24963/ijcai.2022/234)

**Abstract**:

Despite recent progress in deep learning, visual emotion recognition remains a challenging problem due to ambiguity of emotion perception, diverse concepts related to visual emotion and lack of large-scale annotated dataset. In this paper, we present a large-scale multimodal pre-training method to learn visual emotion representation by aligning emotion, object, attribute triplet with a contrastive loss. We conduct our pre-training on a large web dataset with noisy tags and fine-tune on visual emotion classification datasets. Our method achieves state-of-the-art performance for visual emotion classification.

----

## [234] Distilling Inter-Class Distance for Semantic Segmentation

**Authors**: *Zhengbo Zhang, Chunluan Zhou, Zhigang Tu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/235](https://doi.org/10.24963/ijcai.2022/235)

**Abstract**:

Knowledge distillation is widely adopted in semantic segmentation to reduce the computation cost. The previous knowledge distillation methods for semantic segmentation focus on pixel-wise feature alignment and intra-class feature variation distillation, neglecting to transfer the knowledge of the inter-class distance in the feature space, which is important for semantic segmentation such a pixel-wise classification task. To address this issue, we propose an Inter-class Distance Distillation (IDD) method to transfer the inter-class distance in the feature space from the teacher network to the student network. Furthermore, semantic segmentation is a position-dependent task, thus we exploit a position information distillation module to help the student network encode more position information. Extensive experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show that our method is helpful to improve the accuracy of semantic segmentation models and achieves the state-of-the-art performance. E.g. it boosts the benchmark model (``PSPNet+ResNet18") by 7.50% in accuracy on the Cityscapes dataset.

----

## [235] Domain Adversarial Learning for Color Constancy

**Authors**: *Zhifeng Zhang, Xuejing Kang, Anlong Ming*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/236](https://doi.org/10.24963/ijcai.2022/236)

**Abstract**:

Color Constancy aims to eliminate the color cast of RAW images caused by non-neutral illuminants. Though contemporary approaches based on convolutional neural networks significantly improve illuminant estimation, they suffer from the seriously insufficient data problem. To solve this problem by effectively utilizing multi-domain data, we propose the Domain Adversarial Learning  Color Constancy (DALCC) which consists of the Domain Adversarial Learning Branch (DALB) and the Feature Reweighting Module (FRM). In DALB, the Camera Domain Classifier and the feature extractor compete against each other in an adversarial way to encourage the emergence of domain-invariant features. At the same time, the Illuminant Transformation Module performs color space conversion to solve the inconsistent color space problem caused by those domain-invariant features. They collaboratively avoid model degradation of multi-device training caused by the domain discrepancy of feature distribution, which enables our DALCC to benefit from multi-domain data. Besides, to better utilize multi-domain data, we propose the FRM that reweights the feature map to suppress Non-Primary Illuminant regions, which reduces the influence of misleading illuminant information. Experiments show that the proposed DALCC can more effectively take advantage of multi-domain data and thus achieve state-of-the-art performance on commonly used benchmark datasets.

----

## [236] Domain Adaptation via Maximizing Surrogate Mutual Information

**Authors**: *Haiteng Zhao, Chang Ma, Qinyu Chen, Zhi-Hong Deng*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/237](https://doi.org/10.24963/ijcai.2022/237)

**Abstract**:

Unsupervised domain adaptation (UDA), which is an important topic in transfer learning, aims to predict unlabeled data from target domain with access to labeled data from the source domain. In this work, we propose a novel framework called SIDA (Surrogate Mutual Information Maximization Domain Adaptation) with strong theoretical guarantees. To be specific, SIDA implements adaptation by maximizing mutual information (MI) between features. In the framework, a surrogate joint distribution models the underlying joint distribution of the unlabeled target domain. Our theoretical analysis validates SIDA by bounding the expected risk on target domain with MI and surrogate distribution bias. Experiments show that our approach is comparable with state-of-the-art unsupervised adaptation methods on standard UDA tasks.

----

## [237] C3-STISR: Scene Text Image Super-resolution with Triple Clues

**Authors**: *Minyi Zhao, Miao Wang, Fan Bai, Bingjia Li, Jie Wang, Shuigeng Zhou*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/238](https://doi.org/10.24963/ijcai.2022/238)

**Abstract**:

Scene text image super-resolution (STISR) has been regarded as an important pre-processing task for text recognition from low-resolution scene text images. Most recent approaches use the recognizer's feedback as clues to guide super-resolution. However, directly using recognition clue has two problems: 1) Compatibility. It is in the form of probability distribution, has an obvious modal gap with STISR - a pixel-level task; 2) Inaccuracy. it usually contains wrong information, thus will mislead the main task and degrade super-resolution performance. In this paper, we present a novel method C3-STISR that jointly exploits the recognizer's feedback, visual and linguistical information as clues to guide super-resolution. Here, visual clue is from the images of texts predicted by the recognizer, which is informative and more compatible with the STISR task; while linguistical clue is generated by a pre-trained character-level language model, which is able to correct the predicted texts. We design effective extraction and fusion mechanisms for the triple cross-modal clues to generate a comprehensive and unified guidance for super-resolution. Extensive experiments on TextZoom show that C3-STISR outperforms the SOTA methods in fidelity and recognition performance. Code is available in https://github.com/zhaominyiz/C3-STISR.

----

## [238] Learning to Generate Image Source-Agnostic Universal Adversarial Perturbations

**Authors**: *Pu Zhao, Parikshit Ram, Songtao Lu, Yuguang Yao, Djallel Bouneffouf, Xue Lin, Sijia Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/239](https://doi.org/10.24963/ijcai.2022/239)

**Abstract**:

Adversarial perturbations are critical for certifying the robustness of deep learning models. A ``universal adversarial perturbation'' (UAP) can simultaneously attack multiple images, and thus offers a more unified threat model, obviating an image-wise attack algorithm. However, the existing UAP generator is underdeveloped when images are drawn from different image sources (e.g., with different image resolutions). Towards an authentic universality across image sources, we take a novel view of UAP generation as a customized instance of ``few-shot learning'', which leverages bilevel optimization and learning-to-optimize (L2O) techniques for UAP generation with improved attack success rate (ASR). We begin by considering the popular model agnostic meta-learning (MAML) framework to meta-learn a UAP generator. However, we see that the MAML framework does not directly offer the universal attack across image sources, requiring us to integrate it with another meta-learning framework of L2O. The resulting scheme for meta-learning a UAP generator (i) has better performance (50% higher ASR) than baselines such as Projected Gradient Descent, (ii) has better performance (37% faster) than the vanilla L2O and MAML frameworks (when applicable), and (iii) is able to simultaneously handle UAP generation for different victim models and data sources.

----

## [239] Test-time Fourier Style Calibration for Domain Generalization

**Authors**: *Xingchen Zhao, Chang Liu, Anthony Sicilia, Seong Jae Hwang, Yun Fu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/240](https://doi.org/10.24963/ijcai.2022/240)

**Abstract**:

The topic of generalizing machine learning models learned on a collection of source domains to unknown target domains is challenging. While many domain generalization (DG) methods have achieved promising results, they primarily rely on the source domains at train-time without manipulating the target domains at test-time. Thus, it is still possible that those methods can overfit to source domains and perform poorly on target domains. Driven by the observation that domains are strongly related to styles, we argue that reducing the gap between source and target styles can boost models’ generalizability. To solve the dilemma of having no access to the target domain during training, we introduce Test-time Fourier Style Calibration (TF-Cal) for calibrating the target domain style on the fly during testing. To access styles, we utilize Fourier transformation to decompose features into amplitude (style) features and phase (semantic) features. Furthermore, we present an effective technique to Augment Amplitude Features (AAF) to complement TF-Cal. Extensive experiments on several popular DG benchmarks and a segmentation dataset for medical images demonstrate that our method outperforms state-of-the-art methods.

----

## [240] Visual Similarity Attention

**Authors**: *Meng Zheng, Srikrishna Karanam, Terrence Chen, Richard J. Radke, Ziyan Wu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/241](https://doi.org/10.24963/ijcai.2022/241)

**Abstract**:

While there has been substantial progress in learning suitable distance metrics, these techniques in general lack transparency and decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person re-identification, and low-shot semantic segmentation.

----

## [241] Imperceptible Backdoor Attack: From Input Space to Feature Representation

**Authors**: *Nan Zhong, Zhenxing Qian, Xinpeng Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/242](https://doi.org/10.24963/ijcai.2022/242)

**Abstract**:

Backdoor attacks are rapidly emerging threats to deep neural networks (DNNs). In the backdoor attack scenario, attackers usually implant the backdoor into the target model by manipulating the training dataset or training process. Then, the compromised model behaves normally for benign input yet makes mistakes when the pre-defined trigger appears. In this paper, we analyze the drawbacks of existing attack approaches and propose a novel imperceptible backdoor attack. We treat the trigger pattern as a special kind of noise following a multinomial distribution. A U-net-based network is employed to generate concrete parameters of multinomial distribution for each benign input. This elaborated trigger ensures that our approach is invisible to both humans and statistical detection. Besides the design of the trigger, we also consider the robustness of our approach against model diagnose-based defences. We force the feature representation of malicious input stamped with the trigger to be entangled with the benign one. We demonstrate the effectiveness and robustness against multiple state-of-the-art defences through extensive datasets and networks. Our trigger only modifies less than 1\% pixels of a benign image while the modification magnitude is 1. Our source code is available at https://github.com/Ekko-zn/IJCAI2022-Backdoor.

----

## [242] Rainy WCity: A Real Rainfall Dataset with Diverse Conditions for Semantic Driving Scene Understanding

**Authors**: *Xian Zhong, Shidong Tu, Xianzheng Ma, Kui Jiang, Wenxin Huang, Zheng Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/243](https://doi.org/10.24963/ijcai.2022/243)

**Abstract**:

Scene understanding in adverse weather conditions (e.g. rainy and foggy days) has drawn increasing attention, arising some specific benchmarks and algorithms. However, scene segmentation under rainy weather is still challenging and under-explored due to the following limitations on the datasets and methods: 1) Manually synthetic rainy samples with empirically settings and human subjective assumptions; 2) Limited rainy conditions, including the rain patterns, intensity, and degradation factors; 3) Separated training manners for image deraining and semantic segmentation. To break these limitations, we pioneer a real, comprehensive, and well-annotated scene understanding dataset under rainy weather, named Rainy WCity. It covers various rain patterns and their bring-in negative visual effects, covering wiper, droplet, reflection, refraction, shadow, windshield-blurring, etc. In addition, to alleviate dependence on paired training samples, we design an unsupervised contrastive learning network for real image deraining and the final rainy scene semantic segmentation via multi-task joint optimization. A comprehensive comparison analysis is also provided, which shows that scene understanding in rainy weather is a largely open problem. Finally, we summarize our general observations, identify open research challenges, and point out future directions.

----

## [243] HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control

**Authors**: *Feida Zhu, Junwei Zhu, Wenqing Chu, Ying Tai, Zhifeng Xie, Xiaoming Huang, Chengjie Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/244](https://doi.org/10.24963/ijcai.2022/244)

**Abstract**:

We propose HifiHead, a high fidelity neural talking head synthesis method, which can well preserve the source image's appearance and control the motion (e.g., pose, expression, gaze) flexibly with 3D morphable face models (3DMMs) parameters derived from a driving image or indicated by users. Existing head synthesis works mainly focus on low-resolution inputs. Instead, we exploit the powerful generative prior embedded in StyleGAN to achieve high-quality head synthesis and editing. Specifically, we first extract the source image's appearance and driving image's motion to construct 3D face descriptors, which are employed as latent style codes for the generator. Meanwhile, hierarchical representations are extracted from the source and rendered 3D images respectively to provide faithful appearance and shape guidance. Considering the appearance representations need high-resolution flow fields for spatial transform, we propose a coarse-to-fine style-based generator consisting of a series of feature alignment and refinement (FAR) blocks. Each FAR block updates the dense flow fields and refines RGB outputs simultaneously for efficiency. Extensive experiments show that our method blends source appearance and target motion more accurately along with more photo-realistic results than previous state-of-the-art approaches.

----

## [244] Hierarchical Bilevel Learning with Architecture and Loss Search for Hadamard-based Image Restoration

**Authors**: *Guijing Zhu, Long Ma, Xin Fan, Risheng Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/245](https://doi.org/10.24963/ijcai.2022/245)

**Abstract**:

In the past few decades, Hadamard-based image restoration problems (e.g., low-light image enhancement) attract wide concerns in multiple areas related to artificial intelligence. However, existing works mostly focus on heuristically defining architecture and loss by the engineering experiences that came from extensive practices. This way brings about expensive verification costs for seeking out the optimal solution. To this end, we develop a novel hierarchical bilevel learning scheme to discover the architecture and loss simultaneously for different Hadamard-based image restoration tasks. 
  More concretely, we first establish a new Hadamard-inspired neural unit to aggregate domain knowledge into the network design. Then we model a triple-level optimization that consists of the architecture, loss and parameters optimizations to deliver a macro perspective for network learning. Then we introduce a new hierarchical bilevel learning scheme for solving the built triple-level model to progressively generate the desired architecture and loss. We also define an architecture search space consisting of a series of simple operations and an image quality-oriented loss search space. 
  Extensive experiments on three Hadamard-based image restoration tasks (including low-light image enhancement, single image haze removal and underwater image enhancement) fully verify our superiority against state-of-the-art methods.

----

## [245] A Solver + Gradient Descent Training Algorithm for Deep Neural Networks

**Authors**: *Dhananjay Ashok, Vineel Nagisetty, Christopher Srinivasa, Vijay Ganesh*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/246](https://doi.org/10.24963/ijcai.2022/246)

**Abstract**:

We present a novel hybrid algorithm for training Deep Neural Networks that combines the state-of-the-art Gradient Descent (GD) method with a Mixed Integer Linear Programming (MILP) solver, outperforming GD and variants in terms of accuracy, as well as resource and data efficiency for both regression and classification tasks. 
Our GD+Solver hybrid algorithm, called GDSolver, works as follows: given a DNN D as input, GDSolver invokes GD to partially train D until it gets stuck in a local minima, at which point GDSolver invokes an MILP solver to exhaustively search a region of the loss landscape around the weight assignments of Dâ€™s final layer parameters with the goal of tunnelling through and escaping the local minima. The process is repeated until desired accuracy is achieved. 
In our experiments, we find that GDSolver not only scales well to additional data and very large model sizes, but also outperforms all other competing methods in terms of rates of convergence and data efficiency. For regression tasks, GDSolver produced models that, on average, had 31.5% lower MSE in 48% less time, and for classification tasks on MNIST and CIFAR10, GDSolver was able to achieve the highest accuracy over all competing methods, using only 50% of the training data that GD baselines required.

----

## [246] Fine-grained Complexity of Partial Minimum Satisfiability

**Authors**: *Ivan Bliznets, Danil Sagunov, Kirill Simonov*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/247](https://doi.org/10.24963/ijcai.2022/247)

**Abstract**:

There is a well-known approach to cope with NP-hard problems in practice: reduce the given problem to SAT or MAXSAT and run a SAT or a MaxSAT solver. This method is very efficient since SAT/MaxSAT solvers are extremely well-studied, as well as the complexity of these problems. At AAAI 2011, Li et al. proposed an alternative to this approach and suggested the Partial Minimum Satisfiability problem as a reduction target for NP-hard problems. They developed the MinSatz solver and showed that reducing to Partial Minimum Satisfiability and using MinSatz is in some cases more efficient than reductions to SAT or MaxSAT. Since then many results connected to the Partial Minimum Satisfiability problem were published. However, to the best of our knowledge, the worst-case complexity of Partial Minimum Satisfiability has not been studied up until now.  Our goal is to fix the issue and show a O*((2-É›)^m) lower bound under the SETH assumption (here m is the total number of clauses), as well as several other lower bounds and parameterized exact algorithms with better-than-trivial running time.

----

## [247] QCDCL with Cube Learning or Pure Literal Elimination - What is Best?

**Authors**: *Benjamin Böhm, Tomás Peitl, Olaf Beyersdorff*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/248](https://doi.org/10.24963/ijcai.2022/248)

**Abstract**:

Quantified conflict-driven clause learning (QCDCL) is one of the main approaches for solving quantified Boolean formulas (QBF). We formalise and investigate several versions of QCDCL that include cube learning and/or pure-literal elimination, and formally compare the resulting solving models via proof complexity techniques. Our results show that almost all of the QCDCL models are exponentially incomparable with respect to proof size (and hence solver running time), pointing towards different orthogonal ways how to practically implement QCDCL.

----

## [248] Combining Constraint Solving and Bayesian Techniques for System Optimization

**Authors**: *Franz Brauße, Zurab Khasidashvili, Konstantin Korovin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/249](https://doi.org/10.24963/ijcai.2022/249)

**Abstract**:

Application domains of Bayesian optimization include optimizing black-box functions or very complex functions.
The functions we are interested in describe complex real-world systems applied in industrial settings.
Even though they do have explicit representations, standard optimization techniques fail to provide validated solutions and correctness guarantees for them.
In this paper we present a combination of Bayesian optimization and SMT-based constraint solving to achieve safe and stable solutions with optimality guarantees.

----

## [249] DPSampler: Exact Weighted Sampling Using Dynamic Programming

**Authors**: *Jeffrey M. Dudek, Aditya A. Shrotri, Moshe Y. Vardi*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/250](https://doi.org/10.24963/ijcai.2022/250)

**Abstract**:

The problem of exact weighted sampling of solutions of Boolean formulas has applications in Bayesian inference, testing, and verification. The state-of-the-art approach to sampling involves carefully decomposing the input formula and compiling a data structure called d-DNNF in the process. Recent work in the closely connected field of model counting, however, has shown that smartly composing different subformulas using dynamic programming and Algebraic Decision Diagrams (ADDs) can outperform d-DNNF-style approaches on many benchmarks. In this work, we present a modular algorithm called DPSampler that extends such dynamic-programming techniques to the problem of exact weighted sampling. 

DPSampler operates in three phases. First, an execution plan in the form of a project-join tree is computed using tree decompositions. Second, the plan is used to compile the input formula into a succinct tree-of-ADDs representation. Third, this tree is traversed to generate a random sample. This decoupling of planning, compilation and sampling phases enables usage of specialized libraries for each purpose in a black-box fashion. Further, our novel ADD-sampling algorithm avoids the need for expensive dynamic memory allocation required in previous work. Extensive experiments over diverse sets of benchmarks show DPSampler is more scalable and versatile than existing approaches.

----

## [250] A Multivariate Complexity Analysis of Qualitative Reasoning Problems

**Authors**: *Leif Eriksson, Victor Lagerkvist*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/251](https://doi.org/10.24963/ijcai.2022/251)

**Abstract**:

Qualitative reasoning is an important subfield of artificial intelligence where one describes relationships with qualitative, rather than numerical, relations. Many such reasoning tasks, e.g., Allen's interval algebra, can be solved in 2^O(n*log n) time, but single-exponential running times 2^O(n) are currently far out of reach. 
In this paper we consider single-exponential algorithms via a multivariate analysis consisting of a fine-grained parameter n (e.g., the number of variables) and a coarse-grained parameter k expected to be relatively small.
We introduce the classes FPE and XE of problems solvable in f(k)*2^O(n), respectively f(k)^n, time, and prove several fundamental properties of these classes. We proceed by studying temporal reasoning problems and (1) show that the partially ordered time problem of effective width k is solvable in 16^{kn} time and is thus included in XE, and (2) that the network consistency problem for Allen's interval algebra with no interval overlapping with more than k others is solvable in (2nk)^{2k}*2^n time and is included in FPE. Our multivariate approach is in no way limited to these to specific problems and may be a generally useful approach for obtaining single-exponential algorithms.

----

## [251] Accelerated Multiplicative Weights Update Avoids Saddle Points Almost Always

**Authors**: *Yi Feng, Ioannis Panageas, Xiao Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/252](https://doi.org/10.24963/ijcai.2022/252)

**Abstract**:

We consider nonconvex optimization problem with constraint that is a product of simplices. A commonly used algorithm in solving this type of problem is the Multiplicative Weights Update (MWU), an algorithm that is widely used in game theory, machine learning and multi agent systems. Despite it has been known that MWU avoids saddle points, there is a question that remains unaddressed:  ``Is there an accelerated version of MWU that avoids saddle points provably?'' In this paper we provide a positive answer to above question. We provide an accelerated MWU based on Riemannian Accelerated Gradient Descent, and prove that the Riemannian Accelerated Gradient Descent, thus the accelerated MWU, avoid saddle points.

----

## [252] Large Neighbourhood Search for Anytime MaxSAT Solving

**Authors**: *Randy Hickey, Fahiem Bacchus*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/253](https://doi.org/10.24963/ijcai.2022/253)

**Abstract**:

Large Neighbourhood Search (LNS) is an algorithmic framework for optimization problems that can yield good performance in many domains. In this paper, we present a method for applying LNS to improve anytime maximum satisfiability (MaxSAT) solving by introducing a neighbourhood selection policy that shows good empirical performance. We show that our LNS solver can often improve the suboptimal solutions produced by other anytime MaxSAT solvers. When starting with a suboptimal solution of reasonable quality, our approach often finds a better solution than the original anytime solver can achieve. We demonstrate that implementing our LNS solver on top of three different state-of-the-art anytime solvers improves the anytime performance of all three solvers within the standard time limit used in the incomplete tracks of the annual MaxSAT Evaluation.

----

## [253] Online Matching with Controllable Rewards and Arrival Probabilities

**Authors**: *Yuya Hikima, Yasunori Akagi, Naoki Marumo, Hideaki Kim*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/254](https://doi.org/10.24963/ijcai.2022/254)

**Abstract**:

Online bipartite matching has attracted much attention due to its importance in various applications such as advertising, ride-sharing, and crowdsourcing. In most online matching problems, the rewards and node arrival probabilities are given in advance and are not controllable. However, many real-world matching services require them to be controllable  and the decision-maker faces a non-trivial problem of optimizing them. In this study, we formulate a new optimization problem, Online Matching with Controllable Rewards and Arrival probabilities (OM-CRA), to simultaneously determine not only the matching strategy but also the rewards and arrival probabilities. Even though our problem is more complex than the existing ones, we propose a fast 1/2-approximation algorithm for OM-CRA. The proposed approach transforms OM-CRA to a saddle-point problem by approximating the objective function, and then solves it by the Primal-Dual Hybrid Gradient (PDHG) method with acceleration through the use of the problem structure. In simulations on real data from crowdsourcing and ride-sharing platforms, we show that the proposed algorithm can find solutions with high total rewards in practical times.

----

## [254] Encoding Probabilistic Graphical Models into Stochastic Boolean Satisfiability

**Authors**: *Cheng-Han Hsieh, Jie-Hong R. Jiang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/255](https://doi.org/10.24963/ijcai.2022/255)

**Abstract**:

Statistical inference is a powerful technique in various applications. Although many statistical inference tools are available, answering inference queries involving complex quantification structures remains challenging. Recently, solvers for Stochastic Boolean Satisfiability (SSAT), a powerful formalism allowing concise encodings of PSPACE decision problems under uncertainty, are under active development and applied in more and more applications. In this work, we exploit SSAT solvers for the inference of Probabilistic Graphical Models (PGMs), an essential representation for probabilistic reasoning. Specifically, we develop encoding methods to systematically convert PGM inference problems into SSAT formulas for effective solving. Experimental results demonstrate that, by using our encoding, SSAT-based solving can complement existing PGM tools, especially in answering complex queries.

----

## [255] Degradation Accordant Plug-and-Play for Low-Rank Tensor Completion

**Authors**: *Yexun Hu, Tai-Xiang Jiang, Xi-Le Zhao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/256](https://doi.org/10.24963/ijcai.2022/256)

**Abstract**:

Tensor completion aims at estimating missing values from an incomplete observation, playing a fundamental role for many applications. This work proposes a novel low-rank tensor completion model, in which the inherent low-rank prior and external degradation accordant data-driven prior are simultaneously utilized. Specifically, the tensor nuclear norm (TNN) is adopted to characterize the overall low-dimensionality of the tensor data. Meanwhile, an implicit regularizer is formulated and its related subproblem is solved via a deep convolutional neural network (CNN) under the plug-and-play framework. This CNN, pretrained for the inpainting task on a mass of natural images, is expected to express the external data-driven prior and this plugged inpainter is consistent with the original degradation process. Then, an efficient alternating direction method of multipliers (ADMM) is designed to solve the proposed optimization model. Extensive experiments are conducted on different types of tensor imaging data with the comparison with state-of-the-art methods,  illustrating the effectiveness and the remarkable generalization ability of our method.

----

## [256] Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies

**Authors**: *Mikhail Kazdagli, Mohit Tiwari, Akshat Kumar*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/257](https://doi.org/10.24963/ijcai.2022/257)

**Abstract**:

Modern software systems rely on mining insights from business sensitive data stored in public clouds. A data breach usually incurs significant (monetary) loss for a commercial organization. Conceptually, cloud security heavily relies on Identity Access Management (IAM) policies that IT admins need to properly configure and periodically update. Security negligence and human errors often lead to misconfiguring IAM policies which may open a backdoor for attackers. To address these challenges, first, we develop a novel framework that encodes generating optimal IAM policies using constraint programming (CP). We identify reducing dormant permissions of cloud users as an optimality criterion, which intuitively implies minimizing unnecessary datastore access permissions. Second, to make IAM policies interpretable, we use graph representation learning applied to historical access patterns of users to augment our CP model with similarity constraints: similar users should be grouped together and share common IAM policies. Third, we describe multiple attack models and show that our optimized IAM policies significantly reduce the impact of security attacks using real data from 8 commercial organizations, and synthetic instances.

----

## [257] Best Heuristic Identification for Constraint Satisfaction

**Authors**: *Frédéric Koriche, Christophe Lecoutre, Anastasia Paparrizou, Hugues Wattez*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/258](https://doi.org/10.24963/ijcai.2022/258)

**Abstract**:

In constraint satisfaction problems, the variable ordering heuristic takes a central place by selecting the variables to branch on during backtrack search. As many hand-crafted branching heuristics have been proposed in the literature, a key issue is to identify, from a pool of candidate heuristics, which one is the best for solving a given constraint satisfaction task.  Based on the observation that modern constraint solvers are using restart sequences, the best heuristic identification problem can be cast in the context of multi-armed bandits as a non-stochastic best arm identification problem. Namely, during each run of some given restart sequence, the bandit algorithm selects a branching heuristic and receives a reward for this heuristic before proceeding to the next run. The goal is to identify the best heuristic using few runs, and without any stochastic assumption about the constraint solver. In this study, we propose an adaptive variant of Successive Halving that exploits Luby's universal restart sequence. We analyze the convergence of this bandit algorithm in the non-stochastic setting, 
and we demonstrate its empirical effectiveness on various constraint satisfaction benchmarks.

----

## [258] AllSATCC: Boosting AllSAT Solving with Efficient Component Analysis

**Authors**: *Jiaxin Liang, Feifei Ma, Junping Zhou, Minghao Yin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/259](https://doi.org/10.24963/ijcai.2022/259)

**Abstract**:

All Solution SAT (AllSAT) is a variant of Propositional Satisfiability, which aims to find all satisfying assignments for a given formula. AllSAT has significant applications in different domains, such as software testing, data mining, and network verification. In this paper, observing that the lack of component analysis may result in more work for algorithms with non-chronological backtracking, we propose a DPLL-based algorithm for solving AllSAT problem, named AllSATCC, which takes advantage of component analysis to reduce work repetition caused by non-chronological backtracking. The experimental results show that our algorithm outperforms the state-of-the-art algorithms on most instances.

----

## [259] Automated Program Analysis: Revisiting Precondition Inference through Constraint Acquisition

**Authors**: *Grégoire Menguy, Sébastien Bardin, Nadjib Lazaar, Arnaud Gotlieb*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/260](https://doi.org/10.24963/ijcai.2022/260)

**Abstract**:

Program annotations under the form of function pre/postconditions are crucial for many software engineering and program verification applications. Unfortunately, such annotations are rarely available and must be retrofit by hand. In this paper, we explore how Constraint Acquisition (CA), a learning framework from Constraint Programming, can be leveraged to automatically infer	program preconditions in a black-box manner, from input-output observations. We propose PreCA, the first ever framework based on active constraint acquisition dedicated to infer memory-related preconditions. PreCA overpasses prior techniques based on program analysis and formal methods, offering well-identified guarantees and returning more precise results in practice.

----

## [260] Threshold-free Pattern Mining Meets Multi-Objective Optimization: Application to Association Rules

**Authors**: *Charles Vernerey, Samir Loudni, Noureddine Aribi, Yahia Lebbah*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/261](https://doi.org/10.24963/ijcai.2022/261)

**Abstract**:

Constraint-based pattern mining is at the core of numerous data mining tasks. Unfortunately, thresholds which are involved in these constraints cannot be easily chosen. This paper investigates a Multi-objective Optimization approach where several (often conflicting) functions need to be optimized at the same time. We introduce a new model for efficiently mining Pareto optimal patterns with constraint programming. Our model exploits condensed pattern representations to reduce the mining effort. To this end, we design a new  global constraint for ensuring the closeness of patterns over a set of measures. We show how our approach can be applied to derive high-quality non redundant association rules without the use of thresholds whose added-value is studied on both UCI datasets and case study related to the analysis of genes expression data integrating multiple external genes annotations.

----

## [261] An Exact MaxSAT Algorithm: Further Observations and Further Improvements

**Authors**: *Mingyu Xiao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/262](https://doi.org/10.24963/ijcai.2022/262)

**Abstract**:

In the maximum satisfiability problem (MaxSAT), given a CNF formula with m clauses and n variables, we are asked to find an assignment of the variables to satisfy the maximum number of clauses. Chen and Kanj showed that this problem can be solved in O*(1.3248^m) time (DAM 2004) and the running time bound was improved to O*(1.2989^m) by Xu et al. (IJCAI 2019). In this paper, we further improve the result to O*(1.2886^m). By using some new reduction and branching techniques we can avoid several bottlenecks in previous algorithms and get the improvement on this important problem.

----

## [262] Inverting 43-step MD4 via Cube-and-Conquer

**Authors**: *Oleg Zaikin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/263](https://doi.org/10.24963/ijcai.2022/263)

**Abstract**:

MD4 is a prominent cryptographic hash function proposed in 1990. The full version consists of 48 steps and produces a hash of size 128 bits given a message of an arbitrary finite size. In 2007, its truncated 39-step version was inverted via reducing to SAT and applying a CDCL solver. Since that time, several attempts have been made but the 40-step version still remains unbroken. In this study, 40-, 41-, 42-, and 43-step versions of MD4 are successfully inverted. The problems are reduced to SAT and solved via the Cube-and-Conquer approach. Two algorithms are proposed for this purpose. The first one generates inversion problems for MD4 by adding special constraints. The second one is aimed at finding a proper threshold for the cubing phase of Cube-and-Conquer. While the first algorithm is focused on inverting MD4 and similar cryptographic hash functions, the second one is not area specific and so is applicable to a variety of classes of hard SAT instances.

----

## [263] BandMaxSAT: A Local Search MaxSAT Solver with Multi-armed Bandit

**Authors**: *Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, Chu-Min Li, Felip Manyà*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/264](https://doi.org/10.24963/ijcai.2022/264)

**Abstract**:

We address Partial MaxSAT (PMS) and Weighted PMS (WPMS), two practical generalizations of the MaxSAT problem, and propose a local search algorithm called BandMaxSAT, that applies a multi-armed bandit to guide the search direction, for these problems. The bandit in our method is associated with all the soft clauses in the input (W)PMS instance. Each arm corresponds to a soft clause. The bandit model can help BandMaxSAT to select a good direction to escape from local optima by selecting a soft clause to be satisfied in the current step, that is, selecting an arm to be pulled. We further propose an initialization method for (W)PMS that prioritizes both unit and binary clauses when producing the initial solutions. Extensive experiments demonstrate that BandMaxSAT significantly outperforms the state-of-the-art (W)PMS local search algorithm SATLike3.0. Specifically, the number of instances in which BandMaxSAT obtains better results is about twice that obtained by SATLike3.0. We further combine BandMaxSAT with the complete solver TT-Open-WBO-Inc. The resulting solver BandMaxSAT-c also outperforms some of the best state-of-the-art complete (W)PMS solvers, including SATLike-c, Loandra and TT-Open-WBO-Inc.

----

## [264] A Strengthened Branch and Bound Algorithm for the Maximum Common (Connected) Subgraph Problem

**Authors**: *Jianrong Zhou, Kun He, Jiongzhi Zheng, Chu-Min Li, Yanli Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/265](https://doi.org/10.24963/ijcai.2022/265)

**Abstract**:

We propose a new and strengthened Branch-and-Bound (BnB) algorithm for the maximum common (connected) induced subgraph problem based on two new operators, Long-Short Memory (LSM) and Leaf vertex Union Match (LUM). Given two graphs for which we search for the maximum common (connected) induced subgraph, the first operator of LSM maintains a score for the branching node using the short-term reward of each vertex of the first graph and the long-term reward of each vertex pair of the two graphs. In this way, the BnB process learns to reduce the search tree size significantly and boost the algorithm performance. The second operator of LUM further improves the performance by simultaneously matching the leaf vertices connected to the current matched vertices, and allows the algorithm to match multiple vertex pairs without affecting the optimality of solution. We incorporate the two operators into the state-of-the-art BnB algorithm McSplit, and denote the resulting algorithm as McSplit+LL. Experiments show that McSplit+LL outperforms McSplit+RL, a more recent variant of McSplit using reinforcement learning that is superior than McSplit.

----

## [265] Doubly Sparse Asynchronous Learning for Stochastic Composite Optimization

**Authors**: *Runxue Bao, Xidong Wu, Wenhan Xian, Heng Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/266](https://doi.org/10.24963/ijcai.2022/266)

**Abstract**:

Parallel optimization has become popular for large-scale learning in the past decades. However, existing methods suffer from huge computational costs, memory usage, and communication burden in high-dimensional scenarios. To address the challenges, we propose a new accelerated doubly sparse asynchronous learning (DSAL) method for stochastic composite optimization, under which two algorithms are proposed on shared-memory and distributed-memory architecture respectively, which only conducts gradient descent on the nonzero coordinates (data sparsity) and active set (model sparsity). The proposed algorithm can converge much faster and achieve significant speedup by simultaneously enjoying the sparsity of the model and data. Moreover, by sending the gradients on the active set only, communication costs are dramatically reduced. Theoretically, we prove that the proposed method achieves the linear convergence rate with lower overall complexity and can achieve the model identification in a finite number of iterations almost surely. Finally, extensive experimental results on benchmark datasets confirm the superiority of our proposed method.

----

## [266] Hypergraph Structure Learning for Hypergraph Neural Networks

**Authors**: *Derun Cai, Moxian Song, Chenxi Sun, Baofeng Zhang, Shenda Hong, Hongyan Li*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/267](https://doi.org/10.24963/ijcai.2022/267)

**Abstract**:

Hypergraphs are natural and expressive modeling tools to encode high-order relationships among entities. Several variations of Hypergraph Neural Networks (HGNNs) are proposed to learn the node representations and complex relationships in the hypergraphs. Most current approaches assume that the input hypergraph structure accurately depicts the relations in the hypergraphs. However, the input hypergraph structure inevitably contains noise, task-irrelevant information, or false-negative connections.
Treating the input hypergraph structure as ground-truth information unavoidably leads to sub-optimal performance. In this paper, we propose a Hypergraph Structure Learning (HSL) framework, which optimizes the hypergraph structure and the HGNNs simultaneously in an end-to-end way. 
HSL learns an informative and concise hypergraph structure that is optimized for downstream tasks. To efficiently learn the hypergraph structure, HSL adopts a two-stage sampling process: hyperedge sampling for pruning redundant hyperedges and incident node sampling for pruning irrelevant incident nodes and discovering potential implicit connections. 
The consistency between the optimized structure and the original structure is maintained by the intra-hyperedge contrastive learning module.
The sampling processes are jointly optimized with HGNNs towards the objective of the downstream tasks. Experiments conducted on 7 datasets show shat HSL outperforms the state-of-the-art baselines while adaptively sparsifying hypergraph structures.

----

## [267] Entity Alignment with Reliable Path Reasoning and Relation-aware Heterogeneous Graph Transformer

**Authors**: *Weishan Cai, Wenjun Ma, Jieyu Zhan, Yuncheng Jiang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/268](https://doi.org/10.24963/ijcai.2022/268)

**Abstract**:

Entity Alignment (EA) has attracted widespread attention in both academia and industry, which aims to seek entities with same meanings from different Knowledge Graphs (KGs). There are substantial multi-step relation paths between entities in KGs, indicating the semantic relations of entities. However, existing methods rarely consider path information because not all natural paths facilitate for EA judgment. In this paper, we propose a more effective entity alignment framework, RPR-RHGT, which integrates relation and path structure information, as well as the heterogeneous information in KGs. Impressively, an initial reliable path reasoning algorithm is developed to generate the paths favorable for EA task from the relation structures of KGs. This is the first algorithm in the literature to successfully use unrestricted path information. In addition, to efficiently capture heterogeneous features in entity neighborhoods, a relation-aware heterogeneous graph transformer is designed to model the relation and path structures of KGs. Extensive experiments on three well-known datasets show RPR-RHGT significantly outperforms 10 state-of-the-art methods, exceeding the best performing baseline up to 8.62% on Hits@1. We also show its better performance than the baselines on different ratios of training set, and harder datasets.

----

## [268] Non-Euclidean Self-Organizing Maps

**Authors**: *Dorota Celinska-Kopczynska, Eryk Kopczynski*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/269](https://doi.org/10.24963/ijcai.2022/269)

**Abstract**:

Self-Organizing Maps (SOMs, Kohonen networks) belong to neural network models of the unsupervised class. In this paper, we present the generalized setup for non-Euclidean SOMs. Most data analysts take it for granted to use some subregions of a flat space as their data model; however, by the assumption that the underlying geometry is non-Euclidean we obtain a new degree of freedom for the techniques that translate the similarities into spatial neighborhood relationships. We improve the traditional SOM algorithm by introducing topology-related extensions. Our proposition can be successfully applied to dimension reduction, clustering or finding similarities in big data (both hierarchical and non-hierarchical).

----

## [269] Can Abnormality be Detected by Graph Neural Networks?

**Authors**: *Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, Weihao Jiang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/270](https://doi.org/10.24963/ijcai.2022/270)

**Abstract**:

Anomaly detection in graphs has attracted considerable interests in both academia and industry due to its wide applications in numerous domains ranging from finance to biology. Meanwhile, graph neural networks (GNNs) is emerging as a powerful tool for modeling graph data. A natural and fundamental question that arises here is: can abnormality be detected by graph neural networks?

In this paper, we aim to answer this question, which is nontrivial. As many existing works have explored, graph neural networks can be seen as filters for graph signals, with the favor of low frequency in graphs. In other words, GNN will smooth the signals of adjacent nodes. However, abnormality in a graph intuitively has the characteristic that it tends to be dissimilar to its neighbors, which are mostly normal samples. It thereby conflicts with the general assumption with traditional GNNs. To solve this, we propose a novel Adaptive Multi-frequency Graph Neural Network (AMNet), aiming to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. Experimental  results on real-world datasets demonstrate that our model achieves a significant improvement comparing with several state-of-the-art baseline methods.

----

## [270] Robust High-Dimensional Classification From Few Positive Examples

**Authors**: *Deepayan Chakrabarti, Benjamin Fauber*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/271](https://doi.org/10.24963/ijcai.2022/271)

**Abstract**:

We tackle an extreme form of imbalanced classification, with up to 105 features but as few as 5 samples from the minority class. This problem occurs in predicting predicting tumor types and fraud detection, among others. Standard imbalanced classification methods are not designed for such severe data scarcity. Sampling-based methods need too many samples due to the high-dimensionality, while cost-based methods must place too high a weight on the limited minority samples. Our proposed method, called DIRECT, bypasses sample generation by training the classifier over a robust smoothed distribution of the minority class. DIRECT is fast, simple, robust, parameter-free, and easy to interpret. We validate DIRECT on several real-world datasets spanning document, image, and medical classification. DIRECT is up to 5x − 7x better than SMOTE-like methods, 30−200% better than ensemble methods, 3x − 7x better than cost-sensitive methods. The greatest gains are for settings with the fewest samples in the minority class, where DIRECT’s robustness is most helpful.

----

## [271] Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification

**Authors**: *Chaochao Chen, Jun Zhou, Longfei Zheng, Huiwen Wu, Lingjuan Lyu, Jia Wu, Bingzhe Wu, Ziqi Liu, Li Wang, Xiaolin Zheng*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/272](https://doi.org/10.24963/ijcai.2022/272)

**Abstract**:

Recently, Graph Neural Network (GNN) has achieved remarkable progresses in various real-world tasks on graph data, consisting of node features and the adjacent information between different nodes. High-performance GNN models always depend on both rich features and complete edge information in graph. However, such information could possibly be isolated by different data holders in practice, which is the so-called data isolation problem. To solve this problem, in this paper, we propose VFGNN, a federated GNN learning paradigm for privacy-preserving node classification task under data vertically partitioned setting, which can be generalized to existing GNN models. Specifically, we split the computation graph into two parts. We leave the private data (i.e., features, edges, and labels) related computations on data holders, and delegate the rest of computations to a semi-honest server. We also propose to apply differential privacy to prevent potential information leakage from the server. We conduct experiments on three benchmarks and the results demonstrate the effectiveness of VFGNN.

----

## [272] Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting

**Authors**: *Mingyang Chen, Wen Zhang, Zhen Yao, Xiangnan Chen, Mengxiao Ding, Fei Huang, Huajun Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/273](https://doi.org/10.24963/ijcai.2022/273)

**Abstract**:

We study the knowledge extrapolation problem to embed new components (i.e., entities and relations) that come with emerging knowledge graphs (KGs) in the federated setting. In this problem, a model trained on an existing KG needs to embed an emerging KG with unseen entities and relations. To solve this problem, we introduce the meta-learning setting, where a set of tasks are sampled on the existing KG to mimic the link prediction task on the emerging KG. Based on sampled tasks, we meta-train a graph neural network framework that can construct features for unseen components based on structural information and output embeddings for them. Experimental results show that our proposed method can effectively embed unseen components and outperforms models that consider inductive settings for KGs and baselines that directly use conventional KG embedding methods.

----

## [273] Mutual Distillation Learning Network for Trajectory-User Linking

**Authors**: *Wei Chen, Shuzhe Li, Chao Huang, Yanwei Yu, Yongguo Jiang, Junyu Dong*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/274](https://doi.org/10.24963/ijcai.2022/274)

**Abstract**:

Trajectory-User Linking (TUL), which links trajectories to users who generate them, has been a challenging problem due to the sparsity in check-in mobility data. Existing methods ignore the utilization of historical data or rich contextual features in check-in data, resulting in poor performance for TUL task. 
In this paper, we propose a novel Mutual distillation learning network to solve the TUL problem for sparse check-in mobility data, named MainTUL.  
Specifically, MainTUL is composed of a Recurrent Neural Network (RNN) trajectory encoder that models sequential patterns of input trajectory and a temporal-aware Transformer trajectory encoder that captures long-term time dependencies for the corresponding augmented historical trajectories. 
Then, the knowledge learned on historical trajectories is transferred between the two trajectory encoders to guide the learning of both encoders to achieve mutual distillation of information. Experimental results on two real-world check-in mobility datasets demonstrate the superiority of \model against state-of-the-art baselines. The source code of our model is available at https://github.com/Onedean/MainTUL.

----

## [274] Towards Robust Dense Retrieval via Local Ranking Alignment

**Authors**: *Xuanang Chen, Jian Luo, Ben He, Le Sun, Yingfei Sun*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/275](https://doi.org/10.24963/ijcai.2022/275)

**Abstract**:

Dense retrieval (DR) has extended the employment of pre-trained language models, like BERT, for text ranking. However, recent studies have raised the robustness issue of DR model against query variations, like query with typos, along with non-trivial performance losses. Herein, we argue that it would be beneficial to allow the DR model to learn to align the relative positions of query-passage pairs in the representation space, as query variations cause the query vector to drift away from its original position, affecting the subsequent DR effectiveness. To this end, we propose RoDR, a novel robust DR model that learns to calibrate the in-batch local ranking of query variation to that of original query for the DR space alignment. Extensive experiments on MS MARCO and ANTIQUE datasets show that RoDR significantly improves the retrieval results on both the original queries and different types of query variations. Meanwhile, RoDR provides a general query noise-tolerate learning framework that boosts the robustness and effectiveness of various existing DR models. Our code and models are openly available at https://github.com/cxa-unique/RoDR.

----

## [275] Filtration-Enhanced Graph Transformation

**Authors**: *Zijian Chen, Rong-Hua Li, Hongchao Qin, Huanzhong Duan, Yanxiong Lu, Qiangqiang Dai, Guoren Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/276](https://doi.org/10.24963/ijcai.2022/276)

**Abstract**:

Graph kernels and graph neural networks (GNNs) are widely used for the classification of graph data. However, many existing graph kernels and GNNs have limited expressive power, because they cannot distinguish graphs if the classic 1-dimensional Weisfeiler-Leman (1-WL) algorithm does not distinguish them. To break the 1-WL expressiveness barrier, we propose a novel method called filtration-enhanced graph transformation, which is based on a concept from the area of topological data analysis. In a nutshell, our approach first transforms each original graph into a filtration-enhanced graph based on a certain pre-defined filtration operation, and then uses the transformed graphs as the inputs for graph kernels or GNNs. The striking feature of our approach is that it is a plug-in method and can be applied in any graph kernel and GNN to enhance their expressive power. We theoretically and experimentally demonstrate that our solutions exhibit significantly better performance than the state-of-the art solutions for graph classification tasks.

----

## [276] Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting

**Authors**: *Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, Shirui Pan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/277](https://doi.org/10.24963/ijcai.2022/277)

**Abstract**:

A variety of real-world applications rely on far future information to make decisions, thus calling for efficient and accurate long sequence multivariate time series forecasting. While recent attention-based forecasting models show strong abilities in
capturing long-term dependencies, they still suffer from two key limitations. First, canonical self attention has a quadratic complexity w.r.t. the input time series length, thus falling short in efficiency. Second, different variables’ time series often have
distinct temporal dynamics, which existing studies fail to capture, as they use the same model parameter space, e.g., projection matrices, for all variables’ time series, thus falling short in accuracy. To ensure high efficiency and accuracy, we propose Triformer, a triangular, variable-specific attention. (i) Linear complexity: we introduce a novel patch attention with linear complexity. When stacking multiple layers of the patch attentions, a triangular structure is proposed such that the
layer sizes shrink exponentially, thus maintaining linear complexity. (ii) Variable-specific parameters: we propose a light-weight method to enable distinct sets of model parameters for different variables’ time series to enhance accuracy
without compromising efficiency and memory usage. Strong empirical evidence on four datasets from multiple domains justifies our design choices, and it demonstrates that Triformer outperforms state-of-the-art methods w.r.t. both accuracy and
efficiency. Source code is publicly available at https://github.com/razvanc92/triformer.

----

## [277] CADET: Calibrated Anomaly Detection for Mitigating Hardness Bias

**Authors**: *Ailin Deng, Adam Goodge, Lang Yi Ang, Bryan Hooi*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/278](https://doi.org/10.24963/ijcai.2022/278)

**Abstract**:

The detection of anomalous samples in large, high-dimensional datasets is a challenging task with numerous practical applications. Recently, state-of-the-art performance is achieved with deep learning methods: for example, using the reconstruction error from an autoencoder as anomaly scores. However, the scores are uncalibrated: that is, they follow an unknown distribution and lack a clear interpretation. Furthermore, the reconstruction error is highly influenced by the `hardness' of a given sample, which leads to false negative and false positive errors. In this paper, we empirically show the significance of this hardness bias present in a range of recent deep anomaly detection methods. To mitigate this, we propose an efficient and plug-and-play error calibration method which mitigates this hardness bias in the anomaly scoring without the need to retrain the model. We verify the effectiveness of our method on a range of image, time-series, and tabular datasets and against several baseline methods.

----

## [278] Private Semi-Supervised Federated Learning

**Authors**: *Chenyou Fan, Junjie Hu, Jianwei Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/279](https://doi.org/10.24963/ijcai.2022/279)

**Abstract**:

We study a federated learning (FL) framework to effectively train models from scarce and skewly distributed labeled data. We consider a challenging yet practical scenario: a few data sources own a  small amount of labeled data, while the rest mass sources own purely unlabeled data. Classical FL requires each client to have enough labeled data for local training, thus is not applicable in this scenario. In this work, we design an effective federated semi-supervised learning framework (FedSSL) to fully leverage both labeled and unlabeled data sources. We establish a unified data space across all participating agents, so that each agent can generate mixed data samples to boost semi-supervised learning (SSL), while keeping data locality. We further show that FedSSL can integrate differential privacy protection techniques to prevent labeled data leakage at the cost of minimum performance degradation. On SSL tasks with as small as 0.17% and 1% of MNIST and CIFAR-10 datasets as labeled data, respectively, our approach can achieve 5-20% performance boost over the state-of-the-art methods.

----

## [279] Feature and Instance Joint Selection: A Reinforcement Learning Perspective

**Authors**: *Wei Fan, Kunpeng Liu, Hao Liu, Hengshu Zhu, Hui Xiong, Yanjie Fu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/280](https://doi.org/10.24963/ijcai.2022/280)

**Abstract**:

Feature selection and instance selection are two important techniques of data processing. However, such selections have mostly been studied separately, while existing work towards the joint selection conducts feature/instance selection coarsely; thus neglecting the latent fine-grained interaction between feature space and instance space. To address this challenge, we propose a reinforcement learning solution to accomplish the joint selection task and simultaneously capture the interaction between the selection of each feature and each instance. In particular, a sequential-scanning mechanism is designed as action strategy of agents and a collaborative-changing environment is used to enhance agent collaboration. In addition, an interactive paradigm introduces prior selection knowledge to help agents for more efficient exploration. Finally, extensive experiments on real-world datasets have demonstrated improved performances.

----

## [280] MetaER-TTE: An Adaptive Meta-learning Model for En Route Travel Time Estimation

**Authors**: *Yu Fan, Jiajie Xu, Rui Zhou, Jianxin Li, Kai Zheng, Lu Chen, Chengfei Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/281](https://doi.org/10.24963/ijcai.2022/281)

**Abstract**:

En route travel time estimation (ER-TTE) aims to predict the travel time on the remaining route. Since the traveled and remaining parts of a trip usually have some common characteristics like driving speed, it is desirable to explore these characteristics for improved performance via effective adaptation. This yet faces the severe problem of data sparsity due to the few sampled points in a traveled partial trajectory. Since trajectories with different contextual information tend to have different characteristics, the existing meta-learning method for ER-TTE cannot fit each trajectory well because it uses the same model for all trajectories. To this end, we propose a novel adaptive meta-learning model called MetaER-TTE. Particularly, we utilize soft-clustering and derive cluster-aware initialized parameters to better transfer the shared knowledge across trajectories with similar contextual information. In addition, we adopt a distribution-aware approach for adaptive learning rate optimization, so as to avoid task-overfitting which will occur when guiding the initial parameters with a fixed learning rate for tasks under imbalanced distribution. Finally, we conduct comprehensive experiments to demonstrate the superiority of MetaER-TTE.

----

## [281] When Transfer Learning Meets Cross-City Urban Flow Prediction: Spatio-Temporal Adaptation Matters

**Authors**: *Ziquan Fang, Dongen Wu, Lu Pan, Lu Chen, Yunjun Gao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/282](https://doi.org/10.24963/ijcai.2022/282)

**Abstract**:

Urban flow prediction is a fundamental task to build smart cities, where neural networks have become the most popular method. However, the deep learning methods typically rely on massive training data that are probably inaccessible in real world. In light of this, the community calls for knowledge transfer. However, when adapting transfer learning for cross-city prediction tasks, existing studies are built on static knowledge transfer, ignoring the fact inter-city correlations change dynamically across time. The dynamic correlations make urban feature transfer challenging. This paper proposes a novel Spatio-Temporal Adaptation Network (STAN) to perform urban flow prediction for data-scarce cities via the spatio-temporal knowledge transferred from data-rich cities. STAN encompasses three modules: i) spatial adversarial adaptation module that adopts an adversarial manner to capture the transferable spatial features; ii) temporal attentive adaptation module to attend to critical dynamics for temporal feature transfer; iii) prediction module that aims to learn task-driven transferable knowledge. Extensive experiments on five real datasets show STAN substantially outperforms state-of-the-art methods.

----

## [282] Disentangling the Computational Complexity of Network Untangling

**Authors**: *Vincent Froese, Pascal Kunz, Philipp Zschoche*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/283](https://doi.org/10.24963/ijcai.2022/283)

**Abstract**:

We study the recently introduced network untangling problem, a variant of Vertex Cover on temporal graphs---graphs whose edge set changes over discrete time steps. There are two versions of this problem. The goal is to select at most k time intervals for each vertex such that all time-edges are covered and (depending on the problem variant) either the maximum interval length or the total sum of interval lengths is minimized. This problem has data mining applications in finding activity timelines that explain the interactions of entities in complex networks.

Both variants of the problem are NP-hard. In this paper, we initiate a multivariate complexity analysis involving the following parameters: number of vertices, lifetime of the temporal graph, number of intervals per vertex, and the interval length bound. For both problem versions, we (almost) completely settle the parameterized complexity for all combinations of those four parameters, thereby delineating the border of fixed-parameter tractability.

----

## [283] Modeling Precursors for Temporal Knowledge Graph Reasoning via Auto-encoder Structure

**Authors**: *Yifu Gao, Linhui Feng, Zhigang Kan, Yi Han, Linbo Qiao, Dongsheng Li*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/284](https://doi.org/10.24963/ijcai.2022/284)

**Abstract**:

Temporal knowledge graph (TKG) reasoning that infers missing facts in the future is an essential and challenging task. When predicting a future event, there must be a narrative evolutionary process composed of closely related historical facts to support the event's occurrence, namely fact precursors. However, most existing models employ a sequential reasoning process in an auto-regressive manner, which cannot capture precursor information. This paper proposes a novel auto-encoder architecture that introduces a relation-aware graph attention layer into transformer (rGalT) to accommodate inference over the TKG. Specifically, we first calculate the correlation between historical and predicted facts through multiple attention mechanisms along intra-graph and inter-graph dimensions, then constitute these mutually related facts into diverse fact segments. Next, we borrow the translation generation idea to decode in parallel the precursor information associated with the given query, which enables our model to infer future unknown facts by progressively generating graph structures. Experimental results on four benchmark datasets demonstrate that our model outperforms other state-of-the-art methods, and precursor identiﬁcation provides supporting evidence for prediction.

----

## [284] Self-supervised Graph Neural Networks for Multi-behavior Recommendation

**Authors**: *Shuyun Gu, Xiao Wang, Chuan Shi, Ding Xiao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/285](https://doi.org/10.24963/ijcai.2022/285)

**Abstract**:

Traditional recommendation usually focuses on utilizing only one target user behavior (e.g., purchase) but ignoring other auxiliary behaviors (e.g., click, add to cart). Early efforts of multi-behavior recommendation often emphasize the differences between multiple behaviors, i.e., they aim to extract useful information by distinguishing different behaviors. However, the commonality between them, which reflects user's common preference for items associated with different behaviors, is largely ignored. Meanwhile, the multi-behavior recommendation still severely suffers from limited supervision signal issue. In this paper, we propose a novel self-supervised graph collaborative filtering model for multi-behavior recommendation named S-MBRec. Specifically, for each behavior, we execute the GCNs to learn the user and item embeddings. Then we design a supervised task, distinguishing the importance of different behaviors, to capture the differences between embeddings. Meanwhile, we propose a star-style contrastive learning task to capture the embedding commonality between target and auxiliary behaviors, so as to alleviate the sparsity of supervision signal, reduce the redundancy among auxiliary behavior, and extract the most critical information. Finally, we jointly optimize the above two tasks. Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of S-MBRec, where the maximum improvement can reach to 20%.

----

## [285] Constrained Adaptive Projection with Pretrained Features for Anomaly Detection

**Authors**: *Xingtai Gui, Di Wu, Yang Chang, Shicai Fan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/286](https://doi.org/10.24963/ijcai.2022/286)

**Abstract**:

Anomaly detection aims to separate anomalies from normal samples, and the pretrained network is promising for anomaly detection. However, adapting the pretrained features would be confronted with the risk of pattern collapse when finetuning on one-class training data. In this paper, we propose an anomaly detection framework called constrained adaptive projection with pretrained features (CAP). Combined with pretrained features, a simple linear projection head applied on a specific input and its k most similar pretrained normal representations is designed for feature adaptation, and a reformed self-attention is leveraged to mine the inner-relationship among one-class semantic features. A loss function is proposed to avoid potential pattern collapse. Concretely, it considers the similarity between a specific data and its corresponding adaptive normal representation, and incorporates a constraint term slightly aligning pretrained and adaptive spaces. Our method achieves state-of-the-art anomaly detection performance on semantic anomaly detection and sensory anomaly detection benchmarks including 96.5% AUROC on CIFAR-100 dataset, 97.0% AUROC on CIFAR-10 dataset and 89.9% AUROC on MvTec dataset.

----

## [286] Quaternion Ordinal Embedding

**Authors**: *Wenzheng Hou, Qianqian Xu, Ke Ma, Qianxiu Hao, Qingming Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/287](https://doi.org/10.24963/ijcai.2022/287)

**Abstract**:

Ordinal embedding (OE) aims to project objects into a low-dimensional space while preserving their ordinal constraints as well as possible. Generally speaking, a reasonable OE algorithm should simultaneously capture  a) semantic meaning and b) the ordinal relationship of the objects. However, most of the existing methods merely focus on b). To address this issue, our goal in this paper is to seek a generic OE method to embrace the two features simultaneously. We argue that different dimensions of vector-based embedding are naturally entangled with each other. To realize a), we expect to decompose the D dimensional embedding space into D different semantic subspaces, where each subspace is associated with a  matrix representation. Unfortunately, introducing a matrix-based representation requires far more complex parametric space than its vector-based counterparts. Thanks to the algebraic property of quaternions, we are able to find a more efficient way to represent a matrix with quaternions. For b), inspired by the classic chordal Grassmannian distance, a new distance function is defined to measure the distance between different quaternions/matrices, on top of which we construct a generic OE loss function. Experimental results for different tasks on both simulated and real-world datasets verify the effectiveness of our proposed method.

----

## [287] MERIT: Learning Multi-level Representations on Temporal Graphs

**Authors**: *Binbin Hu, Zhengwei Wu, Jun Zhou, Ziqi Liu, Zhigang Huangfu, Zhiqiang Zhang, Chaochao Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/288](https://doi.org/10.24963/ijcai.2022/288)

**Abstract**:

Recently,    representation   learning   on   temporal graphs   has   drawn   increasing   attention,   which aims  at  learning  temporal  patterns  to  characterize the evolving nature of dynamic graphs in real-world  applications.    Despite  effectiveness,  these methods   commonly   ignore   the   individual-   and combinatorial-level patterns derived from different types of interactions (e.g.,user-item), which are at the heart of the representation learning on temporal  graphs.   To  fill  this  gap,  we  propose  MERIT, a novel multi-level graph attention network for inductive representation learning on temporal graphs.We  adaptively  embed  the  original  timestamps  to a higher,  continuous dimensional space for learn-ing  individual-level  periodicity  through  Personalized Time Encoding (PTE) module.  Furthermore, we equip MERIT with Continuous time and Con-text aware Attention (Coco-Attention) mechanism which chronologically locates most relevant neighbors  by  jointly  capturing  multi-level  context  on temporal  graphs.   Finally,  MERIT  performs  multiple aggregations and propagations to explore and exploit high-order structural information for down-stream tasks. Extensive experiments on four public datasets demonstrate the effectiveness of MERITon both (inductive / transductive) link prediction and node classification task.

----

## [288] GraphDIVE: Graph Classification by Mixture of Diverse Experts

**Authors**: *Fenyu Hu, Liping Wang, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/289](https://doi.org/10.24963/ijcai.2022/289)

**Abstract**:

Graph classification is a challenging research task in many applications across a broad range of domains. Recently, Graph Neural Network (GNN) models have achieved superior performance on various real-world graph datasets. Despite their successes, most of current GNN models largely suffer from the ubiquitous class imbalance problem, which typically results in prediction bias towards majority classes. Although many imbalanced learning methods have been proposed, they mainly focus on regular Euclidean data and cannot well utilize topological structure of graph (non-Euclidean) data.  To boost the performance of GNNs and investigate the relationship between topological structure and class imbalance, we propose GraphDIVE, which learns multi-view graph representations and combine multi-view experts (i.e., classifiers). Specifically, multi-view graph representations correspond to the intrinsic diverse graph topological structure characteristics. Extensive experiments on molecular benchmark datasets demonstrate the effectiveness of the proposed approach.

----

## [289] End-to-End Open-Set Semi-Supervised Node Classification with Out-of-Distribution Detection

**Authors**: *Tiancheng Huang, Donglin Wang, Yuan Fang, Zhengyu Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/290](https://doi.org/10.24963/ijcai.2022/290)

**Abstract**:

Out-Of-Distribution (OOD) samples are prevalent in real-world applications. The OOD issue becomes even more severe on graph data, as the effect of OOD nodes can be potentially amplified by propagation through the graph topology. Recent works have considered the OOD detection problem, which is critical for reducing the uncertainty in learning and improving the robustness. However, no prior work considers simultaneously OOD detection and node classification on graphs in an end-to-end manner. In this paper, we study a novel problem of end-to-end open-set semi-supervised node classification (OSSNC) on graphs, which deals with node classification in the presence of OOD nodes. Given the lack of supervision on OOD nodes, we introduce a latent variable to indicate in-distribution or OOD nodes in a variational inference framework, and further propose a novel algorithm named as Learning to Mix Neighbors (LMN) which learns to dampen the influence of OOD nodes through the messaging-passing in typical graph neural networks. Extensive experiments on various datasets show that the proposed method outperforms state-of-the-art baselines in terms of both node classification and OOD detection.

----

## [290] A Sparse-Motif Ensemble Graph Convolutional Network against Over-smoothing

**Authors**: *Xuan Jiang, Zhiyong Yang, Peisong Wen, Li Su, Qingming Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/291](https://doi.org/10.24963/ijcai.2022/291)

**Abstract**:

The over-smoothing issue is a well-known challenge for Graph Convolutional Networks (GCN). Specifically, it is often observed that increasing the depth of GCN ends up in a trivial embedding subspace where the difference among node embeddings belonging to the same cluster tends to vanish. This paper believes that the main cause lies in the limited diversity along the message passing pipeline. Inspired by this, we propose a Sparse-Motif Ensemble Graph Convolutional Network (SMEGCN). We argue that merely employing the original graph Laplacian as the spectrum of the graph cannot capture the diversified local structure of complex graphs. Hence, to improve the diversity of the graph spectrum, we introduce local topological structures of complex graphs into GCN by employing the so-called graph motifs or the small network subgraphs. Moreover, we find that the motif connections are much denser than the edge connections, which might converge to an all-one matrix within a few times of message-passing. To fix this, we first propose the notion of sparse motif to avoid spurious motif connections. Subsequently, we propose a hierarchical motif aggregation mechanism to integrate the graph spectral information from a series of different sparse-motif message passing paths. Finally, we conduct a series of theoretical and experimental analyses to demonstrate the superiority of the proposed method.

----

## [291] CGMN: A Contrastive Graph Matching Network for Self-Supervised Graph Similarity Learning

**Authors**: *Di Jin, Luzhi Wang, Yizhen Zheng, Xiang Li, Fei Jiang, Wei Lin, Shirui Pan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/292](https://doi.org/10.24963/ijcai.2022/292)

**Abstract**:

Graph similarity learning refers to calculating the similarity score between two graphs, which is required in many realistic applications, such as visual tracking, graph classification, and collaborative filtering. As most of the existing graph neural networks yield effective graph representations of a single graph, little effort has been made for jointly learning two graph representations and calculating their similarity score. In addition, existing unsupervised graph similarity learning methods are mainly clustering-based, which ignores the valuable information embodied in graph pairs. To this end, we propose a contrastive graph matching network (CGMN) for self-supervised graph similarity learning in order to calculate the similarity between any two input graph objects. Specifically, we generate two augmented views for each graph in a pair respectively. Then, we employ two strategies, namely cross-view interaction and cross-graph interaction, for effective node representation learning. The former is resorted to strengthen the consistency of node representations in two views. The latter is utilized to identify node differences between different graphs. Finally, we transform node representations into graph-level representations via pooling operations for graph similarity computation. We have evaluated CGMN on eight real-world datasets, and the experiment results show that the proposed new approach is superior to the state-of-the-art methods in graph similarity learning downstream tasks.

----

## [292] RAW-GNN: RAndom Walk Aggregation based Graph Neural Network

**Authors**: *Di Jin, Rui Wang, Meng Ge, Dongxiao He, Xiang Li, Wei Lin, Weixiong Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/293](https://doi.org/10.24963/ijcai.2022/293)

**Abstract**:

Graph-Convolution-based methods have been successfully applied to representation learning on homophily graphs where nodes with the same label or similar attributes tend to connect with one another. Due to the homophily assumption of Graph Convolutional Networks (GCNs) that these methods use, they are not suitable for heterophily graphs where nodes with different labels or dissimilar attributes tend to be adjacent. Several methods have attempted to address this heterophily problem, but they do not change the fundamental aggregation mechanism of GCNs because they rely on summation operators to aggregate information from neighboring nodes, which is implicitly subject to the homophily assumption. Here, we introduce a novel aggregation mechanism and develop a RAndom Walk Aggregation-based Graph Neural Network (called RAW-GNN) method. The proposed approach integrates the random walk strategy with graph neural networks. The new method utilizes breadth-first random walk search to capture homophily information and depth-first search to collect heterophily information. It replaces the conventional neighborhoods with path-based neighborhoods and introduces a new path-based aggregator based on Recurrent Neural Networks. These designs make RAW-GNN suitable for both homophily and heterophily graphs. Extensive experimental results showed that the new method achieved state-of-the-art performance on a variety of homophily and heterophily graphs.

----

## [293] Gromov-Wasserstein Discrepancy with Local Differential Privacy for Distributed Structural Graphs

**Authors**: *Hongwei Jin, Xun Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/294](https://doi.org/10.24963/ijcai.2022/294)

**Abstract**:

Learning the similarity between structured data, especially the graphs, is one of the essential problems. Besides the approach like graph kernels, Gromov-Wasserstein (GW) distance recently draws a big attention due to its flexibility to capture both topological and feature characteristics, as well as handling the permutation invariance. However, structured data are widely distributed for different data mining and machine learning applications. With privacy concerns, accessing the decentralized data is limited to either individual clients or different silos. 
To tackle these issues, we propose a privacy-preserving framework to analyze the GW discrepancy of node embedding learned locally from graph neural networks in a federated flavor, and then explicitly place local differential privacy (LDP) based on Multi-bit Encoder to protect sensitive information. Our experiments show that, with strong privacy protection guaranteed by Îµ-LDP algorithm, the proposed framework not only preserves privacy in graph learning, but also presents a noised structural metric under GW distance, resulting in comparable and even better performance in classification and clustering tasks. Moreover, we reason the rationale behind the LDP-based GW distance analytically and empirically.

----

## [294] TGNN: A Joint Semi-supervised Framework for Graph-level Classification

**Authors**: *Wei Ju, Xiao Luo, Meng Qu, Yifan Wang, Chong Chen, Minghua Deng, Xian-Sheng Hua, Ming Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/295](https://doi.org/10.24963/ijcai.2022/295)

**Abstract**:

This paper studies semi-supervised graph classification, a crucial task with a wide range of applications in social network analysis and bioinformatics. Recent works typically adopt graph neural networks to learn graph-level representations for classification, failing to explicitly leverage features derived from graph topology (e.g., paths). Moreover, when labeled data is scarce, these methods are far from satisfactory due to their insufficient topology exploration of unlabeled data. We address the challenge by proposing a novel semi-supervised framework called Twin Graph Neural Network (TGNN). To explore graph structural information from complementary views, our TGNN has a message passing module and a graph kernel module. To fully utilize unlabeled data, for each module, we calculate the similarity of each unlabeled graph to other labeled graphs in the memory bank and our consistency loss encourages consistency between two similarity distributions in different embedding spaces. The two twin modules collaborate with each other by exchanging instance similarity knowledge to fully explore the structure information of both labeled and unlabeled data. We evaluate our TGNN on various public datasets and show that it achieves strong performance.

----

## [295] HashNWalk: Hash and Random Walk Based Anomaly Detection in Hyperedge Streams

**Authors**: *Geon Lee, Minyoung Choe, Kijung Shin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/296](https://doi.org/10.24963/ijcai.2022/296)

**Abstract**:

Sequences of group interactions, such as emails, online discussions, and co-authorships, are ubiquitous; and they are naturally represented as a stream of hyperedges (i.e.,  sets of nodes). Despite its broad potential applications, anomaly detection in hypergraphs (i.e., sets of hyperedges) has received surprisingly little attention, compared to anomaly detection in graphs. While it is tempting to reduce hypergraphs to graphs and apply existing graph-based methods, according to our experiments, taking higher-order structures of hypergraphs into consideration is worthwhile. We propose HashNWalk, an incremental algorithm that detects anomalies in a stream of hyperedges. It maintains and updates a constant-size summary of the structural and temporal information about the input stream. Using the summary, which is the form of a proximity matrix, HashNWalk measures the anomalousness of each new hyperedge as it appears. HashNWalk is (a) Fast: it processes each hyperedge in near real-time and billions of hyperedges within a few hours, (b) Space Efficient: the size of the maintained summary is a user-specific constant,  (c) Effective: it successfully detects anomalous hyperedges in real-world hypergraphs.

----

## [296] MLP4Rec: A Pure MLP Architecture for Sequential Recommendations

**Authors**: *Muyang Li, Xiangyu Zhao, Chuan Lyu, Minghao Zhao, Runze Wu, Ruocheng Guo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/297](https://doi.org/10.24963/ijcai.2022/297)

**Abstract**:

Self-attention models have achieved state-of-the-art performance in sequential recommender systems by capturing the sequential dependencies among user-item interactions. However, they rely on positional embeddings to retain the sequential information, which may break the semantics of item embeddings. In addition, most existing works assume that such sequential dependencies exist solely in the item embeddings, but neglect their existence among the item features. In this work, we propose a novel sequential recommender system (MLP4Rec) based on the recent advances of MLP-based architectures, which is naturally sensitive to the order of items in a sequence. To be specific, we develop a tri-directional fusion scheme to coherently capture sequential, cross-channel and cross-feature correlations. Extensive experiments demonstrate the effectiveness of MLP4Rec over various representative baselines upon two benchmark datasets. The simple architecture of MLP4Rec also leads to the linear computational complexity as well as much fewer model parameters than existing self-attention methods.

----

## [297] Community Question Answering Entity Linking via Leveraging Auxiliary Data

**Authors**: *Yuhan Li, Wei Shen, Jianbo Gao, Yadong Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/298](https://doi.org/10.24963/ijcai.2022/298)

**Abstract**:

Community Question Answering (CQA) platforms contain plenty of CQA texts (i.e., questions and answers corresponding to the question) where named entities appear ubiquitously. In this paper, we define a new task of CQA entity linking (CQAEL) as linking the textual entity mentions detected from CQA texts with their corresponding entities in a knowledge base. This task can facilitate many downstream applications including expert finding and knowledge base enrichment. Traditional entity linking methods mainly focus on linking entities in news documents, and are suboptimal over this new task of CQAEL since they cannot effectively leverage various informative auxiliary data involved in the CQA platform to aid entity linking, such as parallel answers and two types of meta-data (i.e., topic tags and users). To remedy this crucial issue, we propose a novel transformer-based framework to effectively harness the knowledge delivered by different kinds of auxiliary data to promote the linking performance. We validate the superiority of our framework through extensive experiments over a newly released CQAEL data set against state-of-the-art entity linking methods.

----

## [298] TiRGN: Time-Guided Recurrent Graph Network with Local-Global Historical Patterns for Temporal Knowledge Graph Reasoning

**Authors**: *Yujia Li, Shiliang Sun, Jing Zhao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/299](https://doi.org/10.24963/ijcai.2022/299)

**Abstract**:

Temporal knowledge graphs (TKGs) have been widely used in various fields that model the dynamics of facts along the timeline. In the extrapolation setting of TKG reasoning, since facts happening in the future are entirely unknowable, insight into history is the key to predicting future facts. However, it is still a great challenge for existing models as they hardly learn the characteristics of historical events adequately. From the perspective of historical development laws, comprehensively considering the sequential, repetitive, and cyclical patterns of historical facts is conducive to predicting future facts. To this end, we propose a novel representation learning model for TKG reasoning, namely TiRGN, a time-guided recurrent graph network with local-global historical patterns. Specifically, TiRGN uses a local recurrent graph encoder network to model the historical dependency of events at adjacent timestamps and uses the global history encoder network to collect repeated historical facts. After the trade-off between the two encoders, the final inference is performed by a decoder with periodicity. We use six benchmark datasets to evaluate the proposed method. The experimental results show that TiRGN outperforms the state-of-the-art TKG reasoning methods in most cases.

----

## [299] Discrete Listwise Personalized Ranking for Fast Top-N Recommendation with Implicit Feedback

**Authors**: *Fangyuan Luo, Jun Wu, Tao Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/300](https://doi.org/10.24963/ijcai.2022/300)

**Abstract**:

We address the efficiency problem of personalized ranking from implicit feedback by hashing users and items with binary codes, so that top-N recommendation can be fast executed in a Hamming space by bit operations. However, current hashing methods for top-N recommendation fail to align their learning objectives (such as pointwise or pairwise loss) with the benchmark metrics for ranking quality (e.g. Average Precision, AP), resulting in sub-optimal accuracy. To this end, we propose a Discrete Listwise Personalized Ranking (DLPR) model that optimizes AP under discrete constraints for fast and accurate top-N recommendation. To resolve the challenging DLPR problem, we devise an efficient algorithm that can directly learn binary codes in a relaxed continuous solution space. Specifically, theoretical analysis shows that the optimal solution to the relaxed continuous optimization problem is exactly the same as that of the original discrete DLPR problem. Through extensive experiments on two real-world datasets, we show that DLPR consistently surpasses state-of-the-art hashing methods for top-N recommendation.

----

## [300] Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning

**Authors**: *Jun Luo, Shandong Wu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/301](https://doi.org/10.24963/ijcai.2022/301)

**Abstract**:

Conventional federated learning (FL) trains one global model for a federation of clients with decentralized data, reducing the privacy risk of centralized training. However, the distribution shift across non-IID datasets, often poses a challenge to this one-model-fits-all solution. Personalized FL aims to mitigate this issue systematically. In this work, we propose APPLE, a personalized cross-silo FL framework that adaptively learns how much each client can benefit from other clientsâ€™ models. We also introduce a method to flexibly control the focus of training APPLE between global and local objectives. We empirically evaluate our method's convergence and generalization behaviors, and perform extensive experiments on two benchmark datasets and two medical imaging datasets under two non-IID settings. The results show that the proposed personalized FL framework, APPLE, achieves state-of-the-art performance compared to several other personalized FL approaches in the literature. The code is publicly available at https://github.com/ljaiverson/pFL-APPLE.

----

## [301] Reconciling Cognitive Modeling with Knowledge Forgetting: A Continuous Time-aware Neural Network Approach

**Authors**: *Haiping Ma, Jingyuan Wang, Hengshu Zhu, Xin Xia, Haifeng Zhang, Xingyi Zhang, Lei Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/302](https://doi.org/10.24963/ijcai.2022/302)

**Abstract**:

As an emerging technology of computer-aided education, cognitive modeling aims at discovering the knowledge proficiency or learning ability of students, which can enable a wide range of intelligent educational applications. While considerable efforts have been made in this direction, a long-standing research challenge is how to naturally integrate the forgetting mechanism into the learning process of knowledge concepts. To this end, in this paper, we propose a novel Continuous Time based Neural Cognitive Modeling(CT-NCM) approach to integrate the dynamism and continuity of knowledge forgetting into students' learning process modeling in a realistic manner. To be specific, we first adapt the neural Hawkes process with a specially-designed learning event encoding method to model the relationship between knowledge learning and forgetting with continuous time. Then, we propose a learning function with extendable settings to jointly model the change of different knowledge states and their interactions with the exercises at each moment. In this way, CT-NCM can simultaneously predict the future knowledge state and exercise performance of students. Finally, we conduct extensive experiments on five real-world datasets with various benchmark methods. The experimental results clearly validate the effectiveness of CT-NCM and show its interpretability in terms of knowledge learning visualization.

----

## [302] Continual Federated Learning Based on Knowledge Distillation

**Authors**: *Yuhang Ma, Zhongle Xie, Jue Wang, Ke Chen, Lidan Shou*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/303](https://doi.org/10.24963/ijcai.2022/303)

**Abstract**:

Federated learning (FL) is a promising approach for learning a shared global model on decentralized data owned by multiple clients without exposing their privacy. In real-world scenarios, data accumulated at the client-side varies in distribution over time. As a consequence, the global model tends to forget the knowledge obtained from previous tasks while learning new tasks, showing signs of "catastrophic forgetting". Previous studies in centralized learning use techniques such as data replay and parameter regularization to mitigate catastrophic forgetting. Unfortunately, these techniques cannot adequately solve the non-trivial problem in FL. We propose Continual Federated Learning with Distillation (CFeD) to address catastrophic forgetting under FL. CFeD performs knowledge distillation on both the clients and the server, with each party independently having an unlabeled surrogate dataset, to mitigate forgetting. Moreover, CFeD assigns different learning objectives, namely learning the new task and reviewing old tasks, to different clients, aiming to improve the learning ability of the model. The results show that our method performs well in mitigating catastrophic forgetting and achieves a good trade-off between the two objectives.

----

## [303] Physics-Informed Long-Sequence Forecasting From Multi-Resolution Spatiotemporal Data

**Authors**: *Chuizheng Meng, Hao Niu, Guillaume Habault, Roberto Legaspi, Shinya Wada, Chihiro Ono, Yan Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/304](https://doi.org/10.24963/ijcai.2022/304)

**Abstract**:

Spatiotemporal data aggregated over regions or time windows at various resolutions demonstrate heterogeneous patterns and dynamics in each resolution. Meanwhile, the multi-resolution characteristic provides rich contextual information, which is critical for effective long-sequence forecasting. The importance of such inter-resolution information is more significant in practical cases, where fine-grained data is usually collected via approaches with lower costs but also lower qualities compared to those for coarse-grained data. However, existing works focus on uni-resolution data and cannot be directly applied to fully utilize the aforementioned extra information in multi-resolution data. In this work, we propose Spatiotemporal Koopman Multi-Resolution Network (ST-KMRN), a physics-informed learning framework for long-sequence forecasting from multi-resolution spatiotemporal data. Our method jointly models data aggregated in multiple resolutions and captures the inter-resolution dynamics with the self-attention mechanism. We also propose downsampling and upsampling modules among resolutions to further strengthen the connections among data of multiple resolutions. Moreover, we enhance the modeling of intra-resolution dynamics with physics-informed modules based on Koopman theory. Experimental results demonstrate that our proposed approach achieves the best performance on the long-sequence forecasting tasks compared to baselines without a specific design for multi-resolution data.

----

## [304] Raising the Bar in Graph-level Anomaly Detection

**Authors**: *Chen Qiu, Marius Kloft, Stephan Mandt, Maja Rudolph*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/305](https://doi.org/10.24963/ijcai.2022/305)

**Abstract**:

Graph-level anomaly detection has become a critical topic in diverse areas, such as financial fraud detection and detecting anomalous activities in social networks. While most research has focused on anomaly detection for visual data such as images, where high detection accuracies have been obtained, existing deep learning approaches for graphs currently show considerably worse performance. This paper raises the bar on graph-level anomaly detection, i.e., the task of detecting abnormal graphs in a set of graphs. By drawing on ideas from self-supervised learning and transformation learning, we present a new deep learning approach that significantly improves existing deep one-class approaches by fixing some of their known problems, including hypersphere collapse and performance flip. Experiments on nine real-world data sets involving nine techniques reveal that our method achieves an average performance improvement of 11.8% AUC compared to the best existing approach.

----

## [305] Poisoning Deep Learning Based Recommender Model in Federated Learning Scenarios

**Authors**: *Dazhong Rong, Qinming He, Jianhai Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/306](https://doi.org/10.24963/ijcai.2022/306)

**Abstract**:

Various attack methods against recommender systems have been proposed in the past years, and the security issues of recommender systems have drawn considerable attention.
Traditional attacks attempt to make target items recommended to as many users as possible by poisoning the training data.
Benifiting from the feature of protecting users' private data, federated recommendation can effectively defend such attacks.
Therefore, quite a few works have devoted themselves to developing federated recommender systems.
For proving current federated recommendation is still vulnerable, in this work we probe to design attack approaches targeting deep learning based recommender models in federated learning scenarios.
Specifically, our attacks generate poisoned gradients for manipulated malicious users to upload based on two strategies (i.e., random approximation and hard user mining).
Extensive experiments show that our well-designed attacks can effectively poison the target models, and the attack effectiveness sets the state-of-the-art.

----

## [306] Towards Resolving Propensity Contradiction in Offline Recommender Learning

**Authors**: *Yuta Saito, Masahiro Nomura*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/307](https://doi.org/10.24963/ijcai.2022/307)

**Abstract**:

We study offline recommender learning from explicit rating feedback in the presence of selection bias. A current promising solution for dealing with the bias is the inverse propensity score (IPS) estimation. However, the existing propensity-based methods can suffer significantly from the propensity estimation bias. In fact, most of the previous IPS-based methods require some amount of missing-completely-at-random (MCAR) data to accurately estimate the propensity. This leads to a critical self-contradiction; IPS is ineffective without MCAR data, even though it originally aims to learn recommenders from only missing-not-at-random feedback. To resolve this propensity contradiction, we derive a propensity-independent generalization error bound and propose a novel algorithm to minimize the theoretical bound via adversarial learning. Our theory and algorithm do not require a propensity estimation procedure, thereby leading to a well-performing rating predictor without the true propensity information. Extensive experiments demonstrate that the proposed algorithm is superior to a range of existing methods both in rating prediction and ranking metrics in practical settings without MCAR data. Full version of the paper (including the appendix) is available at: https://arxiv.org/abs/1910.07295.

----

## [307] Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features

**Authors**: *Xinyi Shang, Yang Lu, Gang Huang, Hanzi Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/308](https://doi.org/10.24963/ijcai.2022/308)

**Abstract**:

Federated learning (FL) provides a privacy-preserving solution for distributed machine learning tasks. One challenging problem that severely damages the performance of FL models is the co-occurrence of data heterogeneity and long-tail distribution, which frequently appears in real FL applications. In this paper, we reveal an intriguing fact that the biased classifier is the primary factor leading to the poor performance of the global model. Motivated by the above finding, we propose a novel and privacy-preserving FL method for heterogeneous and long-tailed data via Classifier Re-training with Federated Features (CReFF). The classifier re-trained on federated features can produce comparable performance as the one re-trained on real data in a privacy-preserving manner without information leakage of local data or class distribution. Experiments on several benchmark datasets show that the proposed CReFF is an effective solution to obtain a promising FL model under heterogeneous and long-tailed data. Comparative results with the state-of-the-art FL methods also validate the superiority of CReFF. Our code is available at https://github.com/shangxinyi/CReFF-FL.

----

## [308] Long-term Spatio-Temporal Forecasting via Dynamic Multiple-Graph Attention

**Authors**: *Wei Shao, Zhiling Jin, Shuo Wang, Yufan Kang, Xiao Xiao, Hamid Menouar, Zhaofeng Zhang, Junshan Zhang, Flora D. Salim*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/309](https://doi.org/10.24963/ijcai.2022/309)

**Abstract**:

Many real-world ubiquitous applications, such as parking recommendations and air pollution monitoring, benefit significantly from accurate long-term spatio-temporal forecasting (LSTF). LSTF makes use of long-term dependency structure between the spatial and temporal domains, as well as the contextual information. Recent studies have revealed the potential of multi-graph neural networks (MGNNs) to improve prediction performance. However, existing MGNN methods do not work well when applied to LSTF due to several issues: the low level of generality, insufficient use of contextual information, and the imbalanced graph fusion approach. To address these issues, we construct new graph models to represent the contextual information of each node and exploit the long-term spatio-temporal data dependency structure. To aggregate the information across multiple graphs, we propose a new dynamic multi-graph fusion module to characterize the correlations of nodes within a graph and the nodes across graphs via the spatial attention and graph attention mechanisms. Furthermore, we introduce a trainable weight tensor to indicate the importance of each node in different graphs. Extensive experiments on two large-scale datasets demonstrate that our proposed approaches significantly improve the performance of existing graph neural network models in LSTF prediction tasks.

----

## [309] Beyond Homophily: Structure-aware Path Aggregation Graph Neural Network

**Authors**: *Yifei Sun, Haoran Deng, Yang Yang, Chunping Wang, Jiarong Xu, Renhong Huang, Linfeng Cao, Yang Wang, Lei Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/310](https://doi.org/10.24963/ijcai.2022/310)

**Abstract**:

Graph neural networks (GNNs) have been intensively studied in various real-world tasks. However, the homophily assumption of GNNs' aggregation function limits their representation learning ability in heterophily graphs. 
In this paper, we shed light on the path level patterns in graphs that can explicitly reflect rich semantic and structural information.
We therefore propose a novel Structure-aware Path Aggregation Graph Neural Network (PathNet) aiming to generalize GNNs for both homophily and heterophily graphs. Specifically, we first introduce a maximal entropy path sampler, which helps us sample a number of paths containing structural context. Then, we introduce a structure-aware recurrent cell consisting of order-preserving and distance-aware components to learn the semantic information of neighborhoods. Finally, we model the preference of different paths to target node after path encoding.
Experimental results demonstrate that our model achieves superior performance in node classification on both heterophily and homophily graphs.

----

## [310] Personalized Federated Learning with Contextualized Generalization

**Authors**: *Xueyang Tang, Song Guo, Jingcai Guo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/311](https://doi.org/10.24963/ijcai.2022/311)

**Abstract**:

The prevalent personalized federated learning (PFL) usually pursues a trade-off between personalization and generalization by maintaining a shared global model to guide the training process of local models. However, the sole global model may easily transfer deviated context knowledge to some local models when multiple latent contexts exist across the local datasets. In this paper, we propose a novel concept called contextualized generalization (CG) to provide each client with fine-grained context knowledge that can better fit the local data distributions and facilitate faster model convergence, based on which we properly design a framework of PFL, dubbed CGPFL. We conduct detailed theoretical analysis, in which the convergence guarantee is presented and a speedup of order 1/2 w.r.t the number of contexts over most existing methods is granted. To quantitatively study the generalization-personalization trade-off, we introduce the generalization error measure and prove that the proposed CGPFL can achieve a better trade-off than existing solutions. Moreover, our theoretical analysis further inspires a heuristic algorithm to find a near-optimal trade-off in CGPFL. Experimental results on multiple real-world datasets show that our approach surpasses the state-of-the-art methods on test accuracy by a significant margin.

----

## [311] Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion

**Authors**: *Zhenwei Tang, Shichao Pei, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, Robert Hoehndorf, Xiangliang Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/312](https://doi.org/10.24963/ijcai.2022/312)

**Abstract**:

Most real-world knowledge graphs (KG) are far from complete and comprehensive. This problem has motivated efforts in predicting the most plausible missing facts to complete a given KG, i.e., knowledge graph completion (KGC). However, existing KGC methods suffer from two main issues, 1) the false negative issue, i.e., the sampled negative training instances may include potential true facts; and 2) the data sparsity issue, i.e., true facts account for only a tiny part of all possible facts. To this end, we propose positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC. In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task to deal with the false negative issue. Furthermore, to address the data sparsity issue, PUDA achieves a data augmentation strategy by unifying adversarial training and positive-unlabeled learning under the positive-unlabeled minimax game. Extensive experimental results on real-world benchmark datasets demonstrate the effectiveness and compatibility of our proposed method.

----

## [312] Anomaly Detection by Leveraging Incomplete Anomalous Knowledge with Anomaly-Aware Bidirectional GANs

**Authors**: *Bowen Tian, Qinliang Su, Jian Yin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/313](https://doi.org/10.24963/ijcai.2022/313)

**Abstract**:

The goal of anomaly detection is to identify anomalous samples from normal ones. In this paper, a small number of anomalies are assumed to be available at the training stage, but they are assumed to be collected only from several anomaly types, leaving the majority of anomaly types not represented in the collected anomaly dataset at all. To effectively leverage this kind of incomplete anomalous knowledge represented by the collected anomalies, we propose to learn a probability distribution that can not only model the normal samples, but also guarantee to assign low density values for the collected anomalies. To this end, an anomaly-aware generative adversarial network (GAN) is developed, which, in addition to modeling the normal samples as most GANs do, is able to explicitly avoid assigning probabilities for collected anomalous samples. Moreover, to facilitate the computation of anomaly detection criteria like reconstruction error, the proposed anomaly-aware GAN is designed to be bidirectional, attaching an encoder for the generator.  Extensive experimental results demonstrate that our proposed method is able to effectively make use of the incomplete anomalous information, leading to significant performance gains comparing to existing methods.

----

## [313] MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction

**Authors**: *Hung Nghiep Tran, Atsuhiro Takasu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/314](https://doi.org/10.24963/ijcai.2022/314)

**Abstract**:

Knowledge graph embedding aims to predict the missing relations between entities in knowledge graphs. Tensor-decomposition-based models, such as ComplEx, provide a good trade-off between efficiency and expressiveness, that is crucial because of the large size of real world knowledge graphs. The recent multi-partition embedding interaction (MEI) model subsumes these models by using the block term tensor format and provides a systematic solution for the trade-off. However, MEI has several drawbacks, some of which carried from its subsumed tensor-decomposition-based models. In this paper, we address these drawbacks and introduce the Multi-partition Embedding Interaction iMproved beyond block term format (MEIM) model, with independent core tensor for ensemble effects and soft orthogonality for max-rank mapping, in addition to multi-partition embedding. MEIM improves expressiveness while still being highly efficient, helping it to outperform strong baselines and achieve state-of-the-art results on difficult link prediction benchmarks using fairly small embedding sizes. The source code is released at https://github.com/tranhungnghiep/MEIM.

----

## [314] HCFRec: Hash Collaborative Filtering via Normalized Flow with Structural Consensus for Efficient Recommendation

**Authors**: *Fan Wang, Weiming Liu, Chaochao Chen, Mengying Zhu, Xiaolin Zheng*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/315](https://doi.org/10.24963/ijcai.2022/315)

**Abstract**:

The ever-increasing data scale of user-item interactions makes it challenging for an effective and efficient recommender system. Recently, hash-based collaborative filtering (Hash-CF) approaches employ efficient Hamming distance of learned binary representations of users and items to accelerate recommendations. However, Hash-CF often faces two challenging problems, i.e., optimization on discrete representations and preserving semantic information in learned representations. To address the above two challenges, we propose HCFRec, a novel Hash-CF approach for effective and efficient recommendations. Specifically, HCFRec not only innovatively introduces normalized flow to learn the optimal hash code by efficiently fitting a proposed approximate mixture  multivariate  normal distribution, a continuous but approximately discrete distribution, but also deploys a cluster consistency preserving mechanism to preserve the semantic structure in representations for more accurate recommendations. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our HCFRec compared to the state-of-art methods in terms of effectiveness and efficiency.

----

## [315] Augmenting Knowledge Graphs for Better Link Prediction

**Authors**: *Jiang Wang, Filip Ilievski, Pedro A. Szekely, Ke-Thia Yao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/316](https://doi.org/10.24963/ijcai.2022/316)

**Abstract**:

Embedding methods have demonstrated robust performance on the task of link prediction in knowledge graphs, by mostly encoding entity relationships. Recent methods propose to enhance the loss function with a literal-aware term. In this paper, we propose KGA: a knowledge graph augmentation method that incorporates literals in an embedding model without modifying its loss function. KGA discretizes quantity and year values into bins, and chains these bins both horizontally, modeling neighboring values, and vertically, modeling multiple levels of granularity. KGA is scalable and can be used as a pre-processing step for any existing knowledge graph embedding model. Experiments on legacy benchmarks and a new large benchmark, DWD, show that augmenting the knowledge graph with quantities and years is beneficial for predicting both entities and numbers, as KGA outperforms the vanilla models and other relevant baselines. Our ablation studies confirm that both quantities and years contribute to KGA's performance, and that its performance depends on the discretization and binning settings. We make the code, models, and the DWD benchmark publicly available to facilitate reproducibility and future research.

----

## [316] FAITH: Few-Shot Graph Classification with Hierarchical Task Graphs

**Authors**: *Song Wang, Yushun Dong, Xiao Huang, Chen Chen, Jundong Li*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/317](https://doi.org/10.24963/ijcai.2022/317)

**Abstract**:

Few-shot graph classification aims at predicting classes for graphs, given limited labeled graphs for each class. To tackle the bottleneck of label scarcity, recent works propose to incorporate few-shot learning frameworks for fast adaptations to graph classes with limited labeled graphs. Specifically, these works propose to accumulate meta-knowledge across diverse meta-training tasks, and then generalize such meta-knowledge to the target task with a disjoint label set. However, existing methods generally ignore task correlations among meta-training tasks while treating them independently. Nevertheless, such task correlations can advance the model generalization to the target task for better classification performance. On the other hand, it remains non-trivial to utilize task correlations due to the complex components in a large number of meta-training tasks. To deal with this, we propose a novel few-shot learning framework FAITH that captures task correlations via constructing a hierarchical task graph at different granularities. Then we further design a loss-based sampling strategy to select tasks with more correlated classes. Moreover, a task-specific classifier is proposed to utilize the learned task correlations for few-shot classification. Extensive experiments on four prevalent few-shot graph classification datasets demonstrate the superiority of FAITH over other state-of-the-art baselines.

----

## [317] Language Models as Knowledge Embeddings

**Authors**: *Xintao Wang, Qianyu He, Jiaqing Liang, Yanghua Xiao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/318](https://doi.org/10.24963/ijcai.2022/318)

**Abstract**:

Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in  training and evaluation. Experimental results show that LMKE achieves state-of-the-art performance on KE benchmarks of link prediction and triple classification, especially for long-tail entities.

----

## [318] Ensemble Multi-Relational Graph Neural Networks

**Authors**: *Yuling Wang, Hao Xu, Yanhua Yu, Mengdi Zhang, Zhenhao Li, Yuji Yang, Wei Wu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/319](https://doi.org/10.24963/ijcai.2022/319)

**Abstract**:

It is well established that graph neural networks (GNNs) can be interpreted and designed from the perspective of optimization objective. With this clear optimization objective, the deduced GNNs architecture has sound theoretical foundation, which is able to flexibly remedy the weakness of GNNs. However, this optimization objective is only proved for GNNs with single-relational graph. Can we infer a new type of GNNs for multi-relational graphs by extending this optimization objective, so as to simultaneously solve the issues in previous multi-relational GNNs, e.g., over-parameterization? In this paper, we propose a novel ensemble multi-relational GNNs by designing an ensemble multi-relational (EMR) optimization objective. This EMR optimization objective is able to derive an iterative updating rule, which can be formalized as an ensemble message passing (EnMP) layer with multi-relations. We further analyze the nice properties of EnMP layer, e.g., the relationship with multi-relational personalized PageRank. Finally, a new multi-relational GNNs which well alleviate the over-smoothing and over-parameterization issues are proposed. Extensive experiments conducted on four benchmark datasets well demonstrate the effectiveness of the proposed model.

----

## [319] CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for Single-Corpus and Cross-Corpus Speech Emotion Recognition

**Authors**: *Xin-Cheng Wen, Jiaxin Ye, Yan Luo, Yong Xu, Xuan-Ze Wang, Chang-Li Wu, Kun-Hong Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/320](https://doi.org/10.24963/ijcai.2022/320)

**Abstract**:

Speech Emotion Recognition (SER) has become a growing focus of research in human-computer interaction. An essential challenge in SER is to extract common attributes from different speakers or languages, especially when a specific source corpus has to be trained to recognize the unknown data coming from another speech corpus. To address this challenge, a Capsule Network (CapsNet) and Transfer Learning based Mixed Task Net (CTL-MTNet) are proposed to deal with both the single-corpus and cross-corpus SER tasks simultaneously in this paper. For the single-corpus task, the combination of Convolution-Pooling and Attention CapsNet module (CPAC) is designed by embedding the self-attention mechanism to the CapsNet, guiding the module to focus on the important features that can be fed into different capsules. The extracted high-level features by CPAC provide sufficient discriminative ability. Furthermore, to handle the cross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module (CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can learn the domain-invariant emotion representations through extracting the strong emotion commonness. Experiments including ablation studies and visualizations on both single- and cross-corpus tasks using four well-known SER datasets in different languages are conducted for performance evaluation and comparison. The results indicate that in both tasks the CTL-MTNet showed better performance in all cases compared to a number of state-of-the-art methods. The source code and the supplementary materials are available at: https://github.com/MLDMXM2017/CTLMTNet.

----

## [320] Multi-Graph Fusion Networks for Urban Region Embedding

**Authors**: *Shangbin Wu, Xu Yan, Xiaoliang Fan, Shirui Pan, Shichao Zhu, Chuanpan Zheng, Ming Cheng, Cheng Wang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/321](https://doi.org/10.24963/ijcai.2022/321)

**Abstract**:

Learning the embeddings for urban regions from human mobility data can reveal the functionality of regions, and then enables the correlated but distinct tasks such as crime prediction. Human mobility data contains rich but abundant information, which yields to the comprehensive region embeddings for cross domain tasks. In this paper, we propose multi-graph fusion networks (MGFN) to enable the cross domain prediction tasks. First, we integrate the graphs with spatio-temporal similarity as mobility patterns through a mobility graph fusion module. Then, in the mobility pattern joint learning module, we design the multi-level cross-attention mechanism to learn the comprehensive embeddings from multiple mobility patterns based on intra-pattern and inter-pattern messages. Finally, we conduct extensive experiments on real-world urban datasets. Experimental results demonstrate that the proposed MGFN outperforms the state-of-the-art methods by up to 12.35% improvement. https://github.com/wushangbin/MGFN

----

## [321] Understanding and Mitigating Data Contamination in Deep Anomaly Detection: A Kernel-based Approach

**Authors**: *Shuang Wu, Jingyu Zhao, Guangjian Tian*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/322](https://doi.org/10.24963/ijcai.2022/322)

**Abstract**:

Deep anomaly detection has become popular for its capability of handling complex data. However, training a deep detector is fragile to data contamination due to overfitting. In this work, we study the performance of the anomaly detectors under data contamination and construct a data-efficient countermeasure against data contamination. We show that training a deep anomaly detector induces an implicit kernel machine. We then derive an information-theoretic bound of performance degradation with respect to the data contamination ratio. To mitigate the degradation, we propose a contradicting training approach. Apart from learning normality on the contaminated dataset, our approach discourages learning an additional small auxiliary dataset of labeled anomalies. Our approach is much more affordable than constructing a completely clean training dataset. Experiments on public datasets show that our approach significantly improves anomaly detection in the presence of contamination and outperforms some recently proposed detectors.

----

## [322] Decentralized Unsupervised Learning of Visual Representations

**Authors**: *Yawen Wu, Zhepeng Wang, Dewen Zeng, Meng Li, Yiyu Shi, Jingtong Hu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/323](https://doi.org/10.24963/ijcai.2022/323)

**Abstract**:

Collaborative learning enables distributed clients to learn a shared model for prediction while keeping the training data local on each client. However, existing collaborative learning methods require fully-labeled data for training, which is inconvenient or sometimes infeasible to obtain due to the high labeling cost and the requirement of expertise. The lack of labels makes collaborative learning impractical in many realistic settings. Self-supervised learning can address this challenge by learning from unlabeled data. Contrastive learning (CL), a self-supervised learning approach, can effectively learn visual representations from unlabeled image data. However, the distributed data collected on clients are usually not independent and identically distributed (non-IID) among clients, and each client may only have few classes of data, which degrades the performance of CL and learned representations. To tackle this problem, we propose a collaborative contrastive learning framework consisting of two approaches: feature fusion and neighborhood matching, by which a unified feature space among clients is learned for better data representations. Feature fusion provides remote features as accurate contrastive information to each client for better local learning. Neighborhood matching further aligns each clientâ€™s local features to the remote features such that well-clustered features among clients can be learned. Extensive experiments show the effectiveness of the proposed framework. It outperforms other methods by 11% on IID data and matches the performance of centralized learning.

----

## [323] FedCG: Leverage Conditional GAN for Protecting Privacy and Maintaining Competitive Performance in Federated Learning

**Authors**: *Yuezhou Wu, Yan Kang, Jiahuan Luo, Yuanqin He, Lixin Fan, Rong Pan, Qiang Yang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/324](https://doi.org/10.24963/ijcai.2022/324)

**Abstract**:

Federated learning (FL) aims to protect data privacy by enabling clients to build machine learning models collaboratively without sharing their private data. Recent works demonstrate that information exchanged during FL is subject to gradient-based privacy attacks and, consequently, a variety of privacy-preserving methods have been adopted to thwart such attacks. However, these defensive methods either introduce orders of magnitudes more computational and communication overheads (e.g., with homomorphic encryption) or incur substantial model performance losses in terms of prediction accuracy (e.g., with differential privacy). In this work, we propose FEDCG, a novel federated learning method that leverages conditional generative adversarial networks to achieve high-level privacy protection while still maintaining competitive model performance. FEDCG decomposes each client's local network into a private extractor and a public classifier and keeps the extractor local to protect privacy. Instead of exposing extractors, FEDCG shares clients' generators with the server for aggregating clients' shared knowledge aiming to enhance the performance of each client's local networks. Extensive experiments demonstrate that FEDCG can achieve competitive model performance compared with FL baselines, and privacy analysis shows that FEDCG has a high-level privacy-preserving capability.

----

## [324] Subgraph Neighboring Relations Infomax for Inductive Link Prediction on Knowledge Graphs

**Authors**: *Xiaohan Xu, Peng Zhang, Yongquan He, Chengpeng Chao, Chaoyang Yan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/325](https://doi.org/10.24963/ijcai.2022/325)

**Abstract**:

Inductive link prediction for knowledge graph aims at predicting missing links between unseen entities, those not shown in training stage. Most previous works learn entity-specific embeddings of entities, which cannot handle unseen entities. Recent several methods utilize enclosing subgraph to obtain inductive ability. However, all these works only consider the enclosing part of subgraph without complete neighboring relations, which leads to the issue that partial neighboring relations are neglected, and sparse subgraphs are hard to be handled. To address that, we propose Subgraph Neighboring Relations Infomax, SNRI, which sufficiently exploits complete neighboring relations from two aspects: neighboring relational feature for node feature and neighboring relational path for sparse subgraph. To further model neighboring relations in a global way, we innovatively apply mutual information (MI) maximization for knowledge graph. Experiments show that SNRI outperforms existing state-of-art methods by a large margin on inductive link prediction task, and verify the effectiveness of exploring complete neighboring relations in a global way to characterize node features and reason on sparse subgraphs.

----

## [325] GOCPT: Generalized Online Canonical Polyadic Tensor Factorization and Completion

**Authors**: *Chaoqi Yang, Cheng Qian, Jimeng Sun*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/326](https://doi.org/10.24963/ijcai.2022/326)

**Abstract**:

Low-rank tensor factorization or completion is well-studied and applied in various online settings, such as online tensor factorization (where the temporal mode grows) and online tensor completion (where incomplete slices arrive gradually). However, in many real-world settings, tensors may have more complex evolving patterns: (i) one or more modes can grow; (ii) missing entries may be filled; (iii) existing tensor elements can change. Existing methods cannot support such complex scenarios. To fill the gap, this paper proposes a Generalized Online Canonical Polyadic (CP) Tensor factorization and completion framework (named GOCPT) for this general setting, where we maintain the CP structure of such dynamic tensors during the evolution. We show that existing online tensor factorization and completion setups can be unified under the GOCPT framework. Furthermore, we propose a variant, named GOCPTE, to deal with cases where historical tensor elements are unavailable (e.g., privacy protection), which achieves similar fitness as GOCPT but with much less computational cost. Experimental results demonstrate that our GOCPT can improve fitness by up to 2.8% on the JHU Covid data and 9.2% on a proprietary patient claim dataset over baselines. Our variant GOCPTE shows up to 1.2% and 5.5% fitness improvement on two datasets with about 20% speedup compared to the best model.

----

## [326] Trading Hard Negatives and True Negatives: A Debiased Contrastive Collaborative Filtering Approach

**Authors**: *Chenxiao Yang, Qitian Wu, Jipeng Jin, Xiaofeng Gao, Junwei Pan, Guihai Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/327](https://doi.org/10.24963/ijcai.2022/327)

**Abstract**:

Collaborative filtering (CF), as a standard method for recommendation with implicit feedback, tackles a semi-supervised learning problem where most interaction data are unobserved. Such a nature makes existing approaches highly rely on mining negatives for providing correct training signals. However, mining proper negatives is not a free lunch, encountering with a tricky trade-off between mining informative hard negatives and avoiding false ones. We devise a new approach named as Hardness-Aware Debiased Contrastive Collaborative Filtering (HDCCF) to resolve the dilemma. It could sufficiently explore hard negatives from two-fold aspects: 1) adaptively sharpening the gradients of harder instances through a set-wise objective, and 2) implicitly leveraging item/user frequency information with a new sampling strategy. To circumvent false negatives, we develop a principled approach to improve the reliability of negative instances and prove that the objective is an unbiased estimation of sampling from the true negative distribution. Extensive experiments demonstrate the superiority of the proposed model over existing CF models and hard negative mining methods.

----

## [327] Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting

**Authors**: *Hongyuan Yu, Ting Li, Weichen Yu, Jianguo Li, Yan Huang, Liang Wang, Alex X. Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/328](https://doi.org/10.24963/ijcai.2022/328)

**Abstract**:

Multivariate time-series forecasting is a critical task for many applications, and graph time-series network is widely studied due to its capability to capture the spatial-temporal correlation simultaneously. However, most existing works focus more on learning with the explicit prior graph structure, while ignoring potential information from the implicit graph structure, yielding incomplete structure modeling. Some recent works attempts to learn the intrinsic or implicit graph structure directly, while lacking a way to combine explicit prior structure with implicit structure together. In this paper, we propose Regularized Graph Structure Learning (RGSL) model to incorporate both explicit prior structure and implicit structure together, and learn the forecasting deep networks along with the graph structure. RGSL consists of two innovative modules. First, we derive an implicit dense similarity matrix through node embedding, and learn the sparse graph structure using the Regularized Graph Generation (RGG) based on the Gumbel Softmax trick. Second, we propose a Laplacian Matrix Mixed-up Module (LM3) to fuse the explicit graph and implicit graph together. We conduct experiments on three real-word datasets. Results show that the proposed RGSL model outperforms existing graph forecasting algorithms with a notable margin, while learning meaningful graph structure simultaneously. Our code and models are made publicly available at https://github.com/alipay/RGSL.git.

----

## [328] CERT: Continual Pre-training on Sketches for Library-oriented Code Generation

**Authors**: *Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/329](https://doi.org/10.24963/ijcai.2022/329)

**Abstract**:

Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large unlabelled code corpora and perform well in generating code. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and generator are continually pre-trained upon a base model using unlabelled data. Also, we carefully craft two benchmarks to evaluate library-oriented code generation named PandasEval and NumpyEval. Experimental results have shown the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT.

----

## [329] Reconstruction Enhanced Multi-View Contrastive Learning for Anomaly Detection on Attributed Networks

**Authors**: *Jiaqiang Zhang, Senzhang Wang, Songcan Chen*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/330](https://doi.org/10.24963/ijcai.2022/330)

**Abstract**:

Detecting abnormal nodes from attributed networks is of great importance in many real applications, such as financial fraud detection and cyber security. This task is challenging due to both the complex interactions between the anomalous nodes with other counterparts and their inconsistency in terms of attributes. This paper proposes a self-supervised learning framework that jointly optimizes a multi-view contrastive learning-based module and an attribute reconstruction-based module to more accurately detect anomalies on attributed networks. Specifically, two contrastive learning views are firstly established, which allow the model to better encode rich local and global information related to the abnormality. Motivated by the attribute consistency principle between neighboring nodes, a masked autoencoder-based reconstruction module is also introduced to identify the nodes which have large reconstruction errors, then are regarded as anomalies. Finally, the two complementary modules are integrated for more accurately detecting the anomalous nodes. Extensive experiments conducted on five benchmark datasets show our model outperforms current state-of-the-art models.

----

## [330] Dynamic Graph Learning Based on Hierarchical Memory for Origin-Destination Demand Prediction

**Authors**: *Ruixing Zhang, Liangzhe Han, Boyi Liu, Jiayuan Zeng, Leilei Sun*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/331](https://doi.org/10.24963/ijcai.2022/331)

**Abstract**:

Recent years have witnessed a rapid growth of applying deep spatiotemporal methods in traffic forecasting. However, the prediction of origin-destination (OD) demands is still a challenging problem since the number of OD pairs is usually quadratic to the number of stations. In this case, most of the existing spatiotemporal methods fail to handle spatial relations on such a large scale. To address this problem, this paper provides a dynamic graph representation learning framework for OD demands prediction. In particular, a hierarchical memory updater is first proposed to maintain a time-aware representation for each node, and the representations are updated according to the most recently observed OD trips in continuous-time and multiple discrete-time ways. Second, a spatiotemporal propagation mechanism is provided to aggregate representations of neighbor nodes along a random spatiotemporal route which treats origin and destination as two different semantic entities. Last, an objective function is designed to derive the future OD demands according to the most recent node representations, and also to tackle the data sparsity problem in OD prediction. Extensive experiments have been conducted on two real-world datasets, and the experimental results demonstrate the superiority of the proposed method. The code and data are available at https://github.com/Rising0321/HMOD.

----

## [331] GRELEN: Multivariate Time Series Anomaly Detection from the Perspective of Graph Relational Learning

**Authors**: *Weiqi Zhang, Chen Zhang, Fugee Tsung*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/332](https://doi.org/10.24963/ijcai.2022/332)

**Abstract**:

System monitoring and anomaly detection is a crucial task in daily operation. With the rapid development of cyber-physical systems and IT systems, multiple sensors get involved to represent the system state from different perspectives, which inspires us to detect anomalies considering feature dependence relationship among sensors instead of focusing on individual sensor's behavior. In this paper, we propose a novel Graph Relational Learning Network (GReLeN) to detect multivariate time series anomalies from the perspective of between-sensor dependence relationship learning. 
Variational AutoEncoder (VAE) serves as the overall framework for feature extraction and system representation. Graph Neural Network (GNN) and stochastic graph relational learning strategy are also imposed to capture the between-sensor dependence. Then a composite anomaly metric is established with the learned dependence structure explicitly. 
The experiments on four real-world datasets show our superiority in detection accuracy, anomaly diagnosis, and model interpretation.

----

## [332] Enhancing Sequential Recommendation with Graph Contrastive Learning

**Authors**: *Yixin Zhang, Yong Liu, Yonghui Xu, Hao Xiong, Chenyi Lei, Wei He, Lizhen Cui, Chunyan Miao*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/333](https://doi.org/10.24963/ijcai.2022/333)

**Abstract**:

The sequential recommendation systems capture users' dynamic behavior patterns to predict their next interaction behaviors. Most existing sequential recommendation methods only exploit the local context information of an individual interaction sequence and learn model parameters solely based on the item prediction loss. Thus, they usually fail to learn appropriate sequence representations. This paper proposes a novel recommendation framework, namely Graph Contrastive Learning for Sequential Recommendation (GCL4SR). Specifically, GCL4SR employs a Weighted Item Transition Graph (WITG), built based on interaction sequences of all users, to provide global context information for each interaction and weaken the noise information in the sequence data. Moreover, GCL4SR uses subgraphs of WITG to augment the representation of each interaction sequence. Two auxiliary learning objectives have also been proposed to maximize the consistency between augmented representations induced by the same interaction sequence on WITG, and minimize the difference between the representations augmented by the global context on WITG and the local representation of the original sequence. Extensive experiments on real-world datasets demonstrate that GCL4SR consistently outperforms state-of-the-art sequential recommendation methods.

----

## [333] T-SMOTE: Temporal-oriented Synthetic Minority Oversampling Technique for Imbalanced Time Series Classification

**Authors**: *Pu Zhao, Chuan Luo, Bo Qiao, Lu Wang, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/334](https://doi.org/10.24963/ijcai.2022/334)

**Abstract**:

Time series classification is a popular and important topic in machine learning, and it suffers from the class imbalance problem in many real-world applications. In this paper, to address the class imbalance problem, we propose a novel and practical oversampling method named T-SMOTE, which can make full use of the temporal information of time-series data. In particular, for each sample of minority class, T-SMOTE generates multiple samples that are close to class border. Then, based on those samples near class border, T-SMOTE synthesizes more samples. Finally, a weighted sampling method is called on both generated samples near class border and synthetic samples. Extensive experiments on a diverse set of both univariate and multivariate time-series datasets demonstrate that T-SMOTE consistently outperforms the current state-of-the-art methods on imbalanced time series classification. More encouragingly, our empirical evaluations show that T-SMOTE performs better in the scenario of early prediction, an important application scenario in industry, which indicates that T-SMOTE could bring benefits in practice.

----

## [334] MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection

**Authors**: *Jiaqi Zheng, Xi Zhang, Sanchuan Guo, Quan Wang, Wenyu Zang, Yongdong Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/335](https://doi.org/10.24963/ijcai.2022/335)

**Abstract**:

Rumor spreaders are increasingly taking advantage of multimedia content to attract and mislead news consumers on social media. Although recent multimedia rumor detection models have exploited both textual and visual features for classification, they do not integrate the social structure features simultaneously, which have shown promising performance for rumor identification. It is challenging to combine the heterogeneous multi-modal data in consideration of their complex relationships. In this work, we propose a novel Multi-modal Feature-enhanced Attention Networks (MFAN) for rumor detection, which makes the first attempt to integrate textual, visual, and social graph features in one unified framework. Specifically, it considers both the complement and alignment relationships between different modalities to achieve better fusion. Moreover, it takes into account the incomplete links in the social network data due to data collection constraints and proposes to infer hidden links to learn better social graph features. The experimental results show that MFAN can detect rumors effectively and outperform state-of-the-art methods.

----

## [335] Table2Graph: Transforming Tabular Data to Unified Weighted Graph

**Authors**: *Kaixiong Zhou, Zirui Liu, Rui Chen, Li Li, Soo-Hyun Choi, Xia Hu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/336](https://doi.org/10.24963/ijcai.2022/336)

**Abstract**:

Learning useful interactions between input features is  crucial for tabular data modeling. Recent efforts start to explicitly model the feature interactions with  graph, where each feature is treated as an individual node. However, the existing graph construction methods either heuristically formulate a fixed feature-interaction graph based on specific domain knowledge, or simply apply attention function to compute the pairwise feature similarities for each sample. While the fixed graph may be sub-optimal to downstream tasks, the sample-wise graph construction is time-consuming during model training and inference. To tackle these issues, we propose a framework named Table2Graph to transform the feature interaction modeling to learning a unified graph. Represented as a probability adjacency matrix, the unified graph learns to model the key feature interactions shared by the diverse samples in the tabular data. To well optimize the unified graph, we employ the reinforcement learning policy to capture the key feature interactions stably. A sparsity constraint is also proposed to regularize the learned graph from being overly-sparse/smooth. The experimental results in a variety of real-world applications demonstrate the effectiveness and efficiency of our Table2Graph, in terms of the prediction accuracy and feature interaction detection.

----

## [336] Bridging Differential Privacy and Byzantine-Robustness via Model Aggregation

**Authors**: *Heng Zhu, Qing Ling*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/337](https://doi.org/10.24963/ijcai.2022/337)

**Abstract**:

This paper aims at jointly addressing two seemly conflicting issues in federated learning: differential privacy (DP) and Byzantine-robustness, which are particularly challenging when the distributed data are non-i.i.d. (independent and identically distributed). The standard DP mechanisms add noise to the transmitted messages, and entangles with robust stochastic gradient aggregation to defend against Byzantine attacks. In this paper, we decouple the two issues via robust stochastic model aggregation, in the sense that  our proposed DP mechanisms and the defense against Byzantine attacks have separated influence on the learning performance. Leveraging robust stochastic model aggregation, at each iteration, each worker calculates the difference between the local model and the global one, followed by sending the element-wise signs to the master node, which enables robustness to Byzantine attacks. Further, we design two DP mechanisms to perturb the uploaded signs for the purpose of privacy preservation, and prove that they are (epsilon,0)-DP by exploiting the properties of noise distributions. With the tools of Moreau envelop and proximal point projection, we establish the convergence of the proposed algorithm when the cost function is nonconvex. We analyze the trade-off between privacy preservation and learning performance, and show that the influence of our proposed DP mechanisms is decoupled with that of robust stochastic model aggregation. Numerical experiments demonstrate the effectiveness of the proposed algorithm.

----

## [337] Spiking Graph Convolutional Networks

**Authors**: *Zulun Zhu, Jiaying Peng, Jintang Li, Liang Chen, Qi Yu, Siqiang Luo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/338](https://doi.org/10.24963/ijcai.2022/338)

**Abstract**:

Graph Convolutional Networks (GCNs) achieve an impressive performance due to the remarkable representation ability in learning the graph information. However, GCNs, when implemented on a deep network, require expensive computation power, making them difficult to be deployed on battery-powered devices. In contrast, Spiking Neural Networks (SNNs), which perform a bio-fidelity inference process, offer an energy-efficient neural architecture. In this work, we propose SpikingGCN, an end-to-end framework that aims to integrate the embedding of GCNs with the biofidelity characteristics of SNNs. The original graph data are encoded into spike trains based on the incorporation of graph convolution. We further model biological information processing
by utilizing a fully connected layer combined with neuron nodes. In a wide range of scenarios (e.g., citation networks, image graph classification, and recommender systems), our experimental results show that the proposed method could gain competitive performance against state-of-the-art approaches. Furthermore, we show that SpikingGCN on a neuromorphic chip can bring a clear advantage of energy efficiency into graph data analysis, which demonstrates its great potential to construct environment-friendly machine learning models.

----

## [338] Data-Free Adversarial Knowledge Distillation for Graph Neural Networks

**Authors**: *Yuanxin Zhuang, Lingjuan Lyu, Chuan Shi, Carl Yang, Lichao Sun*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/339](https://doi.org/10.24963/ijcai.2022/339)

**Abstract**:

Graph neural networks (GNNs) have been widely used in modeling graph structured data, owing to its impressive performance in a wide range of practical applications. Recently, knowledge distillation (KD) for GNNs has enabled remarkable progress in graph model compression and knowledge transfer. However, most of the existing KD methods require a large volume of real data, which are not readily available in practice, and may preclude their applicability in scenarios where the teacher model is trained on rare or hard to acquire datasets. To address this problem, we propose the first end-to-end framework for data-free adversarial knowledge distillation on graph structured data (DFAD-GNN). To be specific, our DFAD-GNN employs a generative adversarial network, which mainly consists of three components: a pre-trained teacher model and a student model are regarded as two discriminators, and a generator is utilized for deriving training graphs to distill knowledge from the teacher model into the student model. Extensive experiments on various benchmark models and six representative datasets demonstrate that our DFAD-GNN significantly surpasses state-of-the-art data-free baselines in the graph classification task.

----

## [339] Proximity Enhanced Graph Neural Networks with Channel Contrast

**Authors**: *Wei Zhuo, Guang Tan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/340](https://doi.org/10.24963/ijcai.2022/340)

**Abstract**:

We consider graph representation learning in an unsupervised manner. Graph neural networks use neighborhood aggregation as a core component that results in feature smoothing among nodes in proximity. While successful in various prediction tasks, such a paradigm falls short of capturing nodes' similarities over a long distance, which proves to be important for high-quality learning. To tackle this problem, we strengthen the graph with three types of additional graph views, in which each node is directly linked to a set of nodes with the highest similarity in terms of node features, neighborhood features or local structures. Not restricted by connectivity in the original graph, the generated views provide new and complementary perspectives from which to look at the relationship between nodes. Inspired by the recent success of contrastive learning approaches, we propose a self-supervised method that aims to learn node representations by maximizing the agreement between representations across generated views and the original graph, without the requirement of any label information. We also propose a channel-level contrast approach that greatly reduces computation cost. Extensive experiments on six assortative graphs and three disassortative graphs demonstrate the effectiveness of our approach.

----

## [340] On the Utility of Prediction Sets in Human-AI Teams

**Authors**: *Varun Babbar, Umang Bhatt, Adrian Weller*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/341](https://doi.org/10.24963/ijcai.2022/341)

**Abstract**:

Research on human-AI teams usually provides experts with a single label, which ignores the uncertainty in a model's recommendation. Conformal prediction (CP) is a well established line of research that focuses on building a theoretically grounded, calibrated prediction set, which may contain multiple labels. We explore how such prediction sets impact expert decision-making in human-AI teams.  Our evaluation on human subjects finds that set valued predictions positively impact experts. However, we notice that the predictive sets provided by CP can be very large, which leads to unhelpful AI assistants. To mitigate this, we introduce D-CP, a method to perform CP on some examples and defer to experts. We prove that D-CP can reduce the prediction set size of non-deferred examples. We show how D-CP performs in quantitative and in human subject experiments (n=120). Our results suggest that CP prediction sets improve human-AI team performance over showing the top-1 prediction alone, and that experts find D-CP prediction sets are more useful than CP prediction sets.

----

## [341] Multi-Tier Platform for Cognizing Massive Electroencephalogram

**Authors**: *Zheng Chen, Lingwei Zhu, Ziwei Yang, Renyuan Zhang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/342](https://doi.org/10.24963/ijcai.2022/342)

**Abstract**:

An end-to-end platform assembling multiple tiers is built for precisely cognizing brain activities. Being fed massive electroencephalogram (EEG) data, the time-frequency spectrograms are conventionally projected into the episode-wise feature matrices (seen as tier-1). A spiking neural network (SNN) based tier is designed to distill the principle information in terms of spike-streams from the rare features, which maintains the temporal implication in the nature of EEGs. The proposed tier-3 transposes time- and space-domain of spike patterns from the SNN; and feeds the transposed pattern-matrices into an artificial neural network (ANN, Transformer specifically) known as tier-4, where a special spanning topology is proposed to match the two-dimensional input form. In this manner, cognition such as classification is conducted with high accuracy. For proof-of-concept, the sleep stage scoring problem is demonstrated by introducing multiple EEG datasets with the largest comprising 42,560 hours recorded from 5,793 subjects. From experiment results, our platform achieves the general cognition overall accuracy of 87% by leveraging sole EEG, which is 2% superior to the state-of-the-art. Moreover, our developed multi-tier methodology offers visible and graphical interpretations of the temporal characteristics of EEG by identifying the critical episodes, which is demanded in neurodynamics but hardly appears in conventional cognition scenarios.

----

## [342] Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks

**Authors**: *Lang Feng, Qianhui Liu, Huajin Tang, De Ma, Gang Pan*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/343](https://doi.org/10.24963/ijcai.2022/343)

**Abstract**:

Spiking neural networks (SNNs) are bio-inspired neural networks with asynchronous discrete and sparse characteristics, which have increasingly manifested their superiority in low energy consumption. Recent research is devoted to utilizing spatio-temporal information to directly train SNNs by backpropagation. However, the binary and non-differentiable properties of spike activities force directly trained SNNs to suffer from serious gradient vanishing and network degradation, which greatly limits the performance of directly trained SNNs and prevents them from going deeper. In this paper, we propose a multi-level firing (MLF) method based on the existing spatio-temporal back propagation (STBP) method, and spiking dormant-suppressed residual network (spiking DS-ResNet). MLF enables more efficient gradient propagation and the incremental expression ability of the neurons. Spiking DS-ResNet can efficiently perform identity mapping of discrete spikes, as well as provide a more suitable connection for gradient propagation in deep SNNs. With the proposed method, our model achieves superior performances on a non-neuromorphic dataset and two neuromorphic datasets with much fewer trainable parameters and demonstrates the great ability to combat the gradient vanishing and degradation problem in deep SNNs.

----

## [343] Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts

**Authors**: *Patrick Hemmer, Sebastian Schellhammer, Michael Vössing, Johannes Jakubik, Gerhard Satzger*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/344](https://doi.org/10.24963/ijcai.2022/344)

**Abstract**:

Machine learning (ML) models are increasingly being used in application domains that often involve working together with human experts. In this context, it can be advantageous to defer certain instances to a single human expert when they are difficult to predict for the ML model. While previous work has focused on scenarios with one distinct human expert, in many real-world situations several human experts with varying capabilities may be available. In this work, we propose an approach that trains a classification model to complement the capabilities of multiple human experts. By jointly training the classifier together with an allocation system, the classifier learns to accurately predict those instances that are difficult for the human experts, while the allocation system learns to pass each instance to the most suitable team member—either the classifier or one of the human experts. We evaluate our proposed approach in multiple experiments on public datasets with “synthetic” experts and a real-world medical dataset annotated by multiple radiologists. Our approach outperforms prior work and is more accurate than the best human expert or a classifier. Furthermore, it is flexibly adaptable to teams of varying sizes and different levels of expert diversity.

----

## [344] Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes

**Authors**: *Yang Li, Yi Zeng*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/345](https://doi.org/10.24963/ijcai.2022/345)

**Abstract**:

Spiking neural network (SNN), as a brain-inspired energy-efficient neural network, has attracted the interest of researchers. While the training of spiking neural networks is still an open problem. One effective way is to map the weight of trained ANN to SNN to achieve high reasoning ability. However, the converted spiking neural network often suffers from performance degradation and a considerable time delay. To speed up the inference process and obtain higher accuracy, we theoretically analyze the errors in the conversion process from three perspectives: the differences between IF and ReLU, time dimension, and pooling operation. We propose a neuron model for releasing burst spikes, a cheap but highly efficient method to solve residual information. In addition, Lateral Inhibition Pooling (LIPooling) is proposed to solve the inaccuracy problem caused by MaxPooling in the conversion process. Experimental results on CIFAR and ImageNet demonstrate that our algorithm is efficient and accurate. For example, our method can ensure nearly lossless conversion of SNN and only use about 1/10 (less than 100) simulation time under 0.693x energy consumption of the typical method. Our code is available at https://github.com/Brain-Inspired-Cognitive-Engine/Conversion_Burst.

----

## [345] Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations

**Authors**: *Sangwon Seo, Vaibhav V. Unhelkar*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/346](https://doi.org/10.24963/ijcai.2022/346)

**Abstract**:

We present Bayesian Team Imitation Learner (BTIL), an imitation learning algorithm to model the behavior of teams performing sequential tasks in Markovian domains. In contrast to existing multi-agent imitation learning techniques, BTIL explicitly models and infers the time-varying mental states of team members, thereby enabling learning of decentralized team policies from demonstrations of suboptimal teamwork. Further, to allow for sample- and label-efficient policy learning from small datasets, BTIL employs a Bayesian perspective and is capable of learning from semi-supervised demonstrations. We demonstrate and benchmark the performance of BTIL on synthetic multi-agent tasks as well as a novel dataset of human-agent teamwork. Our experiments show that BTIL can successfully learn team policies from demonstrations despite the influence of team members' (time-varying and potentially misaligned) mental states on their behavior.

----

## [346] Signed Neuron with Memory: Towards Simple, Accurate and High-Efficient ANN-SNN Conversion

**Authors**: *Yuchen Wang, Malu Zhang, Yi Chen, Hong Qu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/347](https://doi.org/10.24963/ijcai.2022/347)

**Abstract**:

Spiking Neural Networks (SNNs) are receiving increasing attention due to their biological plausibility and the potential for ultra-low-power event-driven neuromorphic hardware implementation. Due to the complex temporal dynamics and discontinuity of spikes, training SNNs directly usually suffers from high computing resources and a long training time. As an alternative, SNN can be converted from a pre-trained artificial neural network (ANN) to bypass the difficulty in SNNs learning. However, the existing ANN-to-SNN methods neglect the inconsistency of information transmission between synchronous ANNs and asynchronous SNNs. In this work, we first analyze how the asynchronous spikes in SNNs may cause conversion errors between ANN and SNN. To address this problem, we propose a signed neuron with memory function, which enables almost no accuracy loss during the conversion process, and maintains the properties of asynchronous transmission in the converted SNNs. We further propose a new normalization method, named neuron-wise normalization, to significantly shorten the inference latency in the converted SNNs. We conduct experiments on challenging datasets including CIFAR10 (95.44% top-1), CIFAR100 (78.3% top-1) and ImageNet (73.16% top-1). Experimental results demonstrate that the proposed method outperforms the state-of-the-art works in terms of accuracy and inference time. The code is available at https://github.com/ppppps/ANN2SNNConversion_SNM_NeuronNorm.

----

## [347] Rethinking InfoNCE: How Many Negative Samples Do You Need?

**Authors**: *Chuhan Wu, Fangzhao Wu, Yongfeng Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/348](https://doi.org/10.24963/ijcai.2022/348)

**Abstract**:

InfoNCE is a widely used contrastive training loss. It aims to estimate the mutual information between a pair of variables by discriminating between each positive pair and its associated K negative pairs. It is proved that when the sample labels are clean, the lower bound of mutual information estimation is tighter when more negative samples are incorporated, which usually yields better model performance. However, in practice the labels often contain noise, and incorporating too many noisy negative samples into model training may be suboptimal. In this paper, we study how many negative samples are optimal for InfoNCE in different scenarios via a semi-quantitative theoretical framework. More specifically, we first propose a probabilistic model to analyze the influence of the negative sampling ratio K on training sample informativeness. Then, we design a training effectiveness function to measure the overall influence of training samples based on their informativeness. We estimate the optimal negative sampling ratio using the K value that maximizes the training effectiveness function. Based on our framework, we further propose an adaptive negative sampling method that can dynamically adjust the negative sampling ratio to improve InfoNCE-based model training. Extensive experiments in three different tasks show our framework can accurately predict the optimal negative sampling ratio, and various models can benefit from our adaptive negative sampling method.

----

## [348] On Preferences and Priority Rules in Abstract Argumentation

**Authors**: *Gianvincenzo Alfano, Sergio Greco, Francesco Parisi, Irina Trubitsyna*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/349](https://doi.org/10.24963/ijcai.2022/349)

**Abstract**:

Dung's abstract Argumentation Framework (AF) has emerged as a central formalism for argumentation in AI. 
Preferences in AF allow to represent the comparative strength of arguments in a simple yet expressive way.
In this paper we first investigate the complexity of the verification as well as credulous and skeptical acceptance problems in Preference-based AF (PAF) that extends AF with preferences over arguments. 
Next, after introducing new semantics for AF where extensions are selected using cardinality (instead of set inclusion) criteria and investigating their complexity, we introduce a framework called AF with Priority rules (AFP) that extends AF with sequences of priority rules.
AFP generalizes AF with classical set-inclusion and cardinality based semantics, suggesting that argumentation semantics can be viewed as ways to express priorities among extensions.
Finally, we extend AFP by proposing AF with Priority rules and Preferences (AFP^2), where also preferences over arguments can be used to define priority rules, and study the complexity of the above-mentioned problems.

----

## [349] Beyond Strong-Cyclic: Doing Your Best in Stochastic Environments

**Authors**: *Benjamin Aminof, Giuseppe De Giacomo, Sasha Rubin, Florian Zuleger*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/350](https://doi.org/10.24963/ijcai.2022/350)

**Abstract**:

``Strong-cyclic policies" were introduced to formalize trial-and-error strategies and are known to work in Markovian stochastic domains, i.e., they guarantee that the goal is reached with probability 1. We introduce ``best-effort" policies for (not necessarily Markovian) stochastic domains. These generalize strong-cyclic policies by taking advantage of stochasticity even if the goal cannot be reached with probability 1. We compare such policies with optimal policies, i.e., policies that maximize the probability that the goal is achieved, and show that optimal policies are best-effort, but that the converse is false in general. With this framework at hand, we revisit the foundational problem of what it means to plan in nondeterministic domains when the nondeterminism has a stochastic nature. We show that one can view a nondeterministic planning domain as a representation of infinitely many stochastic domains with the same support but different probabilities, and that for temporally extended goals expressed in LTL/LTLf a finite-state best-effort policy in one of these domains is best-effort in each of the domains. In particular, this gives an approach for finding such policies that reduces to solving finite-state MDPs with LTL/LTLf goals. All this shows that ``best-effort" policies are robust to changes in the probabilities, as long as the support is unchanged.

----

## [350] Annotated Sequent Calculi for Paraconsistent Reasoning and Their Relations to Logical Argumentation

**Authors**: *Ofer Arieli, Kees van Berkel, Christian Straßer*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/351](https://doi.org/10.24963/ijcai.2022/351)

**Abstract**:

We introduce annotated sequent calculi, which are extensions of standard sequent calculi, where sequents are combined with annotations that represent their derivation statuses. Unlike in ordinary calculi, sequents that are derived in annotated calculi may still be retracted in the presence of conflicting sequents, thus inferences are made under stricter conditions. Conflicts in the resulting systems are handled like in adaptive logics and argumentation theory. The outcome is a robust family of proof systems for non-monotonic reasoning with inconsistent information, where revision considerations are fully integrated into the object level of the proofs. These systems are shown to be strongly connected to logical argumentation.

----

## [351] Limits and Possibilities of Forgetting in Abstract Argumentation

**Authors**: *Ringo Baumann, Matti Berthold*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/352](https://doi.org/10.24963/ijcai.2022/352)

**Abstract**:

The topic of forgetting has been extensively studied in the field of knowledge representation and reasoning for many major formalisms. Quite recently it has been introduced to abstract argumentation. However, many already known as well as essential aspects about forgetting like strong persistence or strong invariance have been left unconsidered. We show that forgetting in abstract argumentation cannot be reduced to forgetting in logic programming. In addition, we deal with the more general problem of forgetting whole sets of arguments and show that iterative application of existing operators for single arguments does not necessarily yield a desirable result as it may not produce an informationally economic argumentation framework. As a consequence we provide a systematic and exhaustive study of forgetting desiderata and associated operations adapted to the intrinsics of abstract argumentation. We show the limits and shed light on the possibilities.

----

## [352] Body-Decoupled Grounding via Solving: A Novel Approach on the ASP Bottleneck

**Authors**: *Viktor Besin, Markus Hecher, Stefan Woltran*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/353](https://doi.org/10.24963/ijcai.2022/353)

**Abstract**:

Answer-Set Programming (ASP) has seen tremendous progress over the last two decades and is nowadays successfully applied in many real-world domains. However, for certain types of problems, the well-known ASP grounding bottleneck still causes severe problems. This becomes virulent when grounding of rules, where the variables have to be replaced by constants, leads to a ground pro- gram that is too huge to be processed by the ASP solver. In this work, we tackle this problem by a novel method that decouples non-ground atoms in rules in order to delegate the evaluation of rule bodies to the solving process. Our procedure translates a non-ground normal program into a ground disjunctive program that is exponential only in the maximum predicate arity, and thus polynomial if this arity is assumed to be bounded by a constant. We demonstrate the feasibility of this new method experimentally by comparing it to standard ASP technology in terms of grounding size, grounding time and total runtime.

----

## [353] Verification and Monitoring for First-Order LTL with Persistence-Preserving Quantification over Finite and Infinite Traces

**Authors**: *Diego Calvanese, Giuseppe De Giacomo, Marco Montali, Fabio Patrizi*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/354](https://doi.org/10.24963/ijcai.2022/354)

**Abstract**:

We address the problem of model checking first-order dynamic systems where new objects can be injected in the active domain during execution.  Notable examples are systems induced by a first-order action theory, e.g., expressed in the Situation Calculus.  Recent results have shown that, under the state-boundedness assumption, such systems, in spite of having a first-order representation of the state, admit decidable model checking for full first-order mu-calculus. However, interestingly, model checking remains undecidable in the case of first-order LTL (LTL-FO).  In this paper, we show that in LTL-FOp, which is the fragment of LTL-FO in which quantification is over objects that persist along traces, model checking state-bounded systems becomes decidable over finite and infinite traces. We then employ this result to show how to handle monitoring of LTL-FOp properties against a trace stemming from an unknown state-bounded dynamic system, simultaneously considering the finite trace up to the current point, and all its possibly infinite future continuations.

----

## [354] The Limits of Morality in Strategic Games

**Authors**: *Rui Cao, Pavel Naumov*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/355](https://doi.org/10.24963/ijcai.2022/355)

**Abstract**:

An agent, or a coalition of agents, is blameable for an outcome if she had a strategy to prevent it. In this paper we introduce a notion of limited blameworthiness, with a constraint on the amount of sacrifice required to prevent the outcome. The main technical contribution is a sound and complete logical system for reasoning about limited blameworthiness in the strategic game setting.

----

## [355] On Verifying Expectations and Observations of Intelligent Agents

**Authors**: *Sourav Chakraborty, Avijeet Ghosh, Sujata Ghosh, François Schwarzentruber*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/356](https://doi.org/10.24963/ijcai.2022/356)

**Abstract**:

Public observation logic (POL) is a variant of dynamic epistemic logic to reason about agent expectations and agent observations. Agents have certain expectations, regarding the situation at hand, that are actuated by the relevant protocols, and they eliminate possible worlds in which their expectations do not match with their observations.  In this work, we investigate the computational complexity of the model checking problem for POL and prove its PSPACE-completeness. We also study various syntactic fragments of POL. We exemplify the applicability of POL model checking in verifying different characteristics and features of an interactive system with respect to the distinct expectations and (matching) observations of the system. Finally, we provide a discussion on the implementation of the model checking algorithms.

----

## [356] Personalized Federated Learning With a Graph

**Authors**: *Fengwen Chen, Guodong Long, Zonghan Wu, Tianyi Zhou, Jing Jiang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/357](https://doi.org/10.24963/ijcai.2022/357)

**Abstract**:

Knowledge sharing and model personalization are two key components in the conceptual framework of personalized federated learning (PFL). Existing PFL methods focus on proposing new model personalization mechanisms while simply implementing knowledge sharing by aggregating models from all clients, regardless of their relation graph. This paper aims to enhance the knowledge-sharing process in PFL by leveraging the graph-based structural information among clients. We propose a novel structured federated learning (SFL) framework to learn both the global and personalized models simultaneously using client-wise relation graphs and clients' private data. We cast SFL with graph into a novel optimization problem that can model the client-wise complex relations and graph-based structural topology by a unified framework. Moreover, in addition to using an existing relation graph, SFL could be expanded to learn the hidden relations among clients. Experiments on traffic and image benchmark datasets can demonstrate the effectiveness of the proposed method.

----

## [357] On the Complexity of Enumerating Prime Implicants from Decision-DNNF Circuits

**Authors**: *Alexis de Colnet, Pierre Marquis*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/358](https://doi.org/10.24963/ijcai.2022/358)

**Abstract**:

We consider the problem Enum·IP of enumerating prime implicants of Boolean functions represented by decision decomposable negation normal form (dec-DNNF) circuits. We study Enum·IP from dec-DNNF within the framework of enumeration complexity and prove that it is in OutputP, the class of output polynomial enumeration problems, and more precisely in IncP, the class of polynomial incremental time enumeration problems. We then focus on two closely related, but seemingly harder, enumeration problems where further restrictions are put on the prime implicants to be generated. In the first problem, one is only interested in prime implicants representing subset-minimal abductive explanations, a notion much investigated in AI for more than thirty years.  In the second problem, the target is prime implicants representing sufficient reasons, a recent yet important notion in the emerging field of eXplainable AI, since they aim to explain predictions achieved by machine learning classifiers. We provide evidence showing that enumerating specific prime implicants corresponding to subset-minimal abductive explanations or to sufficient reasons is not in OutputP.

----

## [358] LTLf Synthesis as AND-OR Graph Search: Knowledge Compilation at Work

**Authors**: *Giuseppe De Giacomo, Marco Favorito, Jianwen Li, Moshe Y. Vardi, Shengping Xiao, Shufang Zhu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/359](https://doi.org/10.24963/ijcai.2022/359)

**Abstract**:

Synthesis techniques for temporal logic specifications are typically based on exploiting symbolic techniques, as done in model checking. These symbolic techniques typically use backward fixpoint computation. Planning, which can be seen as a specific form of synthesis, is a witness of the success of forward search approaches. In this paper, we develop a forward-search approach to full-fledged Linear Temporal Logic on finite traces (LTLf) synthesis. We show how to compute the Deterministic Finite Automaton (DFA) of an LTLf formula on-the-fly, while performing an adversarial forward search towards the final states, by considering the DFA as a sort of AND-OR graph. Our approach is characterized by branching on suitable propositional formulas, instead of individual evaluations, hence radically reducing the branching factor of the search space. Specifically, we take advantage of techniques developed for knowledge compilation, such as Sentential Decision Diagrams (SDDs), to implement the approach efficiently.

----

## [359] Epistemic Logic of Likelihood and Belief

**Authors**: *James P. Delgrande, Joshua Sack, Gerhard Lakemeyer, Maurice Pagnucco*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/360](https://doi.org/10.24963/ijcai.2022/360)

**Abstract**:

A major challenge in AI is dealing with uncertain information. While probabilistic approaches have been employed to address this issue, in many situations probabilities may not be available or may be unsuitable. As an alternative, qualitative approaches have been introduced to express that one event is no more probable than another. We provide an approach where an agent may reason deductively about notions of likelihood, and may hold beliefs where the subjective probability for a belief is less than 1. Thus, an agent can believe that p holds (with probability <1); and if the agent believes that q is more likely than p, then the agent will also believe q. Our language allows for arbitrary nesting of beliefs and qualitative likelihoods. We provide a sound and complete proof system for the logic with respect to an underlying probabilistic semantics, and show that the language is equivalent to a sublanguage with no nested modalities.

----

## [360] LTL on Weighted Finite Traces: Formal Foundations and Algorithms

**Authors**: *Carmine Dodaro, Valeria Fionda, Gianluigi Greco*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/361](https://doi.org/10.24963/ijcai.2022/361)

**Abstract**:

LTL on finite traces (LTLf ) is a logic that attracted much attention in recent literature, for its ability to formalize the qualitative behavior of dynamical systems in several application domains. However, its practical usage is still rather limited, as LTLf cannot deal with any quantitative aspect, such as with the costs of realizing some desired behaviour. The paper fills the gap by proposing a weighting framework for LTLf encoding such quantitative aspects in the traces over which it is evaluated. The complexity of reasoning problems on weighted traces is analyzed and compared to that of standard LTLf, by considering arbitrary formulas as well as classes of formulas defined in terms of relevant syntactic restrictions. Moreover, a reasoner for LTL on weighted finite traces is presented, and its performances are assessed on benchmark data.

----

## [361] Abstract Argumentation Frameworks with Marginal Probabilities

**Authors**: *Bettina Fazzinga, Sergio Flesca, Filippo Furfaro*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/362](https://doi.org/10.24963/ijcai.2022/362)

**Abstract**:

In the context of probabilistic AAFs, we intro-
duce AAFs with marginal probabilities (mAAFs)
requiring only marginal probabilities of argu-
ments/attacks to be specified and not relying on the
independence assumption. Reasoning over mAAFs
requires taking into account multiple probability
distributions over the possible worlds, so that the
probability of extensions is not determined by a
unique value, but by an interval. We focus on the
problems of computing the max and min probabil-
ities of extensions over mAAFs under Dungâ€™s se-
mantics, characterize their complexity, and provide
closed formulas for polynomial cases.

----

## [362] Plausibility Reasoning via Projected Answer Set Counting - A Hybrid Approach

**Authors**: *Johannes Klaus Fichte, Markus Hecher, Mohamed A. Nadeem*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/363](https://doi.org/10.24963/ijcai.2022/363)

**Abstract**:

Answer set programming is a form of declarative programming widely used to solve difficult search problems. Probabilistic applications however require to go beyond simple search for one solution and need counting. One such application is plausibility reasoning, which provides more fine-grained reasoning mode between simple brave and cautious reasoning. When modeling with ASP, we oftentimes introduce auxiliary atoms in the program. If these atoms are functionally independent of the atoms of interest, we need to hide the auxiliary atoms and project the count to the atoms of interest resulting in the problem projected answer set counting. In practice, counting becomes quickly infeasible with standard systems such as clasp. In this paper, we present a novel hybrid approach for plausibility reasoning under projections, thereby relying on projected answer set counting as basis. Our approach combines existing systems with fast dynamic programming, which in our experiments shows advantages over existing ASP systems.

----

## [363] Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies

**Authors**: *Maurice Funk, Jean Christoph Jung, Carsten Lutz*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/364](https://doi.org/10.24963/ijcai.2022/364)

**Abstract**:

We study ELI queries (ELIQs) in the presence of ontologies formulated in the description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a frontier (set of least general generalizations) that is of polynomial size and can be computed in polynomial time. In the dialect DL-LiteF, in contrast, frontiers may be infinite. We identify a natural syntactic restriction that enables the same positive results as for DL-LiteH. We use our results on frontiers to show that ELIQs are learnable in polynomial time in the presence of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact learning with only membership queries.

----

## [364] Simulating Sets in Answer Set Programming

**Authors**: *Sarah Alice Gaggl, Philipp Hanisch, Markus Krötzsch*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/365](https://doi.org/10.24963/ijcai.2022/365)

**Abstract**:

We study the extension of non-monotonic disjunctive logic programs with terms that represent sets of constants, called DLP(S), under the stable model semantics. This strictly increases expressive power, but keeps reasoning decidable, though cautious entailment is coNEXPTIME^NP-complete, even for data complexity. We present two new reasoning methods for DLP(S): a semantics-preserving translation of DLP(S) to logic programming with function symbols, which can take advantage of lazy grounding techniques, and a ground-and-solve approach that uses non-monotonic existential rules in the grounding stage. Our evaluation considers problems of ontological reasoning that are not in scope for traditional ASP (unless EXPTIME =ΠP2 ), and we find that our new existential-rule grounding performs well in comparison with native implementations of set terms in ASP.

----

## [365] Linear Temporal Logic Modulo Theories over Finite Traces

**Authors**: *Luca Geatti, Alessandro Gianola, Nicola Gigante*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/366](https://doi.org/10.24963/ijcai.2022/366)

**Abstract**:

This paper studies Linear Temporal Logic over Finite Traces (LTLf) where proposition letters are replaced with first-order formulas interpreted over arbitrary theories, in the spirit of Satisfiability Modulo Theories. The resulting logic, called LTLf Modulo Theories (LTLfMT), is semi-decidable. Nevertheless, its high expressiveness comes useful in a number of use cases, such as model-checking of data-aware processes and data-aware planning. Despite the general undecidability of these problems, being able to solve satisfiable instances is a compromise worth studying. After motivating and describing such use cases, we provide a sound and complete semi-decision procedure for LTLfMT based on the SMT encoding of a one-pass tree-shaped tableau system. The algorithm is implemented in the BLACK satisfiability checking tool, and an experimental evaluation shows the feasibility of the approach on novel benchmarks.

----

## [366] A Computationally Grounded Logic of 'Seeing-to-it-that'

**Authors**: *Andreas Herzig, Emiliano Lorini, Elise Perrotin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/367](https://doi.org/10.24963/ijcai.2022/367)

**Abstract**:

We introduce a simple model of agency that is based on the concepts of control and attempt. Both relate agents and propositional variables. Moreover, they can be nested: an agent i may control whether another agent j controls a propositional variable p; i may control whether j attempts to change p; i may attempt to change whether j controls p; i may attempt to change whether j attempts to change p; and so on. In this framework we define several modal operators of time and agency: the LTL operators on the one hand, and the Chellas and the deliberative stit operator on the other. While in the standard stit framework the model checking problem is unfeasible because its models are infinite, in our framework models are represented in a finite and compact way: they are grounded on the primitive concepts of control and attempt. This makes model checking practically feasible. We prove its PSPACE-completeness and we show how the concept of social influence can be captured.

----

## [367] Possibilistic Logic Underlies Abstract Dialectical Frameworks

**Authors**: *Jesse Heyninck, Gabriele Kern-Isberner, Tjitze Rienstra, Kenneth Skiba, Matthias Thimm*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/368](https://doi.org/10.24963/ijcai.2022/368)

**Abstract**:

Abstract dialectical frameworks (in short, ADFs) are one of the most general and unifying approaches to formal argumentation. As the semantics of ADFs are based on three-valued interpretations, we ask which monotonic three-valued logic allows to capture the main semantic concepts underlying ADFs. We show that possibilistic logic is the unique logic that can faithfully encode all other semantical concepts for ADFs. Based on this result, we also characterise strong equivalence and introduce possibilistic ADFs.

----

## [368] Lexicographic Entailment, Syntax Splitting and the Drowning Problem

**Authors**: *Jesse Heyninck, Gabriele Kern-Isberner, Thomas Andreas Meyer*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/369](https://doi.org/10.24963/ijcai.2022/369)

**Abstract**:

Lexicographic inference is a well-known and popular approach to reasoning with non-monotonic conditionals. It is a logic of very high-quality, as it extends rational closure and avoids the so-called drowning problem. It seems, however, this high quality comes at a cost, as reasoning on the basis of lexicographic inference is of high computational complexity. In this paper, we show that lexicographic inference satisfies syntax splitting, which means that we can restrict our attention to parts of the belief base that share atoms with a given query, thus seriously restricting the computational costs for many concrete queries. Furthermore, we make some observations on the relationship between c-representations and lexicographic inference, and reflect on the relation between syntax splitting and the drowning problem.

----

## [369] Computing Concept Referring Expressions for Queries on Horn ALC Ontologies

**Authors**: *Moritz Illich, Birte Glimm*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/370](https://doi.org/10.24963/ijcai.2022/370)

**Abstract**:

Classical instance queries over an ontology only consider explicitly named individuals. Concept referring expressions (CREs) also allow for returning answers in the form of concepts that describe implicitly given individuals in terms of their relation to an explicitly named one. Existing approaches, e.g., based on tree automata, can neither be integrated into state-of-the-art OWL reasoners nor are they directly amenable for an efficient implementation. To address this, we devise a novel algorithm that uses highly optimized OWL reasoners as a black box. In addition to the standard criteria of singularity and certainty for CREs, we devise and consider the criterion of uniqueness of CREs for Horn ALC ontologies. The evaluation of our prototypical implementation shows that computing CREs for the most general concept (Top) can be done in less than one minute for ontologies with thousands of individuals and concepts.

----

## [370] The Egocentric Logic of Preferences

**Authors**: *Junli Jiang, Pavel Naumov*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/371](https://doi.org/10.24963/ijcai.2022/371)

**Abstract**:

The paper studies preferences of agents about other agents in a social network. It proposes a logical system that captures the properties of such preferences, called "likes". The system can express nested constructions "agent likes humbled people", "agent likes those who like humbled people", etc. The main technical results are a model checking algorithm and a sound, complete, and decidable axiomatization of the proposed system.

----

## [371] In Data We Trust: The Logic of Trust-Based Beliefs

**Authors**: *Junli Jiang, Pavel Naumov*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/372](https://doi.org/10.24963/ijcai.2022/372)

**Abstract**:

The paper proposes a data-centred approach to reasoning about the interplay between trust and beliefs. At its core, is the modality "under the assumption that one dataset is trustworthy, another dataset informs a belief in a statement". The main technical result is a sound and complete logical system capturing the properties of this modality.

----

## [372] Conditional Independence for Iterated Belief Revision

**Authors**: *Gabriele Kern-Isberner, Jesse Heyninck, Christoph Beierle*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/373](https://doi.org/10.24963/ijcai.2022/373)

**Abstract**:

Conditional independence is a crucial concept for efficient probabilistic reasoning. For symbolic and qualitative reasoning, however, it has played only a minor role. Recently, Lynn, Delgrande, and Peppas have considered conditional independence in terms of syntactic multivalued dependencies. In this paper, we define conditional independence as a semantic property of epistemic states and present axioms for iterated belief revision operators to obey conditional independence in general. We show that c-revisions for ranking functions satisfy these axioms, and exploit the relevance of these results for iterated belief revision in general.

----

## [373] Search Space Expansion for Efficient Incremental Inductive Logic Programming from Streamed Data

**Authors**: *Mark Law, Krysia Broda, Alessandra Russo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/374](https://doi.org/10.24963/ijcai.2022/374)

**Abstract**:

In the past decade, several systems for learning Answer Set Programs (ASP) have been proposed, including the recent FastLAS system. Compared to other state-of-the-art approaches to learning ASP, FastLAS is more scalable, as rather than computing the hypothesis space in full, it computes a much smaller subset relative to a given set of examples that is nonetheless guaranteed to contain an optimal solution to the task (called an OPT-sufficient subset). On the other hand, like many other Inductive Logic Programming (ILP) systems, FastLAS is designed to be run on a fixed learning task meaning that if new examples are discovered after learning, the whole process must be run again.

In many real applications, data arrives in a stream. Rerunning an ILP system from scratch each time new examples arrive is inefficient. In this paper we address this problem by presenting IncrementalLAS, a system that uses a new technique, called hypothesis space expansion, to enable a FastLAS-like OPT-sufficient subset to be expanded each time new examples are discovered. We prove that this preserves FastLAS's guarantee of finding an optimal solution to the full task (including the new examples), while removing the need to repeat previous computations. Through our evaluation, we demonstrate that running IncrementalLAS on tasks updated with sequences of new examples is significantly faster than re-running FastLAS from scratch on each updated task.

----

## [374] Explanations for Negative Query Answers under Inconsistency-Tolerant Semantics

**Authors**: *Thomas Lukasiewicz, Enrico Malizia, Cristian Molinaro*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/375](https://doi.org/10.24963/ijcai.2022/375)

**Abstract**:

Inconsistency-tolerant semantics have been proposed to provide meaningful query answers even in the presence of inconsistent knowledge. Recently, explainability has also become a prominent problem in different areas of AI. While the complexity of inconsistency-tolerant semantics is rather well-understood, not much attention has been paid yet to the problem of explaining query answers when inconsistencies may exist. Recent work on existential rules in the inconsistent setting has focused only on understanding why a query is entailed.
In this paper, we address another important problem, which is explaining why a query is not entailed under an inconsistency-tolerant semantics. In particular, we consider three popular  semantics, namely, the ABox repair, the intersection of repairs, and the intersection of closed repairs. We provide a thorough complexity analysis for a wide range of existential rule languages and for several complexity measures.

----

## [375] Causes of Effects: Learning Individual Responses from Population Data

**Authors**: *Scott Mueller, Ang Li, Judea Pearl*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/376](https://doi.org/10.24963/ijcai.2022/376)

**Abstract**:

The problem of individualization is crucial in almost every field of science. Identifying causes of specific observed events is likewise essential for accurate decision making as well as explanation. However, such tasks invoke counterfactual relationships, and are therefore indeterminable from population data. For example, the probability of benefiting from a treatment concerns an individual having a favorable outcome if treated and an unfavorable outcome if untreated; it cannot be estimated from experimental data, even when conditioned on fine-grained features, because we cannot test both possibilities for an individual. Tian and Pearl provided bounds on this and other probabilities of causation using a combination of experimental and observational data. Those bounds, though tight, can be narrowed significantly  when structural information is available in the form of a causal model. This added information may provide the power to solve central problems, such as explainable AI, legal responsibility, and personalized medicine, all of which demand counterfactual logic. This paper derives, analyzes, and characterizes these new bounds, and illustrates some of their practical applications.

----

## [376] Inverse Problems for Gradual Semantics

**Authors**: *Nir Oren, Bruno Yun, Srdjan Vesic, Murilo S. Baptista*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/377](https://doi.org/10.24963/ijcai.2022/377)

**Abstract**:

Gradual semantics with abstract argumentation provide each argument with a score reflecting its acceptability. Many different gradual semantics have been proposed in the literature, each following different principles and producing different argument rankings. A sub-class of such semantics, the so-called weighted semantics, takes, in addition to the graph structure, an initial set of weights over the arguments as input, with these weights affecting the resultant argument ranking. In this work, we consider the inverse problem over such weighted semantics. That is, given an argumentation framework and a desired argument ranking, we ask whether there exist initial weights such that a particular semantics produces the given ranking. The contribution of this paper are: (1) an algorithm to answer this problem, (2) a characterisation of the properties that a gradual semantics must satisfy for the algorithm to operate, and (3) an empirical evaluation of the proposed algorithm.

----

## [377] Learning Higher-Order Logic Programs From Failures

**Authors**: *Stanislaw J. Purgal, David M. Cerna, Cezary Kaliszyk*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/378](https://doi.org/10.24963/ijcai.2022/378)

**Abstract**:

Learning complex programs through inductive logic programming (ILP) remains a formidable challenge. Existing higher-order enabled ILP systems show improved accuracy and learning performance, though remain hampered by the limitations of the underlying learning mechanism. Experimental results show that our extension of the versatile Learning From Failures paradigm by higher-order definitions significantly improves learning performance without the
burdensome human guidance required by existing systems. Our theoretical framework captures a class of higher-order definitions preserving soundness of existing subsumption-based pruning methods.

----

## [378] Revision by Comparison for Ranking Functions

**Authors**: *Meliha Sezgin, Gabriele Kern-Isberner*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/379](https://doi.org/10.24963/ijcai.2022/379)

**Abstract**:

Revision by Comparison (RbC) is a non-prioritized belief revision mechanism on epistemic states that specifies constraints on the plausibility of an input sentence via a designated reference sentence, allowing for kind of relative belief revision. In this paper, we make the strategy underlying RbC more explicit and transfer the mechanism together with its intuitive strengths to a semi-quantitative framework based on ordinal conditional functions where a more elegant implementation of RbC is possible. We furthermore show  that RbC can be realized as an iterated  revision by so-called weak conditionals. Finally, we point out relations of RbC to credibility-limited belief revision, illustrating the versatility of RbC for advanced belief revision operations.

----

## [379] Considering Constraint Monotonicity and Foundedness in Answer Set Programming

**Authors**: *Yi-Dong Shen, Thomas Eiter*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/380](https://doi.org/10.24963/ijcai.2022/380)

**Abstract**:

Should the properties of constraint monotonicity and foundedness be mandatory requirements that every answer set and world view semantics must satisfy?  This question is challenging and has incurred a debate in answer set programming (ASP).  In this paper we address the question by introducing natural logic programs whose expected answer sets and world views violate these properties and thus may be viewed as counter-examples to these requirements.  Specifically we use instances of the generalized strategic companies problem for ASP benchmark competitions as concrete examples to demonstrate that the requirements of constraint monotonicity and foundedness may exclude expected answer sets for some simple disjunctive programs and world views for some epistemic specifications. In conclusion these properties should not be mandatory conditions for an answer set and world view semantics in general.

----

## [380] Updating Probability Intervals with Uncertain Inputs

**Authors**: *Karim Tabia*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/381](https://doi.org/10.24963/ijcai.2022/381)

**Abstract**:

Probability intervals provide an intuitive, powerful and unifying setting for encoding and reasoning with  imprecise beliefs. This paper addresses the problem of updating uncertain information specified in the form of probability intervals with new uncertain inputs also expressed as probability intervals. We place ourselves in the framework of Jeffrey's rule of conditioning and propose extensions of this conditioning for the interval-based setting. More precisely, we first extend Jeffrey's rule to credal sets then propose extensions of Jeffrey's rule to three common  conditioning rules for probability intervals (robust, Dempster and geometric conditionings). While the first extension is based on conditioning the extreme points of the credal sets induced by the probability intervals, the other methods directly revise  the interval bounds of the  distributions to be updated. Finally, the paper  discusses related issues and relates the proposed methods with respect  to the state-of-the-art.

----

## [381] Simple and Effective Relation-based Embedding Propagation for Knowledge Representation Learning

**Authors**: *Huijuan Wang, Siming Dai, Weiyue Su, Hui Zhong, Zeyang Fang, Zhengjie Huang, Shikun Feng, Zeyu Chen, Yu Sun, Dianhai Yu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/382](https://doi.org/10.24963/ijcai.2022/382)

**Abstract**:

Relational graph neural networks have garnered particular attention to encode graph context in knowledge graphs (KGs). Although they achieved competitive performance on small KGs, how to efficiently and effectively utilize graph context for large KGs remains an open problem. To this end, we propose the Relation-based Embedding Propagation (REP) method. It is a post-processing technique to adapt pre-trained KG embeddings with graph context. As relations in KGs are directional, we model the incoming head context and the outgoing tail context separately. Accordingly, we design relational context functions with no external parameters. Besides, we use averaging to aggregate context information, making REP more computation-efficient. We theoretically prove that such designs can avoid information distortion during propagation. Extensive experiments also demonstrate that REP has significant scalability while improving or maintaining prediction quality. Particularly, it averagely brings about 10% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and takes 5%-83% time to achieve comparable results as the state-of-the-art GC-OTE.

----

## [382] Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention

**Authors**: *Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/383](https://doi.org/10.24963/ijcai.2022/383)

**Abstract**:

Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear.
 By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems.
 We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. 
 In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.

----

## [383] GL-RG: Global-Local Representation Granularity for Video Captioning

**Authors**: *Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, Dongfang Liu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/384](https://doi.org/10.24963/ijcai.2022/384)

**Abstract**:

Video captioning is a challenging task as it needs to  accurately  transform  visual  understanding  into natural language description.  To date, state-of-the-art  methods  inadequately  model  global-local representation across video frames for caption generation, leaving plenty of room for improvement.  In this work, we approach the video captioning task from a new perspective and propose  a GL-RG framework for video captioning, namely a Global-Local Representation Granularity. Our GL-RG demonstrates three advantages over the prior efforts: 1) we explicitly exploit extensive visual representations from different video ranges to improve linguistic expression; 2) we devise a novel global-local encoder to produce rich semantic vocabulary to obtain a descriptive granularity of video contents across frames; 3) we develop an incremental training strategy which organizes model learning in an incremental fashion to incur an optimal captioning behavior. Experimental results on the challenging MSR-VTT and MSVD datasets show that our DL-RG outperforms recent state-of-the-art methods by a significant margin. Code is available at https://github.com/ylqi/GL-RG.

----

## [384] FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server

**Authors**: *Hong Zhang, Ji Liu, Juncheng Jia, Yang Zhou, Huaiyu Dai, Dejing Dou*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/385](https://doi.org/10.24963/ijcai.2022/385)

**Abstract**:

Despite achieving remarkable performance, Federated Learning (FL) suffers from two critical challenges, i.e., limited computational resources and low training efficiency. In this paper, we propose a novel FL framework, i.e., FedDUAP, with two original contributions, to exploit the insensitive data on the server and the decentralized data in edge devices to further improve the training efficiency. First, a dynamic server update algorithm is designed to exploit the insensitive data on the server, in order to dynamically determine the optimal steps of the server update for improving the convergence and accuracy of the global model. Second, a layer-adaptive model pruning method is developed to perform unique pruning operations adapted to the different dimensions and importance of multiple layers, to achieve a good balance between efficiency and effectiveness. By integrating the two original techniques together, our proposed FL model, FedDUAP, significantly outperforms baseline approaches in terms of accuracy (up to 4.8% higher), efficiency (up to 2.8 times faster), and computational cost (up to 61.9% smaller).

----

## [385] Synthesis of Maximally Permissive Strategies for LTLf Specifications

**Authors**: *Shufang Zhu, Giuseppe De Giacomo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/386](https://doi.org/10.24963/ijcai.2022/386)

**Abstract**:

In this paper, we study synthesis of maximally permissive  strategies for Linear Temporal Logic on finite traces (LTLf) specifications. That is, instead  of computing a single strategy (aka plan, or policy), we aim at computing the entire set of strategies at once and then choosing among them while in execution, without committing to a single one beforehand. Maximally permissive strategies have been introduced and investigated for safety properties, especially in the context of Discrete Event Control Theory. However, the available results for safety properties do not apply to reachability properties (eventually reach a given state of affair) nor to LTLf properties in general. In this paper, we show that maximally permissive strategies do exist also for reachability and general LTLf properties, and can in fact be computed with minimal overhead wrt the computation of a single strategy using state-of-the-art tools.

----

## [386] Learning Label Initialization for Time-Dependent Harmonic Extension

**Authors**: *Amitoz Azad*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/387](https://doi.org/10.24963/ijcai.2022/387)

**Abstract**:

Node classification on graphs can be formulated as the Dirichlet problem on graphs where the signal is given at the labeled nodes, and the harmonic extension is done on the unlabeled nodes. This paper considers a time-dependent version of the Dirichlet problem on graphs and shows how to improve its solution by learning the proper initialization vector on the unlabeled nodes. Further, we show that the improved solution is at par with state-of-the-art methods used for node classification. Finally, we conclude this paper by discussing the importance of parameter t, pros, and future directions.

----

## [387] Fixed-Budget Best-Arm Identification in Structured Bandits

**Authors**: *Mohammad Javad Azizi, Branislav Kveton, Mohammad Ghavamzadeh*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/388](https://doi.org/10.24963/ijcai.2022/388)

**Abstract**:

Best-arm identification (BAI) in a fixed-budget setting is a bandit problem where the learning agent maximizes the probability of identifying the optimal (best) arm after a fixed number of observations. Most works on this topic study unstructured problems with a small number of arms, which limits their applicability. We propose a general tractable algorithm that incorporates the structure, by successively eliminating suboptimal arms based on their mean reward estimates from a joint generalization model. We analyze our algorithm in linear and generalized linear models (GLMs), and propose a practical implementation based on a G-optimal design. In linear models, our algorithm has competitive error guarantees to prior works and performs at least as well empirically. In GLMs, this is the first practical algorithm with analysis for fixed-budget BAI.

----

## [388] One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model

**Authors**: *Wonho Bae, Junhyug Noh, Milad Jalali Asadabadi, Danica J. Sutherland*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/389](https://doi.org/10.24963/ijcai.2022/389)

**Abstract**:

Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to identify objects in images based on a small number of images with pixel-level labels, and many more images with only image-level labels. Most existing SWSSS algorithms extract pixel-level pseudo-labels from an image classifier - a very difficult task to do well, hence requiring complicated architectures and extensive hyperparameter tuning on fully-supervised validation sets. We propose a method called prediction filtering, which instead of extracting pseudo-labels, just uses the classifier as a classifier: it ignores any segmentation predictions from classes which the classifier is confident are not present. Adding this simple post-processing method to baselines gives results competitive with or better than prior SWSSS algorithms. Moreover, it is compatible with pseudo-label methods: adding prediction filtering to existing SWSSS algorithms further improves segmentation performance.

----

## [389] Logit Mixing Training for More Reliable and Accurate Prediction

**Authors**: *Duhyeon Bang, Kyungjune Baek, Jiwoo Kim, Yunho Jeon, Jin-Hwa Kim, Jiwon Kim, Jongwuk Lee, Hyunjung Shim*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/390](https://doi.org/10.24963/ijcai.2022/390)

**Abstract**:

When a person solves the multi-choice problem, she considers not only what is the answer but also what is not the answer. Knowing what choice is not the answer and utilizing the relationships between choices, she can improve the prediction accuracy. Inspired by this human reasoning process, we propose a new training strategy to fully utilize inter-class relationships, namely LogitMix. Our strategy is combined with recent data augmentation techniques, e.g., Mixup, Manifold Mixup, CutMix, and PuzzleMix. Then, we suggest using a mixed logit, i.e., a mixture of two logits, as an auxiliary training objective. Since the logit can preserve both positive and negative inter-class relationships, it can impose a network to learn the probability of wrong answers correctly. Our extensive experimental results on the image- and language-based tasks demonstrate that LogitMix achieves state-of-the-art performance among recent data augmentation techniques regarding calibration error and prediction accuracy.

----

## [390] Adversarial Explanations for Knowledge Graph Embeddings

**Authors**: *Patrick Betz, Christian Meilicke, Heiner Stuckenschmidt*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/391](https://doi.org/10.24963/ijcai.2022/391)

**Abstract**:

We propose a novel black-box approach for performing adversarial attacks against knowledge graph embedding models. An adversarial attack is a small perturbation of the data at training time to cause model failure at test time. We make use of an efficient rule learning approach and use abductive reasoning to identify triples which are logical explanations for a particular prediction. The proposed attack is then based on the simple idea to suppress or modify one of the triples in the most confident explanation. Although our attack scheme is model independent and only needs access to the training data, we report results on par with state-of-the-art white-box attack methods that additionally require full access to the model architecture, the learned embeddings, and the loss functions. This is a surprising result which indicates that knowledge graph embedding models can partly be explained post hoc with the help of symbolic methods.

----

## [391] Not a Number: Identifying Instance Features for Capability-Oriented Evaluation

**Authors**: *Ryan Burnell, John Burden, Danaja Rutar, Konstantinos Voudouris, Lucy Cheke, José Hernández-Orallo*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/392](https://doi.org/10.24963/ijcai.2022/392)

**Abstract**:

In AI evaluation, performance is often calculated by averaging across various instances. But to fully understand the capabilities of an AI system, we need to understand the factors that cause its pattern of success and failure. In this paper, we present a new methodology to identify and build informative instance features that can provide explanatory and predictive power to analyse the behaviour of AI systems more robustly. The methodology builds on these relevant features that should relate monotonically with success, and represents patterns of performance in a new type of plots known as ‘agent characteristic grids’. We illustrate this methodology with the Animal-AI competition as a representative example of how we can revisit existing competitions and benchmarks in AI—even when evaluation data is sparse. Agents with the same average performance can show very different patterns of performance at the instance level. With this methodology, these patterns can be visualised, explained and predicted, progressing towards a capability-oriented evaluation rather than relying on a less informative average performance score.

----

## [392] Posistive-Unlabeled Learning via Optimal Transport and Margin Distribution

**Authors**: *Nan Cao, Teng Zhang, Xuanhua Shi, Hai Jin*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/393](https://doi.org/10.24963/ijcai.2022/393)

**Abstract**:

Positive-unlabeled (PU) learning deals with the circumstances where only a portion of positive instances are labeled, while the rest and all negative instances are unlabeled, and due to this confusion, the class prior can not be directly available. Existing PU learning methods usually estimate the class prior by training a nontraditional probabilistic classifier, which is prone to give an overestimation. Moreover, these methods learn the decision boundary by optimizing the minimum margin, which is not suitable in PU learning due to its sensitivity to label noise. In this paper, we enhance PU learning methods from the above two aspects. More specifically, we first explicitly learn a transformation from unlabeled data to positive data by entropy regularized optimal transport to achieve a much more precise estimation for class prior. Then we switch to optimizing the margin distribution, rather than the minimum margin, to obtain a label noise insensitive classifier. Extensive empirical studies on both synthetic and real-world data sets demonstrate the superiority of our proposed method.

----

## [393] Neural Contextual Anomaly Detection for Time Series

**Authors**: *Chris U. Carmona, François-Xavier Aubet, Valentin Flunkert, Jan Gasthaus*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/394](https://doi.org/10.24963/ijcai.2022/394)

**Abstract**:

We introduce Neural Contextual Anomaly Detection (NCAD), a framework for anomaly detection on time series that scales seamlessly from the unsupervised to supervised setting, and is applicable to both univariate and multivariate time series. This is achieved by combining recent developments in representation learning for multivariate time series, with techniques for deep anomaly detection originally developed for computer vision that we tailor to the time series setting. Our window-based approach facilitates learning the boundary between normal and anomalous classes by injecting generic synthetic anomalies into the available data. NCAD can effectively take advantage of domain knowledge and of any available training labels. We demonstrate empirically on standard benchmark datasets that our approach obtains a state-of-the-art performance in the supervised, semi-supervised, and unsupervised settings.

----

## [394] Rethinking the Promotion Brought by Contrastive Learning to Semi-Supervised Node Classification

**Authors**: *Deli Chen, Yankai Lin, Lei Li, Xuancheng Ren, Peng Li, Jie Zhou, Xu Sun*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/395](https://doi.org/10.24963/ijcai.2022/395)

**Abstract**:

Graph Contrastive Learning (GCL) has proven highly effective in promoting the performance of Semi-Supervised Node Classification (SSNC). However, existing GCL methods are generally transferred from other fields like CV or NLP, whose underlying working mechanism remains underexplored. In this work, we first deeply probe the working mechanism of GCL in SSNC, and find that the promotion brought by GCL is severely unevenly distributed: the improvement mainly comes from subgraphs with less annotated information, which is fundamentally different from contrastive learning in other fields. However, existing GCL methods generally ignore this uneven distribution of annotated information and apply GCL evenly to the whole graph. To remedy this issue and further improve GCL in SSNC, we propose the Topology InFormation gain-Aware Graph Contrastive Learning (TIFA-GCL) framework that considers the annotated information distribution across graph in GCL. Extensive experiments on six benchmark graph datasets, including the enormous OGB-Products graph, show that TIFA-GCL can bring a larger improvement than existing GCL methods in both transductive and inductive settings. Further experiments demonstrate the generalizability and interpretability of TIFA-GCL.

----

## [395] Self-Supervised Mutual Learning for Dynamic Scene Reconstruction of Spiking Camera

**Authors**: *Shiyan Chen, Chaoteng Duan, Zhaofei Yu, Ruiqin Xiong, Tiejun Huang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/396](https://doi.org/10.24963/ijcai.2022/396)

**Abstract**:

Mimicking the sampling mechanism of the primate fovea, a retina-inspired vision sensor named spiking camera has been developed, which has shown great potential for capturing high-speed dynamic scenes with a sampling rate of 40,000 Hz. Unlike conventional digital cameras, the spiking camera continuously captures photons and outputs asynchronous binary spikes with various inter-spike intervals to record dynamic scenes. However, how to reconstruct dynamic scenes from asynchronous spike streams remains challenging. In this work, we propose a novel pretext task to build a self-supervised reconstruction framework for spiking cameras. Specifically, we utilize the blind-spot network commonly used in self-supervised denoising tasks as our backbone, and perform self-supervised learning by constructing proper pseudo-labels. In addition, in view of the poor scalability and insufficient information utilization of the blind-spot network, we present a mutual learning framework to improve the overall performance of the network through mutual distillation between a non-blind-spot network and a blind-spot network. This also enables the network to bypass constraints of the blind-spot network, allowing state-of-the-art modules to be used to further improve performance. The experimental results demonstrate that our methods evidently outperform previous unsupervised spiking camera reconstruction methods and achieve desirable results compared with supervised methods.

----

## [396] DDDM: A Brain-Inspired Framework for Robust Classification

**Authors**: *Xiyuan Chen, Xingyu Li, Yi Zhou, Tianming Yang*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/397](https://doi.org/10.24963/ijcai.2022/397)

**Abstract**:

Despite their outstanding performance in a broad spectrum of real-world tasks, deep artificial neural networks are sensitive to input noises, particularly adversarial perturbations. On the contrary, human and animal brains are much less vulnerable. In contrast to the one-shot inference performed by most deep neural networks, the brain often solves decision-making with an evidence accumulation mechanism that may trade time for accuracy when facing noisy inputs. The mechanism is well described by the Drift-Diffusion Model (DDM). In the DDM, decision-making is modeled as a process in which noisy evidence is accumulated toward a threshold. Drawing inspiration from the DDM, we propose the Dropout-based Drift-Diffusion Model (DDDM) that combines test-phase dropout and the DDM for improving the robustness for arbitrary neural networks. The dropouts create temporally uncorrelated noises in the network that counter perturbations, while the evidence accumulation mechanism guarantees a reasonable decision accuracy. Neural networks enhanced with the DDDM tested in image, speech, and text classification tasks all significantly outperform their native counterparts, demonstrating the DDDM as a task-agnostic defense against adversarial attacks.

----

## [397] Better Embedding and More Shots for Few-shot Learning

**Authors**: *Ziqiu Chi, Zhe Wang, Mengping Yang, Wei Guo, Xinlei Xu*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/398](https://doi.org/10.24963/ijcai.2022/398)

**Abstract**:

In few-shot learning, methods are enslaved to the scarce labeled data, resulting in suboptimal embedding. Recent studies learn the embedding network by other large-scale labeled data. However, the trained network may give rise to the distorted embedding of target data. We argue two respects are required for an unprecedented and promising solution. We call them Better Embedding and More Shots (BEMS). Suppose we propose to extract embedding from the embedding network. BE maximizes the extraction of general representation and prevents over-fitting information. For this purpose, we introduce the topological relation for global reconstruction, avoiding excessive memorizing. MS maximizes the relevance between the reconstructed embedding and the target class space. In this respect, increasing the number of shots is a pivotal but intractable strategy. As a creative method, we derive the bound of information-theory-based loss function and implicitly achieve infinite shots with negligible cost. A substantial experimental analysis is carried out to demonstrate the state-of-the-art performance. Compared to the baseline, our method improves by up to 10%+. We also prove that BEMS is suitable for both standard pre-trained and meta-learning embedded networks.

----

## [398] Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning

**Authors**: *Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, Dimitrios Dimitriadis*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/399](https://doi.org/10.24963/ijcai.2022/399)

**Abstract**:

Federated learning (FL) enables edge-devices to collaboratively learn a model without disclosing their private data to a central aggregating server. Most existing FL algorithms require models of identical architecture to be deployed across the clients and server, making it infeasible to train large models due to clients' limited system resources. In this work, we propose a novel ensemble knowledge transfer method named Fed-ET in which small models (different in architecture) are trained on clients, and used to train a larger model at the server. Unlike in conventional ensemble learning, in FL the ensemble can be trained on clients' highly heterogeneous data. Cognizant of this property, Fed-ET uses a weighted consensus distillation scheme with diversity regularization that efficiently extracts reliable consensus from the ensemble while improving generalization by exploiting the diversity within the ensemble. We show the generalization bound for the ensemble of weighted models trained on heterogeneous datasets that supports the intuition of Fed-ET. Our experiments on image and language tasks show that Fed-ET significantly outperforms other state-of-the-art FL algorithms with fewer communicated parameters, and is also robust against high data-heterogeneity.

----

## [399] Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?

**Authors**: *Hwanil Choi, Wonjoon Chang, Jaesik Choi*

**Conference**: *ijcai 2022*

**URL**: [https://doi.org/10.24963/ijcai.2022/400](https://doi.org/10.24963/ijcai.2022/400)

**Abstract**:

Even though Generative Adversarial Networks (GANs) have shown a remarkable ability to generate high-quality images, GANs do not always guarantee the generation of photorealistic images. Occasionally, they generate images that have defective or unnatural objects, which are referred to as `artifacts'. Research to investigate why these artifacts emerge and how they can be detected and removed has yet to be sufficiently carried out. To analyze this, we first hypothesize that rarely activated neurons and frequently activated neurons have different purposes and responsibilities for the progress of generating images. In this study, by analyzing the statistics and the roles for those neurons, we empirically show that rarely activated neurons are related to the failure results of making diverse objects and inducing artifacts. In addition, we suggest a correction method, called `Sequential Ablationâ€™, to repair the defective part of the generated images without high computational cost and manual efforts.

----



[Go to the previous page](IJCAI-2022-list01.md)

[Go to the next page](IJCAI-2022-list03.md)

[Go to the catalog section](README.md)